<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yanqiangmiffy</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-15T09:31:20.310Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>致Great</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>我会自动改变</title>
    <link href="http://yoursite.com/2017/09/15/hello-world/"/>
    <id>http://yoursite.com/2017/09/15/hello-world/</id>
    <published>2017-09-15T09:31:20.310Z</published>
    <updated>2017-09-15T09:31:20.310Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>我会自动改变<br>Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.<br><a id="more"></a><br>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</excerpt></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;我会自动改变&lt;br&gt;Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络学习第三周编程作业</title>
    <link href="http://yoursite.com/2017/09/14/12-Planar%20data%20classification%20with%20one%20hidden%20layer%20v3/"/>
    <id>http://yoursite.com/2017/09/14/12-Planar data classification with one hidden layer v3/</id>
    <published>2017-09-14T21:44:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>吴恩达的神经网络学习第三周作业<br><a id="more"></a></excerpt></p><h1 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h1><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. </p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a single hidden layer</li><li>Use units with a non-linear activation function, such as tanh </li><li>Compute the cross entropy loss </li><li>Implement forward and backward propagation</li></ul><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://scikit-learn.org/stable/" target="_blank" rel="external">sklearn</a> provides simple and efficient tools for data mining and data analysis. </li><li><a href="http://matplotlib.org" target="_blank" rel="external">matplotlib</a> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Package imports</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</div><div class="line"><span class="keyword">import</span> sklearn</div><div class="line"><span class="keyword">import</span> sklearn.datasets</div><div class="line"><span class="keyword">import</span> sklearn.linear_model</div><div class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></div></pre></td></tr></table></figure><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2 - Dataset"></a>2 - Dataset</h2><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X, Y = load_planar_dataset()</div></pre></td></tr></table></figure><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Visualize the data:</span></div><div class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2017/09/14/59ba86d06f1df.png" alt="output_6_0.png"></p><p>You have:</p><pre><code>- a numpy-array (matrix) X that contains your features (x1, x2)- a numpy-array (vector) Y that contains your labels (red:0, blue:1).</code></pre><p>Lets first get a better sense of what our data is like. </p><p><strong>Exercise</strong>: How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code> and <code>Y</code>? </p><p><strong>Hint</strong>: How do you get the shape of a numpy array? <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html" target="_blank" rel="external">(help)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">shape_X = X.shape</div><div class="line">shape_Y = Y.shape</div><div class="line">m = X.shape[<span class="number">1</span>]  <span class="comment"># training set size</span></div><div class="line"><span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</div><div class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</div><div class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</div></pre></td></tr></table></figure><pre><code>The shape of X is: (2, 400)The shape of Y is: (1, 400)I have m = 400 training examples!</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%"><br><br>  <tr><br>    <td><strong>shape of X</strong></td><br>    <td> (2, 400) </td><br>  </tr><br><br>  <tr><br>    <td><strong>shape of Y</strong></td><br>    <td>(1, 400) </td><br>  </tr><br><br>    <tr><br>    <td><strong>m</strong></td><br>    <td> 400 </td><br>  </tr><br><br></table><h2 id="3-Simple-Logistic-Regression"><a href="#3-Simple-Logistic-Regression" class="headerlink" title="3 - Simple Logistic Regression"></a>3 - Simple Logistic Regression</h2><p>Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Train the logistic regression classifier</span></div><div class="line">clf = sklearn.linear_model.LogisticRegressionCV();</div><div class="line">clf.fit(X.T, Y.T);</div></pre></td></tr></table></figure><pre><code>/opt/conda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().  y = column_or_1d(y, warn=True)</code></pre><p>You can now plot the decision boundary of these models. Run the code below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></div><div class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</div><div class="line">plt.title(<span class="string">"Logistic Regression"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Print accuracy</span></div><div class="line">LR_predictions = clf.predict(X.T)</div><div class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</div><div class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</div></pre></td></tr></table></figure><pre><code>Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</code></pre><p><img src="https://i.loli.net/2017/09/14/59ba8716f1bcf.png" alt="output_13_1.png"></p><p><strong>Expected Output</strong>:</p><table style="width:20%"><br>  <tr><br>    <td><strong>Accuracy</strong></td><br>    <td> 47% </td><br>  </tr><br><br></table><p><strong>Interpretation</strong>: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now! </p><h2 id="4-Neural-Network-model"><a href="#4-Neural-Network-model" class="headerlink" title="4 - Neural Network model"></a>4 - Neural Network model</h2><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model</strong>:<br><img src="images/classification_kiank.png" style="width:600px;height:300px;"></p><p><strong>Mathematically</strong>:</p><p>For one example $x^{(i)}$:<br>$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}$$<br>$$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$<br>$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}$$<br>$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$<br>$$y^{(i)}_{prediction} = \begin{cases} 1 &amp; \mbox{if } a^{<a href="i">2</a>} &gt; 0.5 \ 0 &amp; \mbox{otherwise } \end{cases}\tag{5}$$</p><p>Given the predictions on all the examples, you can also compute the cost $J$ as follows:<br>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}$$</p><p><strong>Reminder</strong>: The general methodology to build a Neural Network is to:</p><pre><code>1. Define the neural network structure ( # of input units,  # of hidden units, etc). 2. Initialize the model&apos;s parameters3. Loop:    - Implement forward propagation    - Compute loss    - Implement backward propagation to get the gradients    - Update parameters (gradient descent)</code></pre><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learnt the right parameters, you can make predictions on new data.</p><h3 id="4-1-Defining-the-neural-network-structure"><a href="#4-1-Defining-the-neural-network-structure" class="headerlink" title="4.1 - Defining the neural network structure"></a>4.1 - Defining the neural network structure</h3><p><strong>Exercise</strong>: Define three variables:</p><pre><code>- n_x: the size of the input layer- n_h: the size of the hidden layer (set this to 4) - n_y: the size of the output layer</code></pre><p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></div><div class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    n_x -- the size of the input layer</span></div><div class="line"><span class="string">    n_h -- the size of the hidden layer</span></div><div class="line"><span class="string">    n_y -- the size of the output layer</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></div><div class="line">    n_h = <span class="number">4</span></div><div class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X_assess, Y_assess = layer_sizes_test_case()</div><div class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</div><div class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</div><div class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</div><div class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</div></pre></td></tr></table></figure><pre><code>The size of the input layer is: n_x = 5The size of the hidden layer is: n_h = 4The size of the output layer is: n_y = 2</code></pre><p><strong>Expected Output</strong> (these are not the sizes you will use for your network, they are just used to assess the function you’ve just coded).</p><table style="width:20%"><br>  <tr><br>    <td><strong>n_x</strong></td><br>    <td> 5 </td><br>  </tr><br><br>    <tr><br>    <td><strong>n_h</strong></td><br>    <td> 4 </td><br>  </tr><br><br>    <tr><br>    <td><strong>n_y</strong></td><br>    <td> 2 </td><br>  </tr><br><br></table><h3 id="4-2-Initialize-the-model’s-parameters"><a href="#4-2-Initialize-the-model’s-parameters" class="headerlink" title="4.2 - Initialize the model’s parameters"></a>4.2 - Initialize the model’s parameters</h3><p><strong>Exercise</strong>: Implement the function <code>initialize_parameters()</code>.</p><p><strong>Instructions</strong>:</p><ul><li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed.</li><li>You will initialize the weights matrices with random values. <ul><li>Use: <code>np.random.randn(a,b) * 0.01</code> to randomly initialize a matrix of shape (a,b).</li></ul></li><li>You will initialize the bias vectors as zeros. <ul><li>Use: <code>np.zeros((a,b))</code> to initialize a matrix of shape (a,b) with zeros.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Argument:</span></div><div class="line"><span class="string">    n_x -- size of the input layer</span></div><div class="line"><span class="string">    n_h -- size of the hidden layer</span></div><div class="line"><span class="string">    n_y -- size of the output layer</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    params -- python dictionary containing your parameters:</span></div><div class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></div><div class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></div><div class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></div><div class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></div><div class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</div><div class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></div><div class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</div><div class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</div><div class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</div><div class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</div><div class="line">    </div><div class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</div><div class="line">                  <span class="string">"b1"</span>: b1,</div><div class="line">                  <span class="string">"W2"</span>: W2,</div><div class="line">                  <span class="string">"b2"</span>: b2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">n_x, n_h, n_y = initialize_parameters_test_case()</div><div class="line"></div><div class="line">parameters = initialize_parameters(n_x, n_h, n_y)</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure><pre><code>W1 = [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]b1 = [[ 0.] [ 0.] [ 0.] [ 0.]]W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]b2 = [[ 0.]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:90%"><br>  <tr><br>    <td><strong>W1</strong></td><br>    <td> [[-0.00416758 -0.00056267]<br> [-0.02136196  0.01640271]<br> [-0.01793436 -0.00841747]<br> [ 0.00502881 -0.01245288]] </td><br>  </tr><br><br>  <tr><br>    <td><strong>b1</strong></td><br>    <td> [[ 0.]<br> [ 0.]<br> [ 0.]<br> [ 0.]] </td><br>  </tr><br><br>  <tr><br>    <td><strong>W2</strong></td><br>    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td><br>  </tr><br><br><br>  <tr><br>    <td><strong>b2</strong></td><br>    <td> [[ 0.]] </td><br>  </tr><br><br></table><h3 id="4-3-The-Loop"><a href="#4-3-The-Loop" class="headerlink" title="4.3 - The Loop"></a>4.3 - The Loop</h3><p><strong>Question</strong>: Implement <code>forward_propagation()</code>.</p><p><strong>Instructions</strong>:</p><ul><li>Look above at the mathematical representation of your classifier.</li><li>You can use the function <code>sigmoid()</code>. It is built-in (imported) in the notebook.</li><li>You can use the function <code>np.tanh()</code>. It is part of the numpy library.</li><li>The steps you have to implement are:<ol><li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>parameters[&quot;..&quot;]</code>.</li><li>Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).</li></ol></li><li>Values needed in the backpropagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the backpropagation function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Argument:</span></div><div class="line"><span class="string">    X -- input data of size (n_x, m)</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></div><div class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = parameters[<span class="string">'W1'</span>]</div><div class="line">    b1 = parameters[<span class="string">'b1'</span>]</div><div class="line">    W2 = parameters[<span class="string">'W2'</span>]</div><div class="line">    b2 = parameters[<span class="string">'b2'</span>]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    Z1 = np.dot(W1,X)+b1</div><div class="line">    A1 = np.tanh(Z1)</div><div class="line">    Z2 = np.dot(W2,A1)+b2<span class="comment">#第一层的输出作为第二层的输入</span></div><div class="line">    A2 = sigmoid(Z2)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</div><div class="line">    </div><div class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</div><div class="line">             <span class="string">"A1"</span>: A1,</div><div class="line">             <span class="string">"Z2"</span>: Z2,</div><div class="line">             <span class="string">"A2"</span>: A2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> A2, cache</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">X_assess, parameters = forward_propagation_test_case()</div><div class="line"></div><div class="line">A2, cache = forward_propagation(X_assess, parameters)</div><div class="line"></div><div class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></div><div class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</div></pre></td></tr></table></figure><pre><code>-0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852</code></pre><p><strong>Expected Output</strong>:</p><table style="width:55%"><br>  <tr><br>    <td> -0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852 </td><br>  </tr><br></table><p>Now that you have computed $A^{[2]}$ (in the Python variable “<code>A2</code>“), which contains $a^{<a href="i">2</a>}$ for every example, you can compute the cost function as follows:</p><p>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$</p><p><strong>Exercise</strong>: Implement <code>compute_cost()</code> to compute the value of the cost $J$.</p><p><strong>Instructions</strong>:</p><ul><li>There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented<br>$- \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{<a href="i">2</a>})$:<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">logprobs = np.multiply(np.log(A2),Y)</div><div class="line">cost = - np.sum(logprobs)                <span class="comment"># no need to use a for loop!</span></div></pre></td></tr></table></figure></li></ul><p>(you can use either <code>np.multiply()</code> and then <code>np.sum()</code> or directly <code>np.dot()</code>).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></div><div class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></div><div class="line"></div><div class="line">    <span class="comment"># Compute the cross-entropy cost</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(<span class="number">1</span>-A2),<span class="number">1</span>-Y)</div><div class="line">    cost = -(<span class="number">1</span>/m)*np.sum(logprobs)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></div><div class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></div><div class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> cost</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">A2, Y_assess, parameters = compute_cost_test_case()</div><div class="line"></div><div class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</div></pre></td></tr></table></figure><pre><code>cost = 0.692919893776</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%"><br>  <tr><br>    <td><strong>cost</strong></td><br>    <td> 0.692919893776 </td><br>  </tr><br><br></table><p>Using the cache computed during forward propagation, you can now implement backward propagation.</p><p><strong>Question</strong>: Implement the function <code>backward_propagation()</code>.</p><p><strong>Instructions</strong>:<br>Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  </p><p><img src="images/grad_summary.png" style="width:600px;height:300px;"></p><!--$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$- Note that $*$ denotes elementwise multiplication.- The notation you will use is common in deep learning coding:    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$!--><ul><li>Tips:<ul><li>To compute dZ1 you’ll need to compute $g^{[1]’}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]’}(z) = 1-a^2$. So you can compute<br>$g^{[1]’}(Z^{[1]})$ using <code>(1 - np.power(A1, 2))</code>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></div><div class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></div><div class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></div><div class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></div><div class="line"><span class="string">    """</span></div><div class="line">    m = X.shape[<span class="number">1</span>]</div><div class="line">    </div><div class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">    W1 = parameters[<span class="string">"W1"</span>]</div><div class="line">    W2 = parameters[<span class="string">"W2"</span>]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">    A1 = cache[<span class="string">"A1"</span>]</div><div class="line">    A2 = cache[<span class="string">"A2"</span>]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></div><div class="line">    dZ2= A2-Y</div><div class="line">    dW2 = (<span class="number">1</span>/m)*np.dot(dZ2,A1.T)</div><div class="line">    db2 = (<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</div><div class="line">    dZ1 = np.dot(W2.T,dZ2)*(<span class="number">1</span>-np.power(A1,<span class="number">2</span>))</div><div class="line">    dW1 = (<span class="number">1</span>/m)*np.dot(dZ1,X.T)</div><div class="line">    db1 = (<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</div><div class="line">             <span class="string">"db1"</span>: db1,</div><div class="line">             <span class="string">"dW2"</span>: dW2,</div><div class="line">             <span class="string">"db2"</span>: db2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> grads</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</div><div class="line"></div><div class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</div></pre></td></tr></table></figure><pre><code>dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847  0.00369379] [-0.02206365  0.01535126]]db1 = [[-0.00069728] [-0.00060606] [ 0.000364  ] [ 0.00151207]]dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]db2 = [[ 0.06589489]]</code></pre><p><strong>Expected output</strong>:</p><table style="width:80%"><br>  <tr><br>    <td><strong>dW1</strong></td><br>    <td> [[ 0.01018708 -0.00708701]<br> [ 0.00873447 -0.0060768 ]<br> [-0.00530847  0.00369379]<br> [-0.02206365  0.01535126]] </td><br>  </tr><br><br>  <tr><br>    <td><strong>db1</strong></td><br>    <td>  [[-0.00069728]<br> [-0.00060606]<br> [ 0.000364  ]<br> [ 0.00151207]] </td><br>  </tr><br><br>  <tr><br>    <td><strong>dW2</strong></td><br>    <td> [[ 0.00363613  0.03153604  0.01162914 -0.01318316]] </td><br>  </tr><br><br><br>  <tr><br>    <td><strong>db2</strong></td><br>    <td> [[ 0.06589489]] </td><br>  </tr><br><br></table>  <p><strong>Question</strong>: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p><p><strong>General gradient descent rule</strong>: $ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.</p><p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p><p><img src="images/sgd.gif" style="width:400;height:400;"> <img src="images/sgd_bad.gif" style="width:400;height:400;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></div><div class="line"><span class="string">    grads -- python dictionary containing your gradients </span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = parameters[<span class="string">"W1"</span>]</div><div class="line">    b1 = parameters[<span class="string">"b1"</span>]</div><div class="line">    W2 = parameters[<span class="string">"W2"</span>]</div><div class="line">    b2 = parameters[<span class="string">"b2"</span>]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    dW1 = grads[<span class="string">"dW1"</span>]</div><div class="line">    db1 = grads[<span class="string">"db1"</span>]</div><div class="line">    dW2 = grads[<span class="string">"dW2"</span>]</div><div class="line">    db2 = grads[<span class="string">"db2"</span>]</div><div class="line">    <span class="comment">## END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Update rule for each parameter</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">    W1 = W1-dW1*learning_rate</div><div class="line">    b1 = b1-db1*learning_rate</div><div class="line">    W2 = W2-dW2*learning_rate</div><div class="line">    b2 = b2-db2*learning_rate</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</div><div class="line">                  <span class="string">"b1"</span>: b1,</div><div class="line">                  <span class="string">"W2"</span>: W2,</div><div class="line">                  <span class="string">"b2"</span>: b2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">parameters, grads = update_parameters_test_case()</div><div class="line">parameters = update_parameters(parameters, grads)</div><div class="line"></div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure><pre><code>W1 = [[-0.00643025  0.01936718] [-0.02410458  0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]b1 = [[ -1.02420756e-06] [  1.27373948e-05] [  8.32996807e-07] [ -3.20136836e-06]]W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]b2 = [[ 0.00010457]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:80%"><br>  <tr><br>    <td><strong>W1</strong></td><br>    <td> [[-0.00643025  0.01936718]<br> [-0.02410458  0.03978052]<br> [-0.01653973 -0.02096177]<br> [ 0.01046864 -0.05990141]]</td><br>  </tr><br><br>  <tr><br>    <td><strong>b1</strong></td><br>    <td> [[ -1.02420756e-06]<br> [  1.27373948e-05]<br> [  8.32996807e-07]<br> [ -3.20136836e-06]]</td><br>  </tr><br><br>  <tr><br>    <td><strong>W2</strong></td><br>    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td><br>  </tr><br><br><br>  <tr><br>    <td><strong>b2</strong></td><br>    <td> [[ 0.00010457]] </td><br>  </tr><br><br></table>  <h3 id="4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model"><a href="#4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model" class="headerlink" title="4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()"></a>4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()</h3><p><strong>Question</strong>: Build your neural network model in <code>nn_model()</code>.</p><p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></div><div class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></div><div class="line"><span class="string">    n_h -- size of the hidden layer</span></div><div class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></div><div class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    np.random.seed(<span class="number">3</span>)</div><div class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</div><div class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</div><div class="line">    </div><div class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></div><div class="line">    parameters = initialize_parameters(n_x,n_h,n_y)</div><div class="line">    W1 = parameters[<span class="string">"W1"</span>]</div><div class="line">    b1 = parameters[<span class="string">"b1"</span>]</div><div class="line">    W2 = parameters[<span class="string">"W2"</span>]</div><div class="line">    b2 = parameters[<span class="string">"b2"</span>]</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Loop (gradient descent)</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</div><div class="line">         </div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></div><div class="line">        A2, cache =forward_propagation(X, parameters)</div><div class="line">        </div><div class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></div><div class="line">        cost = compute_cost(A2, Y, parameters)</div><div class="line"> </div><div class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></div><div class="line">        grads = backward_propagation(parameters, cache, X, Y)</div><div class="line"> </div><div class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></div><div class="line">        parameters = update_parameters(parameters, grads)</div><div class="line">        </div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">        <span class="comment"># Print the cost every 1000 iterations</span></div><div class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> parameters</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">X_assess, Y_assess = nn_model_test_case()</div><div class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="keyword">False</span>)</div><div class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</div><div class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</div><div class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</div><div class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</div></pre></td></tr></table></figure><pre><code>/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:20: RuntimeWarning: divide by zero encountered in log/home/jovyan/work/Week 3/Planar data classification with one hidden layer/planar_utils.py:34: RuntimeWarning: overflow encountered in exp  s = 1/(1+np.exp(-x))W1 = [[-4.18494056  5.33220609] [-7.52989382  1.24306181] [-4.1929459   5.32632331] [ 7.52983719 -1.24309422]]b1 = [[ 2.32926819] [ 3.79458998] [ 2.33002577] [-3.79468846]]W2 = [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]]b2 = [[-52.66607724]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:90%"><br>  <tr><br>    <td><strong>W1</strong></td><br>    <td> [[-4.18494056  5.33220609]<br> [-7.52989382  1.24306181]<br> [-4.1929459   5.32632331]<br> [ 7.52983719 -1.24309422]]</td><br>  </tr><br><br>  <tr><br>    <td><strong>b1</strong></td><br>    <td> [[ 2.32926819]<br> [ 3.79458998]<br> [ 2.33002577]<br> [-3.79468846]]</td><br>  </tr><br><br>  <tr><br>    <td><strong>W2</strong></td><br>    <td> [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]] </td><br>  </tr><br><br><br>  <tr><br>    <td><strong>b2</strong></td><br>    <td> [[-52.66607724]] </td><br>  </tr><br><br></table>  <h3 id="4-5-Predictions"><a href="#4-5-Predictions" class="headerlink" title="4.5 Predictions"></a>4.5 Predictions</h3><p><strong>Question</strong>: Use your model to predict by building predict().<br>Use forward propagation to predict results.</p><p><strong>Reminder</strong>: predictions = $y_{prediction} = \mathbb 1 \textfalse = \begin{cases}<br>      1 &amp; \text{if}\ activation &gt; 0.5 \<br>      0 &amp; \text{otherwise}<br>    \end{cases}$  </p><p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">```python</div><div class="line"># GRADED FUNCTION: predict</div><div class="line"></div><div class="line">def predict(parameters, X):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    Using the learned parameters, predicts a class for each example in X</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    parameters -- python dictionary containing your parameters </div><div class="line">    X -- input data of size (n_x, m)</div><div class="line">    </div><div class="line">    Returns</div><div class="line">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    </div><div class="line">    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</div><div class="line">    ### START CODE HERE ### (≈ 2 lines of code)</div><div class="line">    A2, cache = forward_propagation(X, parameters)</div><div class="line">    predictions = (A2&gt;0.5)</div><div class="line">    ### END CODE HERE ###</div><div class="line">    </div><div class="line">    return predictions</div></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parameters, X_assess = predict_test_case()</div><div class="line">predictions = predict(parameters, X_assess)</div><div class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</div></pre></td></tr></table></figure><pre><code>predictions mean = 0.666666666667</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%"><br>  <tr><br>    <td><strong>predictions mean</strong></td><br>    <td> 0.666666666667 </td><br>  </tr><br><br></table><p>It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></div><div class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># Plot the decision boundary</span></div><div class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</div><div class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</div></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693048Cost after iteration 1000: 0.288083Cost after iteration 2000: 0.254385Cost after iteration 3000: 0.233864Cost after iteration 4000: 0.226792Cost after iteration 5000: 0.222644Cost after iteration 6000: 0.219731Cost after iteration 7000: 0.217504Cost after iteration 8000: 0.219454Cost after iteration 9000: 0.218607&lt;matplotlib.text.Text at 0x7f1b55d40b38&gt;</code></pre><p><img src="https://i.loli.net/2017/09/14/59ba872ad66e3.png" alt="output_50_2.png"></p><p><strong>Expected Output</strong>:</p><table style="width:40%"><br>  <tr><br>    <td><strong>Cost after iteration 9000</strong></td><br>    <td> 0.218607 </td><br>  </tr><br><br></table><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Print accuracy</span></div><div class="line">predictions = predict(parameters, X)</div><div class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</div></pre></td></tr></table></figure><pre><code>Accuracy: 90%</code></pre><p><strong>Expected Output</strong>: </p><table style="width:15%"><br>  <tr><br>    <td><strong>Accuracy</strong></td><br>    <td> 90% </td><br>  </tr><br></table><p>Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. </p><p>Now, let’s try out several hidden layer sizes.</p><h3 id="4-6-Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#4-6-Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="4.6 - Tuning hidden layer size (optional/ungraded exercise)"></a>4.6 - Tuning hidden layer size (optional/ungraded exercise)</h3><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This may take about 2 minutes to run</span></div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</div><div class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</div><div class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</div><div class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</div><div class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</div><div class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</div><div class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</div><div class="line">    predictions = predict(parameters, X)</div><div class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</div><div class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</div></pre></td></tr></table></figure><pre><code>Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 90.25 %</code></pre><p><img src="https://i.loli.net/2017/09/14/59ba87411a5a5.png" alt="output_56_1.png"></p><p><strong>Interpretation</strong>:</p><ul><li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. </li><li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.</li><li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. </li></ul><p><strong>Optional questions</strong>:</p><p><strong>Note</strong>: Remember to submit the assignment but clicking the blue “Submit Assignment” button at the upper-right. </p><p>Some optional/ungraded questions that you can explore if you wish: </p><ul><li>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</li><li>Play with the learning_rate. What happens?</li><li>What if we change the dataset? (See part 5 below!)</li></ul><p><font color="blue"><br><strong>You’ve learnt to:</strong></font></p><ul><li>Build a complete neural network with a hidden layer</li><li>Make a good use of a non-linear unit</li><li>Implemented forward propagation and backpropagation, and trained a neural network</li><li>See the impact of varying the hidden layer size, including overfitting.</li></ul><p>Nice work! </p><h2 id="5-Performance-on-other-datasets"><a href="#5-Performance-on-other-datasets" class="headerlink" title="5) Performance on other datasets"></a>5) Performance on other datasets</h2><p>If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Datasets</span></div><div class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</div><div class="line"></div><div class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</div><div class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</div><div class="line">            <span class="string">"blobs"</span>: blobs,</div><div class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</div><div class="line"></div><div class="line"><span class="comment">### START CODE HERE ### (choose your dataset)</span></div><div class="line">dataset = <span class="string">"noisy_moons"</span></div><div class="line"><span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">X, Y = datasets[dataset]</div><div class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="comment"># make blobs binary</span></div><div class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</div><div class="line">    Y = Y%<span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># Visualize the data</span></div><div class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2017/09/14/59ba87516be30.png" alt="output_63_0.png"></p><p>Congrats on finishing this Programming Assignment!</p><p>Reference:</p><ul><li><a href="http://scs.ryerson.ca/~aharley/neural-networks/" target="_blank" rel="external">http://scs.ryerson.ca/~aharley/neural-networks/</a></li><li><a href="http://cs231n.github.io/neural-networks-case-study/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-case-study/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;吴恩达的神经网络学习第三周作业&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降法" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>神经网络学习第二周编程作业</title>
    <link href="http://yoursite.com/2017/09/14/11-Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20v3/"/>
    <id>http://yoursite.com/2017/09/14/11-Logistic Regression with a Neural Network mindset v3/</id>
    <published>2017-09-14T09:58:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>吴恩达的神经网络学习第二周作业<br><a id="more"></a></excerpt></p><h1 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a>Logistic Regression with a Neural Network mindset</h1><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li>Build the general architecture of a learning algorithm, including:<ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent) </li></ul></li><li>Gather all three functions above into a main model function, in the right order.</li></ul><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>First, let’s run the cell below to import all the packages that you will need during this assignment. </p><ul><li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://www.h5py.org" target="_blank" rel="external">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li><li><a href="http://matplotlib.org" target="_blank" rel="external">matplotlib</a> is a famous library to plot graphs in Python.</li><li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="external">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="external">scipy</a> are used here to test your model with your own picture at the end.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> h5py</div><div class="line"><span class="keyword">import</span> scipy</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</div><div class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</div><div class="line"></div><div class="line">%matplotlib inline</div></pre></td></tr></table></figure><h2 id="2-Overview-of-the-Problem-set"><a href="#2-Overview-of-the-Problem-set" class="headerlink" title="2 - Overview of the Problem set"></a>2 - Overview of the Problem set</h2><p><strong>Problem Statement</strong>: You are given a dataset (“data.h5”) containing:</p><pre><code>- a training set of m_train images labeled as cat (y=1) or non-cat (y=0)- a test set of m_test images labeled as cat or non-cat- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</code></pre><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Loading the data (cat/non-cat)</span></div><div class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</div></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the <code>index</code> value and re-run to see other images. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Example of a picture</span></div><div class="line">index = <span class="number">25</span></div><div class="line">plt.imshow(train_set_x_orig[index])</div><div class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</div></pre></td></tr></table></figure><pre><code>y = [1], it&apos;s a &apos;cat&apos; picture.</code></pre><p><a href="https://i.loli.net/2017/09/14/59b9e0275a1e0.png" target="_blank" rel="external"><img src="https://i.loli.net/2017/09/14/59b9e0275a1e0.png" alt="output_6_1.png"></a></p><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. </p><p><strong>Exercise:</strong> Find the values for:</p><pre><code>- m_train (number of training examples)- m_test (number of test examples)- num_px (= height = width of a training image)</code></pre><p>Remember that <code>train_set_x_orig</code> is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access <code>m_train</code> by writing <code>train_set_x_orig.shape[0]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></div><div class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</div><div class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</div><div class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</div><div class="line"><span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</div></pre></td></tr></table></figure><pre><code>Number of training examples: m_train = 209Number of testing examples: m_test = 50Height/Width of each image: num_px = 64Each image is of size: (64, 64, 3)train_set_x shape: (209, 64, 64, 3)train_set_y shape: (1, 209)test_set_x shape: (50, 64, 64, 3)test_set_y shape: (1, 50)</code></pre><p><strong>Expected Output for m_train, m_test and num_px</strong>: </p><table style="width:15%"><br>  <tr><br>    <td><strong>m_train</strong></td><br>    <td> 209 </td><br>  </tr><br><br>  <tr><br>    <td><strong>m_test</strong></td><br>    <td> 50 </td><br>  </tr><br><br>  <tr><br>    <td><strong>num_px</strong></td><br>    <td> 64 </td><br>  </tr><br><br></table><p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $<em>$ num_px $</em>$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p><p><strong>Exercise:</strong> Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $<em>$ num_px $</em>$ 3, 1).</p><p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$<em>$c$</em>$d, a) is to use:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></div></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Reshape the training and test examples</span></div><div class="line"></div><div class="line"><span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</div><div class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</div><div class="line"><span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</div><div class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))</div></pre></td></tr></table></figure><pre><code>train_set_x_flatten shape: (12288, 209)train_set_y shape: (1, 209)test_set_x_flatten shape: (12288, 50)test_set_y shape: (1, 50)sanity check after reshaping: [17 31 56 22 33]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:35%"><br>  <tr><br>    <td><strong>train_set_x_flatten shape</strong></td><br>    <td> (12288, 209)</td><br>  </tr><br>  <tr><br>    <td><strong>train_set_y shape</strong></td><br>    <td>(1, 209)</td><br>  </tr><br>  <tr><br>    <td><strong>test_set_x_flatten shape</strong></td><br>    <td>(12288, 50)</td><br>  </tr><br>  <tr><br>    <td><strong>test_set_y shape</strong></td><br>    <td>(1, 50)</td><br>  </tr><br>  <tr><br>  <td><strong>sanity check after reshaping</strong></td><br>  <td>[17 31 56 22 33]</td><br>  </tr><br></table><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p><p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p><!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> <p>Let’s standardize our dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></div><div class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></div></pre></td></tr></table></figure><p><font color="blue"><br><strong>What you need to remember:</strong></font></p><p>Common steps for pre-processing a new dataset are:</p><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</li></ul><h2 id="3-General-Architecture-of-the-learning-algorithm"><a href="#3-General-Architecture-of-the-learning-algorithm" class="headerlink" title="3 - General Architecture of the learning algorithm"></a>3 - General Architecture of the learning algorithm</h2><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p><p><img src="images/LogReg_kiank.png" style="width:650px;height:400px;"></p><p><strong>Mathematical expression of the algorithm</strong>:</p><p>For one example $x^{(i)}$:<br>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$<br>$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$<br>$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$</p><p>The cost is then computed by summing over all training examples:<br>$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$</p><p><strong>Key steps</strong>:<br>In this exercise, you will carry out the following steps: </p><pre><code>- Initialize the parameters of the model- Learn the parameters for the model by minimizing the cost  - Use the learned parameters to make predictions (on the test set)- Analyse the results and conclude</code></pre><h2 id="4-Building-the-parts-of-our-algorithm"><a href="#4-Building-the-parts-of-our-algorithm" class="headerlink" title="4 - Building the parts of our algorithm"></a>4 - Building the parts of our algorithm</h2><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features) </li><li>Initialize the model’s parameters</li><li>Loop:<ul><li>Calculate current loss (forward propagation)</li><li>Calculate current gradient (backward propagation)</li><li>Update parameters (gradient descent)</li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call <code>model()</code>.</p><h3 id="4-1-Helper-functions"><a href="#4-1-Helper-functions" class="headerlink" title="4.1 - Helper functions"></a>4.1 - Helper functions</h3><p><strong>Exercise</strong>: Using your code from “Python Basics”, implement <code>sigmoid()</code>. As you’ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the sigmoid of z</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    s -- sigmoid(z)</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></div><div class="line">    s = <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-z))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> s</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</div></pre></td></tr></table></figure><pre><code>sigmoid([0, 2]) = [ 0.5         0.88079708]</code></pre><p><strong>Expected Output</strong>: </p><table><br>  <tr><br>    <td><strong>sigmoid([0, 2])</strong></td><br>    <td> [ 0.5         0.88079708]</td><br>  </tr><br></table><h3 id="4-2-Initializing-parameters"><a href="#4-2-Initializing-parameters" class="headerlink" title="4.2 - Initializing parameters"></a>4.2 - Initializing parameters</h3><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Argument:</span></div><div class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></div><div class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></div><div class="line">    w, b = np.zeros((dim,<span class="number">1</span>)), <span class="number">0</span></div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</div><div class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> w, b</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dim = <span class="number">2</span></div><div class="line">w, b = initialize_with_zeros(dim)</div><div class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</div></pre></td></tr></table></figure><pre><code>w = [[ 0.] [ 0.]]b = 0</code></pre><p><strong>Expected Output</strong>: </p><table style="width:15%"><br>    <tr><br>        <td>  <strong> w </strong>  </td><br>        <td> [[ 0.]<br> [ 0.]] </td><br>    </tr><br>    <tr><br>        <td>  <strong> b </strong>  </td><br>        <td> 0 </td><br>    </tr><br></table><p>For image inputs, w will be of shape (num_px $\times$ num_px $\times$ 3, 1).</p><h3 id="4-3-Forward-and-Backward-propagation"><a href="#4-3-Forward-and-Backward-propagation" class="headerlink" title="4.3 - Forward and Backward propagation"></a>4.3 - Forward and Backward propagation</h3><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function <code>propagate()</code> that computes the cost function and its gradient.</p><p><strong>Hints</strong>:</p><p>Forward Propagation:</p><ul><li>You get X</li><li>You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$</li><li>You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li></ul><p>Here are the two formulas you will be using: </p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$<br>$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: propagate</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></div><div class="line"><span class="string">    b -- bias, a scalar</span></div><div class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></div><div class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></div><div class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></div><div class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Tips:</span></div><div class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    m = X.shape[<span class="number">1</span>]</div><div class="line">    </div><div class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">    A = sigmoid(np.dot(w.T,X)+b) <span class="comment"># A</span></div><div class="line">    cost = -(<span class="number">1</span>/m)*np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))                            <span class="comment"># compute cost</span></div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></div><div class="line">    dw = <span class="number">1</span>/m*np.dot(X,(A-Y).T)</div><div class="line">    db = <span class="number">1</span>/m*np.sum(A-Y)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</div><div class="line">    <span class="keyword">assert</span>(db.dtype == float)</div><div class="line">    cost = np.squeeze(cost)</div><div class="line">    <span class="keyword">assert</span>(cost.shape == ())</div><div class="line">    </div><div class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</div><div class="line">             <span class="string">"db"</span>: db&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> grads, cost</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">w, b, X, Y = np.array([[<span class="number">1</span>],[<span class="number">2</span>]]), <span class="number">2</span>, np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>]])</div><div class="line">grads, cost = propagate(w, b, X, Y)</div><div class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</div></pre></td></tr></table></figure><pre><code>dw = [[ 0.99993216] [ 1.99980262]]db = 0.499935230625cost = 6.00006477319</code></pre><p><strong>Expected Output</strong>:</p><table style="width:50%"><br>    <tr><br>        <td>  <strong> dw </strong>  </td><br>        <td> [[ 0.99993216]<br> [ 1.99980262]]</td><br>    </tr><br>    <tr><br>        <td>  <strong> db </strong>  </td><br>        <td> 0.499935230625 </td><br>    </tr><br>    <tr><br>        <td>  <strong> cost </strong>  </td><br>        <td> 6.000064773192205</td><br>    </tr><br><br></table><h3 id="d-Optimization"><a href="#d-Optimization" class="headerlink" title="d) Optimization"></a>d) Optimization</h3><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: optimize</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></div><div class="line"><span class="string">    b -- bias, a scalar</span></div><div class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></div><div class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></div><div class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></div><div class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></div><div class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></div><div class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></div><div class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Tips:</span></div><div class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></div><div class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></div><div class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    costs = []</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</div><div class="line">        </div><div class="line">        </div><div class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></div><div class="line">        <span class="comment">### START CODE HERE ### </span></div><div class="line">        grads, cost = propagate(w, b, X, Y)</div><div class="line">        costs.append(cost)</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">        <span class="comment"># Retrieve derivatives from grads</span></div><div class="line">        dw = grads[<span class="string">"dw"</span>]</div><div class="line">        db = grads[<span class="string">"db"</span>]</div><div class="line">        </div><div class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></div><div class="line">        <span class="comment">### START CODE HERE ###</span></div><div class="line">        w = w - learning_rate*dw</div><div class="line">        b = b - learning_rate*db</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        </div><div class="line">        <span class="comment"># Record the costs</span></div><div class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            costs.append(cost)</div><div class="line">        </div><div class="line">        <span class="comment"># Print the cost every 100 training examples</span></div><div class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</div><div class="line">    </div><div class="line">    params = &#123;<span class="string">"w"</span>: w,</div><div class="line">              <span class="string">"b"</span>: b&#125;</div><div class="line">    </div><div class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</div><div class="line">             <span class="string">"db"</span>: db&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> params, grads, costs</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</div><div class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</div></pre></td></tr></table></figure><pre><code>w = [[ 0.1124579 ] [ 0.23106775]]b = 1.55930492484dw = [[ 0.90158428] [ 1.76250842]]db = 0.430462071679</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%"><br>    <tr><br>       <td> <strong>w</strong> </td><br>       <td>[[ 0.1124579 ]<br> [ 0.23106775]] </td><br>    </tr><br><br>    <tr><br>       <td> <strong>b</strong> </td><br>       <td> 1.55930492484 </td><br>    </tr><br>    <tr><br>       <td> <strong>dw</strong> </td><br>       <td> [[ 0.90158428]<br> [ 1.76250842]] </td><br>    </tr><br>    <tr><br>       <td> <strong>db</strong> </td><br>       <td> 0.430462071679 </td><br>    </tr><br><br></table><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li><p>Calculate $\hat{Y} = A = \sigma(w^T X + b)$</p></li><li><p>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_prediction</code>. If you wish, you can use an <code>if</code>/<code>else</code> statement in a <code>for</code> loop (though there is also a way to vectorize this). </p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: predict</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></div><div class="line"><span class="string">    b -- bias, a scalar</span></div><div class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    </div><div class="line">    m = X.shape[<span class="number">1</span>]</div><div class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</div><div class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></div><div class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></div><div class="line">    A =sigmoid(np.dot(w.T,X)+b)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</div><div class="line">        </div><div class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></div><div class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></div><div class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i]&lt;=<span class="number">0.5</span>:</div><div class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Y_prediction</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</div></pre></td></tr></table></figure><pre><code>predictions = [[ 1.  1.]]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:30%"><br>    <tr><br>         <td><br>             <strong>predictions</strong><br>         </td><br>          <td><br>            [[ 1.  1.]]<br>         </td><br>   </tr><br><br></table><p><font color="blue"><br><strong>What to remember:</strong><br>You’ve implemented several functions that:</font></p><ul><li>Initialize (w,b)</li><li>Optimize the loss iteratively to learn parameters (w,b):<ul><li>computing the cost and its gradient </li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w,b) to predict the labels for a given set of examples</li></ul><h2 id="5-Merge-all-functions-into-a-model"><a href="#5-Merge-all-functions-into-a-model" class="headerlink" title="5 - Merge all functions into a model"></a>5 - Merge all functions into a model</h2><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p><pre><code>- Y_prediction for your predictions on the test set- Y_prediction_train for your predictions on the train set- w, costs, grads for the outputs of optimize()</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: model</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></div><div class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></div><div class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></div><div class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></div><div class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></div><div class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></div><div class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns:</span></div><div class="line"><span class="string">    d -- dictionary containing information about the model.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></div><div class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></div><div class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</div><div class="line">    </div><div class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></div><div class="line">    w = parameters[<span class="string">"w"</span>]</div><div class="line">    b = parameters[<span class="string">"b"</span>]</div><div class="line">    </div><div class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></div><div class="line">    Y_prediction_test = predict(w, b, X_test)</div><div class="line">    Y_prediction_train = predict(w, b, X_train)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="comment"># Print train/test Errors</span></div><div class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</div><div class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</div><div class="line"></div><div class="line">    </div><div class="line">    d = &#123;<span class="string">"costs"</span>: costs,</div><div class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </div><div class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </div><div class="line">         <span class="string">"w"</span> : w, </div><div class="line">         <span class="string">"b"</span> : b,</div><div class="line">         <span class="string">"learning_rate"</span> : learning_rate,</div><div class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> d</div></pre></td></tr></table></figure><p>Run the following cell to train your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>)</div></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872train accuracy: 99.04306220095694 %test accuracy: 70.0 %</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%"><br><br>    <tr><br>        <td> <strong>Train Accuracy</strong>  </td><br>        <td> 99.04306220095694 % </td><br>    </tr><br><br>    <tr><br>        <td><strong>Test Accuracy</strong> </td><br>        <td> 70.0 % </td><br>    </tr><br></table> <p><strong>Comment</strong>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the <code>index</code> variable) you can look at predictions on pictures of the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></div><div class="line">index = <span class="number">1</span></div><div class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</div><div class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</div></pre></td></tr></table></figure><pre><code>/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:4: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the futurey = 1, you predicted that it is a &quot;cat&quot; picture.</code></pre><p><img src="https://i.loli.net/2017/09/14/59b9e0563b548.png" alt="output_44_2.png"></p><p>Let’s also plot the cost function and the gradients.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Plot learning curve (with costs)</span></div><div class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</div><div class="line">plt.plot(costs)</div><div class="line">plt.ylabel(<span class="string">'cost'</span>)</div><div class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</div><div class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2017/09/14/59b9e06d98c1a.png" alt="output_46_0.png"></p><p><strong>Interpretation</strong>:<br>You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. </p><h2 id="6-Further-analysis-optional-ungraded-exercise"><a href="#6-Further-analysis-optional-ungraded-exercise" class="headerlink" title="6 - Further analysis (optional/ungraded exercise)"></a>6 - Further analysis (optional/ungraded exercise)</h2><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $\alpha$. </p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>:<br>In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</div><div class="line">models = &#123;&#125;</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</div><div class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</div><div class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="keyword">False</span>)</div><div class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</div><div class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</div><div class="line"></div><div class="line">plt.ylabel(<span class="string">'cost'</span>)</div><div class="line">plt.xlabel(<span class="string">'iterations'</span>)</div><div class="line"></div><div class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="keyword">True</span>)</div><div class="line">frame = legend.get_frame()</div><div class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><pre><code>learning rate is: 0.01train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001train accuracy: 68.42105263157895 %test accuracy: 36.0 %-------------------------------------------------------</code></pre><p><img src="https://i.loli.net/2017/09/14/59b9e08244c12.png" alt="output_50_1.png"></p><p><strong>Interpretation</strong>: </p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). </li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you: <ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) </li></ul></li></ul><h2 id="7-Test-with-your-own-image-optional-ungraded-exercise"><a href="#7-Test-with-your-own-image-optional-ungraded-exercise" class="headerlink" title="7 - Test with your own image (optional/ungraded exercise)"></a>7 - Test with your own image (optional/ungraded exercise)</h2><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:</p><pre><code>1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub.2. Add your image to this Jupyter Notebook&apos;s directory, in the &quot;images&quot; folder3. Change your image&apos;s name in the following code4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## START CODE HERE ## (PUT YOUR IMAGE NAME) </span></div><div class="line">my_image = <span class="string">"my_image.jpg"</span>   <span class="comment"># change this to the name of your image file </span></div><div class="line"><span class="comment">## END CODE HERE ##</span></div><div class="line"></div><div class="line"><span class="comment"># We preprocess the image to fit your algorithm.</span></div><div class="line">fname = <span class="string">"images/"</span> + my_image</div><div class="line">image = np.array(ndimage.imread(fname, flatten=<span class="keyword">False</span>))</div><div class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T</div><div class="line">my_predicted_image = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image)</div><div class="line"></div><div class="line">plt.imshow(image)</div><div class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</div></pre></td></tr></table></figure><pre><code>y = 0.0, your algorithm predicts a &quot;non-cat&quot; picture.</code></pre><p><img src="https://i.loli.net/2017/09/14/59b9e095d3ad5.png" alt="output_53_1.png"></p><p><font color="blue"><br><strong>What to remember from this assignment:</strong></font></p><ol><li>Preprocessing the dataset is important.</li><li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li><li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li></ol><p>Finally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:</p><pre><code>- Play with the learning rate and the number of iterations- Try different initialization methods and compare the results- Test other preprocessings (center the data, or divide each row by its standard deviation)</code></pre><p>Bibliography:</p><ul><li><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="external">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a></li><li><a href="https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c" target="_blank" rel="external">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;吴恩达的神经网络学习第二周作业&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类问题" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>贪心算法总结</title>
    <link href="http://yoursite.com/2017/08/31/10-%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/08/31/10-贪心算法/</id>
    <published>2017-08-31T15:29:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>贪心算法的介绍以及背包问题，活动安排问题，最小生成树问题三大实例的解析<br><a id="more"></a></excerpt></p><h1 id="什么贪心算法"><a href="#什么贪心算法" class="headerlink" title="什么贪心算法"></a>什么贪心算法</h1><p>贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。<br>贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关。</p><h1 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h1><ol><li>建立数学模型来描述问题；</li><li>把求解的问题分成若干个子问题；</li><li>对每一子问题求解，得到子问题的局部最优解；</li><li>把子问题的解局部最优解合成原来解问题的一个解。</li></ol><h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><ol><li>从问题的某个初始解出发。</li><li>采用循环语句，当可以向求解目标前进一步时，就根据局部最优策略，得到一个部分解，缩小问题的范围或规模。</li><li>将所有部分解综合起来，得到问题的最终解。</li></ol><h1 id="实例分析"><a href="#实例分析" class="headerlink" title="实例分析"></a>实例分析</h1><h2 id="实例1-背包问题"><a href="#实例1-背包问题" class="headerlink" title="实例1 背包问题"></a>实例1 背包问题</h2><ul><li>问题描述<br>有一个背包，背包容量是M=150。有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。<br><img src="http://upload-images.jianshu.io/upload_images/1531909-f0413127c4754248.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></li><li>问题分析<br>1.目标函数： ∑pi最大，使得装入背包中的所有物品pi的价值加起来最大。<br>2.约束条件：装入的物品总重量不超过背包容量：∑wi&lt;=M( M=150)<br>3.贪心策略：<ul><li>选择价值最大的物品</li><li>选择价值最大的物品</li><li>选择单位重量价值最大的物品<br>有三个物品A,B,C，其重量分别为{30,10,20}，价值分别为{60,30,80}，背包的容量为50，分别应用三种贪心策略装入背包的物品和获得的价值如下图所示：</li></ul></li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-78c7905f18412c41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="三种策略"></p><ul><li>算法设计：</li></ul><ol><li>计算出每个物品单位重量的价值</li><li>按单位价值从大到小将物品排序</li><li>根据背包当前所剩容量选取物品</li><li>如果背包的容量大于当前物品的重量，那么就将当前物品装进去。否则，那么就将当前物品舍去，然后跳出循环结束。</li></ol><ul><li><p>代码实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></div><div class="line">    <span class="keyword">int</span> w;</div><div class="line">    <span class="keyword">int</span> v;</div><div class="line">    <span class="keyword">double</span> avg;</div><div class="line">&#125;P;</div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(P a,P b)</span></span>&#123;</div><div class="line">    <span class="keyword">return</span> a.avg&gt;b.avg;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    P *p;</div><div class="line">    <span class="keyword">int</span> n,i,m;<span class="comment">//n 物品个数 m背包容量</span></div><div class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;n&gt;&gt;m)&#123;</div><div class="line">        p=<span class="keyword">new</span> P[n];</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="built_in">cin</span>&gt;&gt;p[i].w&gt;&gt;p[i].v;</div><div class="line">            p[i].avg=p[i].v/p[i].w*<span class="number">1.0</span>;</div><div class="line">        &#125;</div><div class="line">        sort(p,p+n,cmp);</div><div class="line">        <span class="keyword">int</span> maxvalue=<span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="keyword">if</span>(p[i].w&lt;=m)&#123;</div><div class="line">                m-=p[i].w;</div><div class="line">                maxvalue+=p[i].v;</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;maxvalue&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>运行结果<br><img src="http://upload-images.jianshu.io/upload_images/1531909-bd56564c206c2060.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p></li></ul><h2 id="实例2-活动安排问题"><a href="#实例2-活动安排问题" class="headerlink" title="实例2 活动安排问题"></a>实例2 活动安排问题</h2><ul><li>问题描述：<br>设有n个活动的集合E={1,2,…,n}，其中每个活动都要求使用同一资源，如演讲会场等，而在同一时间内只有一个活动能使用这一资源。每个活动i都有一个要求使用该资源的起始时间si和一个结束时间fi,且si &lt;fi 。要求设计程序，使得安排的活动最多。</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-86b31fdeff88b7dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>(ps:活动结束时间按从小到大排序)</p><ul><li>问题分析：<br>活动安排问题要求安排一系列争用某一公共资源的活动。用贪心算法可提供一个简单、漂亮的方法，使尽可能多的活动能兼容的使用公共资源。设有n个活动的集合｛0，1，2，…，n-1｝，其中每个活动都要求使用同一资源，如会场等，而在同一时间内只有一个活动能使用这一资源。每个活动i都有一个要求使用该资源的起始时间starti和一个结束时间endi，且starti&lt;endi。如选择了活动i，则它在半开时间区间[starti,endi）内占用资源。若区间[starti,endi)与区间[startj,endj)不相交，称活动i与活动j是相容的。也就是说，当startj≥endi或starti≥endj时，活动i与活动j相容。活动安排问题就是在所给的活动集合中选出最多的不相容活动。<br>活动安排问题就是要在所给的活动集合中选出最大的相容活动子集合，是可以用贪心算法有效求解的很好例子。该问题要求高效地安排一系列争用某一公共资源的活动。贪心算法提供了一个简单、漂亮的方法使得尽可能多的活动能兼容地使用公共资源。</li><li>算法设计：<br>若被检查的活动i的开始时间starti小于最近选择的活动j的结束时间endj，则不选择活动i，否则选择活动i加入集合中。运用该算法解决活动安排问题的效率极高。当输入的活动已按结束时间的非减序排列，算法只需O(n)的时间安排n个活动，使最多的活动能相容地使用公共资源。如果所给出的活动未按非减序排列，可以用O(nlogn)的时间重排。</li><li><p>代码实现：<br>代码1</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">actime</span>&#123;</span></div><div class="line">    <span class="keyword">int</span> start,finish;</div><div class="line">&#125;act[<span class="number">1002</span>];</div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(actime a,actime b)</span></span>&#123;</div><div class="line">    <span class="keyword">return</span> a.finish&lt;b.finish;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> i,n,t,total;</div><div class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;n)&#123;<span class="comment">//活动的个数</span></div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="built_in">cin</span>&gt;&gt;act[i].start&gt;&gt;act[i].finish;</div><div class="line">        &#125;</div><div class="line">        sort(act,act+n,cmp);<span class="comment">//按活动结束时间从小到大排序</span></div><div class="line">        t=<span class="number">-1</span>;</div><div class="line">        total=<span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="keyword">if</span>(t&lt;=act[i].start)&#123;</div><div class="line">                total++;</div><div class="line">                t=act[i].finish;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;total&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>运行结果1<br><img src="http://upload-images.jianshu.io/upload_images/1531909-7f69c2dc3fa512e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p></li></ul><p>代码2<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">Type</span>&gt;</span></div><div class="line"><span class="class"><span class="title">void</span> <span class="title">GreedySelector</span>(<span class="title">int</span> <span class="title">n</span>, <span class="title">Type</span> <span class="title">s</span>[], <span class="title">Type</span> <span class="title">f</span>[], <span class="title">bool</span> <span class="title">A</span>[]);</span></div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">11</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">//下标从1开始,存储活动开始时间</span></div><div class="line">    <span class="keyword">int</span> s[] = &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">12</span>&#125;;</div><div class="line"></div><div class="line">    <span class="comment">//下标从1开始,存储活动结束时间</span></div><div class="line">    <span class="keyword">int</span> f[] = &#123;<span class="number">0</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>&#125;;</div><div class="line"></div><div class="line">    <span class="keyword">bool</span> A[N+<span class="number">1</span>];</div><div class="line"></div><div class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"各活动的开始时间,结束时间分别为："</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">    &#123;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"["</span>&lt;&lt;i&lt;&lt;<span class="string">"]:"</span>&lt;&lt;<span class="string">"("</span>&lt;&lt;s[i]&lt;&lt;<span class="string">","</span>&lt;&lt;f[i]&lt;&lt;<span class="string">")"</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    GreedySelector(N,s,f,A);</div><div class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"最大相容活动子集为："</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N;i++)</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">if</span>(A[i])&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"["</span>&lt;&lt;i&lt;&lt;<span class="string">"]:"</span>&lt;&lt;<span class="string">"("</span>&lt;&lt;s[i]&lt;&lt;<span class="string">","</span>&lt;&lt;f[i]&lt;&lt;<span class="string">")"</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">Type</span>&gt;</span></div><div class="line"><span class="class"><span class="title">void</span> <span class="title">GreedySelector</span>(<span class="title">int</span> <span class="title">n</span>, <span class="title">Type</span> <span class="title">s</span>[], <span class="title">Type</span> <span class="title">f</span>[], <span class="title">bool</span> <span class="title">A</span>[])</span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    A[<span class="number">1</span>]=<span class="literal">true</span>;</div><div class="line">    <span class="keyword">int</span> j=<span class="number">1</span>;<span class="comment">//记录最近一次加入A中的活动</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>;i&lt;=n;i++)<span class="comment">//依次检查活动i是否与当前已选择的活动相容</span></div><div class="line">    &#123;</div><div class="line">        <span class="keyword">if</span> (s[i]&gt;=f[j])</div><div class="line">        &#123;</div><div class="line">            A[i]=<span class="literal">true</span>;</div><div class="line">            j=i;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span></div><div class="line">        &#123;</div><div class="line">            A[i]=<span class="literal">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><ul><li>运行结果2</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-fdab36968693b0f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h2 id="实例3-最小生成树（克鲁斯卡尔算法）"><a href="#实例3-最小生成树（克鲁斯卡尔算法）" class="headerlink" title="实例3 最小生成树（克鲁斯卡尔算法）"></a>实例3 最小生成树（克鲁斯卡尔算法）</h2><ul><li>问题描述<blockquote><p>求一个连通无向图的最小生成树的代价（图边权值为正整数）。</p></blockquote></li></ul><p>输入</p><blockquote><p>第一行是一个整数N（1&lt;=N&lt;=20），表示有多少个图需要计算。以下有N个图，第i图的第一行是一个整数M（1&lt;=M&lt;=50），表示图的顶点数，第i图的第2行至1+M行为一个M*M的二维矩阵，其元素ai,j表示图的i顶点和j顶点的连接情况，如果ai,j=0，表示i顶点和j顶点不相连；如果ai,j&gt;0，表示i顶点和j顶点的连接权值。</p></blockquote><p>输出</p><blockquote><p>每个用例，用一行输出对应图的最小生成树的代价。</p></blockquote><p>样例输入</p><blockquote><p>1<br>6<br>0 6 1 5 0 0<br>6 0 5 0 3 0<br>1 5 0 5 6 4<br>5 0 5 0 0 2<br>0 3 6 0 0 6<br>0 0 4 2 6 0</p></blockquote><p>样例输出</p><blockquote><p>15</p><ul><li>Kruskal算法简述<br>假设 WN=(V,{E}) 是一个含有 n 个顶点的连通网，则按照克鲁斯卡尔算法构造最小生成树的过程为：先构造一个只含 n 个顶点，而边集为空的子图，若将该子图中各个顶点看成是各棵树上的根结点，则它是一个含有 n 棵树的一个森林。之后，从网的边集 E 中选取一条权值最小的边，若该条边的两个顶点分属不同的树，则将其加入子图，也就是说，将这两个顶点分别所在的两棵树合成一棵树；反之，若该条边的两个顶点已落在同一棵树上，则不可取，而应该取下一条权值最小的边再试之。依次类推，直至森林中只有一棵树，也即子图中含有 n-1条边为止。</li><li>模拟过程：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-32e164789d15c188.gif?imageMogr2/auto-orient/strip" alt="模拟过程"></li><li>算法难点：<br>（1）边的选择要求从小到大选择，则开始显然要对边进行升序排序。<br>（2）选择的边是否需要，则从判断该边加入后是否构成环入手。</li><li>算法设计：<br>（1）对边升序排序<br>在此采用链式结构，通过插入排序完成。每一结点存放一条边的左右端点序号、权值及后继结点指针<br>（2）边的加入是否构成环<br>一开始假定各顶点分别为一组，其组号为端点序号。选择某边后，看其两个端点是否在同一组中，即所在组号是否相同，如果是，表示构成了环，则舍去。  如果两个端点所在的组不同，则表示可以加入，则将该边两端的组合并成同一组。</li><li>代码实现：<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line"> <span class="keyword">int</span> l;</div><div class="line"> <span class="keyword">int</span> r;</div><div class="line"> <span class="keyword">int</span> len;</div><div class="line"> node *next;</div><div class="line">&#125;;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(node *&amp;h,node *p)</span>   <span class="comment">//指针插入排序</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"> node *q=h;</div><div class="line"> <span class="keyword">while</span>(q-&gt;next &amp;&amp; q-&gt;next-&gt;len &lt;= p-&gt;len)</div><div class="line"> &#123;</div><div class="line">  q=q-&gt;next;</div><div class="line"> &#125;</div><div class="line"> p-&gt;next=q-&gt;next;</div><div class="line"> q-&gt;next=p;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="comment">// freopen("001.in","r",stdin);</span></div><div class="line"> node *h,*p;</div><div class="line"> <span class="keyword">int</span> n,m,x,temp;</div><div class="line"> <span class="keyword">int</span> *a;</div><div class="line"> <span class="keyword">int</span> i,j;</div><div class="line"> <span class="keyword">int</span> sum;</div><div class="line"> <span class="built_in">cin</span>&gt;&gt;n;</div><div class="line"> <span class="keyword">while</span>(n--)</div><div class="line"> &#123;</div><div class="line">  sum=<span class="number">0</span>;</div><div class="line">  <span class="built_in">cin</span>&gt;&gt;m;</div><div class="line">  a=<span class="keyword">new</span> <span class="keyword">int</span>[m+<span class="number">1</span>];</div><div class="line">  <span class="keyword">for</span> (i=<span class="number">1</span>;i&lt;=m;i++)</div><div class="line">  &#123;</div><div class="line">   a[i]=i;</div><div class="line">  &#125;</div><div class="line">  h=<span class="keyword">new</span> node;</div><div class="line">  p=h;</div><div class="line">  p-&gt;next=<span class="literal">NULL</span>;</div><div class="line">  <span class="keyword">for</span> (i=<span class="number">1</span>;i&lt;=m;i++)</div><div class="line">   <span class="keyword">for</span> (j=<span class="number">1</span>;j&lt;=m;j++)</div><div class="line">   &#123;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;x;</div><div class="line">    <span class="keyword">if</span> (i&gt;j &amp;&amp; x!=<span class="number">0</span>)</div><div class="line">    &#123;</div><div class="line">     p=<span class="keyword">new</span> node;</div><div class="line">     p-&gt;l=i;</div><div class="line">     p-&gt;r=j;</div><div class="line">     p-&gt;len=x;</div><div class="line">     p-&gt;next=<span class="literal">NULL</span>;</div><div class="line">     insert(h,p);   <span class="comment">//调用插入排序</span></div><div class="line">    &#125;</div><div class="line">   &#125;</div><div class="line">          p=h-&gt;next;</div><div class="line">   <span class="keyword">while</span> (p)</div><div class="line">   &#123;</div><div class="line">    <span class="keyword">if</span> (a[p-&gt;l]!=a[p-&gt;r])</div><div class="line">    &#123;</div><div class="line"></div><div class="line">     sum+=p-&gt;len;</div><div class="line">     temp=a[p-&gt;l];</div><div class="line">     <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=m;i++)</div><div class="line">      <span class="keyword">if</span> (a[i]==temp)</div><div class="line">      &#123;</div><div class="line">       a[i]=a[p-&gt;r];</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">   p=p-&gt;next;</div><div class="line">   &#125;</div><div class="line">   <span class="comment">/*   可以测试程序工作是否正常</span></div><div class="line"><span class="comment">   p=h-&gt;next;</span></div><div class="line"><span class="comment">   while(p)</span></div><div class="line"><span class="comment">   &#123;</span></div><div class="line"><span class="comment">    cout&lt;&lt;p-&gt;l&lt;&lt;':';cout&lt;&lt;p-&gt;r&lt;&lt;' ';</span></div><div class="line"><span class="comment">    cout&lt;&lt;p-&gt;len&lt;&lt;"   ";</span></div><div class="line"><span class="comment">    p=p-&gt;next;</span></div><div class="line"><span class="comment">   &#125;</span></div><div class="line"><span class="comment">   */</span></div><div class="line">   <span class="built_in">cout</span>&lt;&lt;sum&lt;&lt;<span class="built_in">endl</span>;</div><div class="line"> &#125;</div><div class="line"> <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul></blockquote><ul><li>运行结果<br><img src="http://upload-images.jianshu.io/upload_images/1531909-a014db737d1ae27d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p> <a href="http://blog.csdn.net/liufeng_king/article/details/8709005" target="_blank" rel="external">0021算法笔记——【贪心算法】贪心算法与活动安排问题</a><br> <a href="http://blog.csdn.net/pukuimin1226/article/details/6440714" target="_blank" rel="external">C++最小生成树问题</a><br><a href="https://wenku.baidu.com/view/8e5f335b77232f60ddcca144.html" target="_blank" rel="external">C++c语言贪心算法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;贪心算法的介绍以及背包问题，活动安排问题，最小生成树问题三大实例的解析&lt;br&gt;
    
    </summary>
    
    
      <category term="贪心算法" scheme="http://yoursite.com/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
    
      <category term="背包问题" scheme="http://yoursite.com/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    
      <category term="活动安排问题" scheme="http://yoursite.com/tags/%E6%B4%BB%E5%8A%A8%E5%AE%89%E6%8E%92%E9%97%AE%E9%A2%98/"/>
    
      <category term="最小生成树" scheme="http://yoursite.com/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>求一个数n次方后的末尾数（数论/快速幂）</title>
    <link href="http://yoursite.com/2017/08/30/09-%E5%BF%AB%E9%80%9F%E5%B9%82/"/>
    <id>http://yoursite.com/2017/08/30/09-快速幂/</id>
    <published>2017-08-30T23:14:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>如何快速求出一个数n次方后的末尾数为多少<br><a id="more"></a></excerpt></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p><a href="http://acm.hdu.edu.cn/showproblem.php?pid=1061" target="_blank" rel="external">hdu1061-Rightmost Digit</a><br><a href="http://acm.hdu.edu.cn/showproblem.php?pid=1097" target="_blank" rel="external">hdu1097-A hard puzzle</a><br>这两个oj题目思路几乎一样，都是为了快速求出一个数n次方后的末尾数为多少？</p><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><blockquote><p>1的所有次方都是1<br>0的所有次方都是0<br>5的所有次方都是5<br>6的所有次方都是6<br>2^1=2 2^2=4 2^3=8 2^4=6(四个一循环)<br>3^1=3 3^2=9 3^3=7 3^4=1(四个一循环)<br>7^1=7 7^2=9 7^3=3 7^4=1(四个一循环)<br>4^1=4 4^2=6(两个一循环)<br>8^1=8 8^2=4(两个一循环)<br>9^1=9 9^2=1(两个一循环)</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>下面以<a href="http://acm.hdu.edu.cn/showproblem.php?pid=1097" target="_blank" rel="external">hdu1097-A hard puzzle</a>为例</p><ul><li>代码1（自己写的傻乎乎）<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> m,n,last;</div><div class="line"></div><div class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;m&gt;&gt;n)&#123;</div><div class="line">        last=m%<span class="number">10</span>;</div><div class="line">        <span class="keyword">if</span>(last==<span class="number">0</span>||last==<span class="number">1</span>||last==<span class="number">5</span>||last==<span class="number">6</span>)&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;last&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">4</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">2</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">4</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">2</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">6</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">9</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">2</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">9</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">2</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">1</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">2</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">2</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">2</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">4</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">3</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">8</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">6</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">3</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">3</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">2</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">9</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">3</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">7</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">1</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">7</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">7</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">2</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">9</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">3</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">3</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">1</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(last==<span class="number">8</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">1</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">8</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">2</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">4</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">3</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">2</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span>(n%<span class="number">4</span>==<span class="number">0</span>)&#123;</div><div class="line">                <span class="built_in">cout</span>&lt;&lt;<span class="number">6</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line">`</div></pre></td></tr></table></figure></li></ul></blockquote><ul><li><p>代码2</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> a[<span class="number">10</span>] = &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>&#125;;</div><div class="line">    <span class="keyword">int</span> n, num, rmd, ans; <span class="comment">// rmd = rightmost digit</span></div><div class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</div><div class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;num);</div><div class="line">        rmd = num % <span class="number">10</span>;</div><div class="line">        ans = (<span class="keyword">int</span>) <span class="built_in">pow</span>(rmd, num % a[rmd] ? num % a[rmd] : a[rmd]);</div><div class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, ans % <span class="number">10</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>代码3</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;//1097</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;    <span class="keyword">int</span> a,b,c[<span class="number">4</span>];</div><div class="line">  <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b)</div><div class="line">  &#123;</div><div class="line">      a=a%<span class="number">10</span>;</div><div class="line">      c[<span class="number">0</span>]=a;<span class="comment">//一次方的末尾数</span></div><div class="line">     c[<span class="number">1</span>]=(c[<span class="number">0</span>]*a)%<span class="number">10</span>;<span class="comment">//二次方的末尾数</span></div><div class="line">     c[<span class="number">2</span>]=(c[<span class="number">1</span>]*a)%<span class="number">10</span>;<span class="comment">//三次方的末尾数</span></div><div class="line">     c[<span class="number">3</span>]=(c[<span class="number">2</span>]*a)%<span class="number">10</span>;<span class="comment">//四次方的末尾数</span></div><div class="line">     <span class="keyword">if</span>(b%<span class="number">4</span>==<span class="number">1</span>)</div><div class="line">         <span class="built_in">cout</span>&lt;&lt;c[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">     <span class="keyword">if</span>(b%<span class="number">4</span>==<span class="number">2</span>)</div><div class="line">         <span class="built_in">cout</span>&lt;&lt;c[<span class="number">1</span>]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">     <span class="keyword">if</span>(b%<span class="number">4</span>==<span class="number">3</span>)</div><div class="line">         <span class="built_in">cout</span>&lt;&lt;c[<span class="number">2</span>]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">     <span class="keyword">if</span>(b%<span class="number">4</span>==<span class="number">0</span>)</div><div class="line">         <span class="built_in">cout</span>&lt;&lt;c[<span class="number">3</span>]&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p><img src="http://upload-images.jianshu.io/upload_images/1531909-9d4ad70adc387245.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="运行及结果"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://segmentfault.com/a/1190000006995186" target="_blank" rel="external">ACM — Rightmost Digit</a><br><a href="http://blog.csdn.net/zuguodexiaoguoabc/article/details/43762335" target="_blank" rel="external">A hard puzzle</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;如何快速求出一个数n次方后的末尾数为多少&lt;br&gt;
    
    </summary>
    
    
      <category term="数论" scheme="http://yoursite.com/tags/%E6%95%B0%E8%AE%BA/"/>
    
      <category term="快速幂" scheme="http://yoursite.com/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/"/>
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>图的广度优先搜索和图的深度优先搜索</title>
    <link href="http://yoursite.com/2017/08/30/08-bfs&amp;&amp;dfs/"/>
    <id>http://yoursite.com/2017/08/30/08-bfs&amp;&amp;dfs/</id>
    <published>2017-08-30T15:29:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>图的邻接链表的表示方法、图的广度优先搜索和图的深度优先搜索<br><a id="more"></a></excerpt></p><h1 id="邻接链表"><a href="#邻接链表" class="headerlink" title="邻接链表"></a>邻接链表</h1><blockquote><p>邻接表表示法将图以邻接表（adjacency  lists）的形式存储在计算机中。所谓图的邻接表，也就是图的所有节点的邻接表的集合；而对每个节点，它的邻接表就是它的所有出弧。邻接表表示法就是对图的每个节点，用一个单向链表列出从该节点出发的所有弧，链表中每个单元对应于一条出弧。为了记录弧上的权，链表中每个单元除列出弧的另一个端点外，还可以包含弧上的权等作为数据域。图的整个邻接表可以用一个指针数组表示。例如下图所示，邻接表表示为</p></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/1531909-e2e11cfa815bf198.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="邻接链表"></p><h1 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h1><h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><ol><li>把根节点放到队列的末尾。</li><li>每次从队列的头部取出一个元素，查看这个元素所有的下一级元素，把它们放到队列的末尾。并把这个元素记为它下一级元素的前驱。</li><li>找到所要找的元素时结束程序。</li><li>如果遍历整个树还没有找到，结束程序。<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//http://www.geeksforgeeks.org/breadth-first-traversal-for-a-graph/</span></div><div class="line"><span class="comment">// Program to print BFS traversal from a given source vertex. BFS(int s)</span></div><div class="line"><span class="comment">// traverses vertices reachable from s.</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// This class represents a directed graph using adjacency list representation</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    <span class="keyword">int</span> V;    <span class="comment">// No. of vertices</span></div><div class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; *adj;    <span class="comment">// Pointer to an array containing adjacency lists</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    Graph(<span class="keyword">int</span> V);  <span class="comment">// Constructor</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addEdge</span><span class="params">(<span class="keyword">int</span> v, <span class="keyword">int</span> w)</span></span>; <span class="comment">// function to add an edge to graph</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> s)</span></span>;  <span class="comment">// prints BFS traversal from a given source s</span></div><div class="line">&#125;;</div><div class="line"></div><div class="line">Graph::Graph(<span class="keyword">int</span> V)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">this</span>-&gt;V = V;</div><div class="line">    adj = <span class="keyword">new</span> <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;[V];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span> Graph::addEdge(<span class="keyword">int</span> v, <span class="keyword">int</span> w)</div><div class="line">&#123;</div><div class="line">    adj[v].push_back(w); <span class="comment">// Add w to v’s list.</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span> Graph::BFS(<span class="keyword">int</span> s)</div><div class="line">&#123;</div><div class="line">    <span class="comment">// Mark all the vertices as not visited</span></div><div class="line">    <span class="keyword">bool</span> *visited = <span class="keyword">new</span> <span class="keyword">bool</span>[V];</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; V; i++)</div><div class="line">        visited[i] = <span class="literal">false</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Create a queue for BFS</span></div><div class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; <span class="built_in">queue</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Mark the current node as visited and enqueue it</span></div><div class="line">    visited[s] = <span class="literal">true</span>;</div><div class="line">    <span class="built_in">queue</span>.push_back(s);</div><div class="line"></div><div class="line">    <span class="comment">// 'i' will be used to get all adjacent vertices of a vertex</span></div><div class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;::iterator i;</div><div class="line"></div><div class="line">    <span class="keyword">while</span>(!<span class="built_in">queue</span>.empty())</div><div class="line">    &#123;</div><div class="line">        <span class="comment">// Dequeue a vertex from queue and print it</span></div><div class="line">        s = <span class="built_in">queue</span>.front();</div><div class="line">        <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="string">" "</span>;</div><div class="line">        <span class="built_in">queue</span>.pop_front();</div><div class="line"></div><div class="line">        <span class="comment">// Get all adjacent vertices of the dequeued vertex s</span></div><div class="line">        <span class="comment">// If a adjacent has not been visited, then mark it visited</span></div><div class="line">        <span class="comment">// and enqueue it</span></div><div class="line">        <span class="keyword">for</span>(i = adj[s].begin(); i != adj[s].end(); ++i)</div><div class="line">        &#123;</div><div class="line">            <span class="keyword">if</span>(!visited[*i])</div><div class="line">            &#123;</div><div class="line">                visited[*i] = <span class="literal">true</span>;</div><div class="line">                <span class="built_in">queue</span>.push_back(*i);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Driver program to test methods of graph class</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">// Create a graph given in the above diagram</span></div><div class="line">    <span class="function">Graph <span class="title">g</span><span class="params">(<span class="number">4</span>)</span></span>;</div><div class="line">    g.addEdge(<span class="number">0</span>, <span class="number">1</span>);</div><div class="line">    g.addEdge(<span class="number">0</span>, <span class="number">2</span>);</div><div class="line">    g.addEdge(<span class="number">1</span>, <span class="number">2</span>);</div><div class="line">    g.addEdge(<span class="number">2</span>, <span class="number">0</span>);</div><div class="line">    g.addEdge(<span class="number">2</span>, <span class="number">3</span>);</div><div class="line">    g.addEdge(<span class="number">3</span>, <span class="number">3</span>);</div><div class="line"></div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Following is Breadth First Traversal "</span></div><div class="line">         &lt;&lt; <span class="string">"(starting from vertex 2) n:"</span>;</div><div class="line">    g.BFS(<span class="number">2</span>);</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ol><h1 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h1><h2 id="基本思路-1"><a href="#基本思路-1" class="headerlink" title="基本思路"></a>基本思路</h2><ol><li>访问顶点v；</li><li>依次从v的未被访问的邻接点出发，对图进行深度优先遍历；直至图中和v有路径相通的顶点都被访问；</li><li>若此时图中尚有顶点未被访问，则从一个未被访问的顶点出发，重新进行深度优先遍历<h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//http://www.geeksforgeeks.org/depth-first-traversal-for-a-graph/</span></div><div class="line"><span class="comment">// C++ program to print DFS traversal from a given vertex in a  given graph</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;list&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// Graph class represents a directed graph using adjacency list representation</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    <span class="keyword">int</span> V;    <span class="comment">// No. of vertices</span></div><div class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; *adj;    <span class="comment">// Pointer to an array containing adjacency lists</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">DFSUtil</span><span class="params">(<span class="keyword">int</span> v, <span class="keyword">bool</span> visited[])</span></span>;  <span class="comment">// A function used by DFS</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    Graph(<span class="keyword">int</span> V);   <span class="comment">// Constructor</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addEdge</span><span class="params">(<span class="keyword">int</span> v, <span class="keyword">int</span> w)</span></span>;   <span class="comment">// function to add an edge to graph</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> v)</span></span>;    <span class="comment">// DFS traversal of the vertices reachable from v</span></div><div class="line">&#125;;</div><div class="line"></div><div class="line">Graph::Graph(<span class="keyword">int</span> V)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">this</span>-&gt;V = V;</div><div class="line">    adj = <span class="keyword">new</span> <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;[V];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span> Graph::addEdge(<span class="keyword">int</span> v, <span class="keyword">int</span> w)</div><div class="line">&#123;</div><div class="line">    adj[v].push_back(w); <span class="comment">// Add w to v’s list.</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span> Graph::DFSUtil(<span class="keyword">int</span> v, <span class="keyword">bool</span> visited[])</div><div class="line">&#123;</div><div class="line">    <span class="comment">// Mark the current node as visited and print it</span></div><div class="line">    visited[v] = <span class="literal">true</span>;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="string">" "</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Recur for all the vertices adjacent to this vertex</span></div><div class="line">    <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;::iterator i;</div><div class="line">    <span class="keyword">for</span> (i = adj[v].begin(); i != adj[v].end(); ++i)</div><div class="line">        <span class="keyword">if</span> (!visited[*i])</div><div class="line">            DFSUtil(*i, visited);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// DFS traversal of the vertices reachable from v.</span></div><div class="line"><span class="comment">// It uses recursive DFSUtil()</span></div><div class="line"><span class="keyword">void</span> Graph::DFS(<span class="keyword">int</span> v)</div><div class="line">&#123;</div><div class="line">    <span class="comment">// Mark all the vertices as not visited</span></div><div class="line">    <span class="keyword">bool</span> *visited = <span class="keyword">new</span> <span class="keyword">bool</span>[V];</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; V; i++)</div><div class="line">        visited[i] = <span class="literal">false</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Call the recursive helper function to print DFS traversal</span></div><div class="line">    DFSUtil(v, visited);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">// Create a graph given in the above diagram</span></div><div class="line">    <span class="function">Graph <span class="title">g</span><span class="params">(<span class="number">4</span>)</span></span>;</div><div class="line">    g.addEdge(<span class="number">0</span>, <span class="number">1</span>);</div><div class="line">    g.addEdge(<span class="number">0</span>, <span class="number">2</span>);</div><div class="line">    g.addEdge(<span class="number">1</span>, <span class="number">2</span>);</div><div class="line">    g.addEdge(<span class="number">2</span>, <span class="number">0</span>);</div><div class="line">    g.addEdge(<span class="number">2</span>, <span class="number">3</span>);</div><div class="line">    g.addEdge(<span class="number">3</span>, <span class="number">3</span>);</div><div class="line"></div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Following is Depth First Traversal (starting from vertex 2) n:"</span>;</div><div class="line">    g.DFS(<span class="number">2</span>);</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ol><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p><img src="http://upload-images.jianshu.io/upload_images/1531909-df3bc225970ef1bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="输入"></p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-c28e0034fe262c31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="广度优先搜索"></p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-0ac37d322b53006b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="深度优先搜索"></p><p>也可以试试从其他定点（0,1,3）开始遍历☺<br><em>参考</em><br><a href="http://blog.csdn.net/dextrad_ihacker/article/details/50132129" target="_blank" rel="external">初识图，图的存储（邻接矩阵，邻接链表）和深搜遍历</a><br><a href="http://www.cnblogs.com/liushang0419/archive/2011/05/06/2039386.html" target="_blank" rel="external">算法与数据结构（2）——图的表示法与常用的转化算法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;图的邻接链表的表示方法、图的广度优先搜索和图的深度优先搜索&lt;br&gt;
    
    </summary>
    
    
      <category term="dfs" scheme="http://yoursite.com/tags/dfs/"/>
    
      <category term="bfs" scheme="http://yoursite.com/tags/bfs/"/>
    
      <category term="广度优先搜索" scheme="http://yoursite.com/tags/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/"/>
    
      <category term="深度优先搜索" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>括号匹配问题</title>
    <link href="http://yoursite.com/2017/08/30/07-%E6%8B%AC%E5%8F%B7%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2017/08/30/07-括号匹配问题/</id>
    <published>2017-08-30T11:42:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>这篇文章主要介绍了c++栈（stack）的应用以及括号匹配问题的实现<br><a id="more"></a></excerpt></p><p><a href="http://acm.nyist.net/JudgeOnline/problem.php?pid=2" target="_blank" rel="external">括号配对问题-题目链接</a></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>现在，有一行括号序列，请你检查这行括号是否配对。</p><h1 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h1><p>第一行输入一个数N（0&lt;N&lt;=100）,表示有N组测试数据。后面的N行输入多组输入数据，每组输入数据都是一个字符串S(S的长度小于10000，且S不是空串），测试数据组数少于5组。数据保证S中只含有”[“,”]”,”(“,”)”四种字符</p><h1 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h1><p>每组输入数据的输出占一行，如果该字符串中所含的括号是配对的，则输出Yes,如果不配对则输出No</p><h1 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">3</div><div class="line">[(])</div><div class="line">(])</div><div class="line">([[]()])</div></pre></td></tr></table></figure><h1 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">No</div><div class="line">No</div><div class="line">Yes</div></pre></td></tr></table></figure><h1 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h1><ul><li>顺序扫描括号字符串中每一个字符,当遇到栈空或者遇到左括号时该括号进栈；</li><li>当扫描到某一种类型的右括号时，比较当前栈顶元素是否与之匹配，若匹配，出栈继续判断；</li><li>若当前栈顶元素与当前扫描的右括号括号不匹配，则将该右括号进栈(此时已经匹配失败)；</li><li>若最终栈为空，则括号匹配成功，如果不为空，则不成功；<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">int</span> n;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;n;</div><div class="line">    <span class="keyword">while</span>(n--)&#123;</div><div class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">char</span>&gt; s;</div><div class="line">        <span class="built_in">string</span> ch;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;ch;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;ch.length();i++)&#123;</div><div class="line">            <span class="keyword">if</span>(s.empty())&#123;<span class="comment">//如果栈为空继续进栈</span></div><div class="line"></div><div class="line">                s.push(ch[i]);</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                <span class="keyword">if</span>(ch[i]==<span class="string">'('</span>||ch[i]==<span class="string">'['</span>) s.push(ch[i]);</div><div class="line">                <span class="keyword">else</span>&#123;</div><div class="line">                    <span class="keyword">if</span>(ch[i]==<span class="string">')'</span>)&#123;</div><div class="line">                        <span class="keyword">if</span>(s.top()==<span class="string">'('</span>)s.pop();</div><div class="line">                        <span class="keyword">else</span> s.push(ch[i]);</div><div class="line">                    &#125;</div><div class="line">                    <span class="keyword">if</span>(ch[i]==<span class="string">']'</span>)&#123;</div><div class="line">                        <span class="keyword">if</span>(s.top()==<span class="string">'['</span>)s.pop();</div><div class="line">                        <span class="keyword">else</span> s.push(ch[i]);</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(s.empty())&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"Yes"</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">        &#125;<span class="keyword">else</span>&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"No"</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p><img src="http://upload-images.jianshu.io/upload_images/1531909-60c6f4e57489130d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="运行结果"></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p> <a href="http://blog.csdn.net/the_victory/article/details/52733985" target="_blank" rel="external">[NYOJ] 02括号配对问题(c语言链栈实现)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;这篇文章主要介绍了c++栈（stack）的应用以及括号匹配问题的实现&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>卡特兰数</title>
    <link href="http://yoursite.com/2017/08/24/06-%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/"/>
    <id>http://yoursite.com/2017/08/24/06-卡特兰数/</id>
    <published>2017-08-24T16:00:25.000Z</published>
    <updated>2017-09-15T09:31:20.278Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>这篇文章主要介绍了卡特兰数、推导和应用<br><a id="more"></a></excerpt></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>卡特兰数又称卡塔兰数，卡特兰数是组合数学中一个常出现在各种计数问题中的数列。<br>卡塔兰数的一般项公式为：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-0c81412304254b5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="卡特兰公式"><br>其前20项为：1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><ol><li>令h(0)=1,h(1)=1，catalan数满足递推式：<br><code>h(n)= h(0)*h(n-1)+h(1)*h(n-2) + ... + h(n-1)*h(0) (n&gt;=2)</code><br>例如：<br><code>h(2)=h(0)*h(1)+h(1)*h(0)=1*1+1*1=2</code><br><code>h(3)=h(0)*h(2)+h(1)*h(1)+h(2)*h(0)=1*2+1*1+2*1=5</code></li><li>另类递推式[3]：<br><code>h(n)=h(n-1)*(4*n-2)/(n+1)</code></li><li>递推关系的解为：<br><code>h(n)=C(2n,n)/(n+1) (n=0,1,2,...)</code></li><li>递推关系的另类解为：<br><code>h(n)=c(2n,n)-c(2n,n-1)(n=0,1,2,...)</code></li></ol><h1 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h1><ol><li>卡特兰数的另一个表达形式为：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-95a9185528096891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="表现形式"><br>所以，卡特兰数是一个自然数；这一点在先前的通项公式中并不显而易见。</li><li>递推关系</li></ol><p><img src="http://upload-images.jianshu.io/upload_images/1531909-5a121b63f73a0c5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="递推1"></p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-327a0abcdd8afa89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="递推2"><br>这是一个比较快速的计算卡特兰数的方法。</p><ol><li>卡特兰数的渐进增长</li></ol><p><img src="http://upload-images.jianshu.io/upload_images/1531909-226df3fd4f0c29bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="渐进增长"><br>它的含义是当n→ ∞时，左式除以右式的商趋向于1。</p><ol><li>所有的奇卡塔兰数Cn都满足：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-a634b4c05f99a0e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="奇卡塔兰数"><br>所有其他的卡塔兰数都是偶数。<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1></li></ol><ul><li><p>dyck word</p><p>Cn 表示长度2n的<code>dyck word</code>的个数。<code>Dyck word</code>是一个有<code>n</code>个X 和<code>n</code> 个Y 组成的字串，且所有的前缀字串皆满足X 的个数大于等于Y 的个数。以下为长度为6的<code>dyck words</code>:<br> <code>XXXYYY XYXXYY XYXYXY XXYYXY XXYXYY</code></p></li><li><p>n对括号正确匹配数目<br>将上例的X换成左括号，Y换成右括号，Cn表示所有包含n组括号的合法运算式的个数：<br><code>((())) ()(()) ()()() (())() (()())</code></p></li></ul><blockquote><p>给定n对括号，求括号正确配对的字符串数，例如：<br>0对括号：[空序列] 1种可能<br>1对括号：() 1种可能<br>2对括号：()() (()) 2种可能<br>3对括号：((())) ()(()) ()()() (())() (()()) 5种可能<br>那么问题来了，n对括号有多少种正确配对的可能呢？<br>考虑n对括号时的任意一种配对方案，最后一个右括号有唯一的与之匹配的左括号，于是有唯一的表示A(B)，其中A和B也是合法的括号匹配序列<br>假设S(n)为n对括号的正确配对数目，那么有递推关系S(n)=S(0)S(n-1)+S(1)S(n-2) +…+S(n-1)S(0)，显然S(n)是卡特兰数。</p></blockquote><ul><li>括号化</li></ul><p>矩阵连乘： P=a1×a2×a3×……×an，依据乘法结合律，不改变其顺序，只用括号表示成对的乘积，试问有几种括号化的方案？(h(n)种)</p><ul><li>出栈次序</li></ul><p>一个栈无穷大的进栈序列为1，2，3，…，n，有多少个不同的出栈序列?<br><code>分析：</code></p><blockquote><p>首先，我们设f（n）=序列个数为n的出栈序列种数。（我们假定，最后出栈的元素为k，显然，k取不同值时的情况是相互独立的，也就是求出每种k最后出栈的情况数后可用加法原则，由于k最后出栈，因此，在k入栈之前，比k小的值均出栈，此处情况有f(k-1)种，而之后比k大的值入栈，且都在k之前出栈，因此有f(n-k)种方式，由于比k小和比k大的值入栈出栈情况是相互独立的，此处可用乘法原则，f(n-k)*f(k-1)种，求和便是Catalan递归式。<br>首次出空之前第一个出栈的序数k将1~n的序列分成两个序列，其中一个是1 <code>~</code> k-1，序列个数为k-1，另外一个是k+1~n，序列个数是n-k。<br>此时，我们若把k视为确定一个序数，那么根据乘法原理，f（n）的问题就等价于——序列个数为k-1的出栈序列种数乘以序列个数为n - k的出栈序列种数，即选择k这个序数的f（n）=f（k-1）×f（n-k）。而k可以选1到n，所以再根据加法原理，将k取不同值的序列种数相加，得到的总序列种数为：<code>f（n）=f（0）f（n-1）+f（1）f（n-2）+……+f（n-1）f（0）</code>。<br>看到此处，再看看卡特兰数的递推式，答案不言而喻，即为<code>f（n）=h（n）= C（2n,n）/（n+1）= c（2n,n）-c（2n,n-1）（n=0，1，2，……）</code>。<br>  最后，令<code>f（0）=1，f（1）=1</code>。</p></blockquote><p> <code>相似问题-买票找零</code></p><blockquote><p>有2n个人排成一行进入剧场。入场费5元。其中只有n个人有一张5元钞票，另外n人只有10元钞票，剧院无其它钞票，问有多少中方法使得只要有10元的人买票，售票处就有5元的钞票找零？(将持5元者到达视作将5元入栈，持10元者到达视作使栈中某5元出栈)</p></blockquote><ul><li>凸多边形三角划分</li></ul><p>Cn表示通过连结顶点而将<em>n</em> + 2边的凸多边形分成三角形的方法个数。下图中为<em>n</em> = 4的情况：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-02e672283a30f88b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="凸多边形三角划分"><br><code>分析 :</code></p><blockquote><p>如果纯粹从f（4）=2，f（5）=5，f（6）=14，……，f（n）=n慢慢去归纳，恐怕很难找到问题的递推式，我们必须从一般情况出发去找规律。<br>因为凸多边形的任意一条边必定属于某一个三角形，所以我们以某一条边为基准，以这条边的两个顶点为起点P1和终点Pn（P即Point），将该凸多边形的顶点依序标记为P1、P2、……、Pn，再在该凸多边形中找任意一个不属于这两个点的顶点Pk（2&lt;=k&lt;=n-1），来构成一个三角形，用这个三角形把一个凸多边形划分成两个凸多边形，其中一个凸多边形，是由P1，P2，……，Pk构成的凸k边形（顶点数即是边数），另一个凸多边形，是由Pk，Pk+1，……，Pn构成的凸n-k+1边形。<br>此时，我们若把Pk视为确定一点，那么根据乘法原理，f（n）的问题就等价于——凸k多边形的划分方案数乘以凸n-k+1多边形的划分方案数，即选择Pk这个顶点的f（n）=f（k）×f（n-k+1）。而k可以选2到n-1，所以再根据加法原理，将k取不同值的划分方案相加，得到的总方案数为：f（n）=f（2）f（n-2+1）+f（3）f（n-3+1）+……+f（n-1）f（2）。看到此处，再看看卡特兰数的递推式，答案不言而喻，即为f（n）=h（n-2） （n=2，3，4，……）。<br>最后，令f（2）=1，f（3）=1。</p></blockquote><p><code>类似问题 ：</code></p><blockquote><p>一位大城市的律师在她住所以北n个街区和以东n个街区处工作。每天她走2n个街区去上班。如果她从不穿越（但可以碰到）从家到办公室的对角线，那么有多少条可能的道路？</p><p>在圆上选择2n个点,将这些点成对连接起来使得所得到的n条线段不相交的方法数？</p></blockquote><ul><li>给定n个节点组成不同的二叉树个数<br>Cn表示有n个节点组成不同构二叉树的方案数。下图中，n等于3，圆形表示节点，月牙形表示什么都没有。</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-f17b64d3c1c49573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="二叉树个数"></p><ul><li>Cn表示所有在n × n格点中不越过对角线的单调路径的个数。一个单调路径从格点左下角出发，在格点右上角结束，每一步均为向上或向右。计算这种路径的个数等价于计算Dyck word的个数：X代表“向右”，Y代表“向上”。下图为n = 4的情况：</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-313c12b171a5f63c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="单调路径的个数"></p><ul><li>Cn表示用n个长方形填充一个高度为n的阶梯状图形的方法个数。下图为n = 4的情况：</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-7400fb385f18363b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="阶梯状图形的方法个数"></p><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p><a href="https://baike.baidu.com/item/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0#7" target="_blank" rel="external">卡特兰数-百度百科</a><br><a href="https://zh.wikipedia.org/wiki/%E5%8D%A1%E5%A1%94%E5%85%B0%E6%95%B0" target="_blank" rel="external">卡塔兰数-维基百科</a><br><a href="http://blog.csdn.net/wuzhekai1985/article/details/6764858" target="_blank" rel="external">Catalan数计算及应用</a><br><a href="http://blog.csdn.net/lishuhuakai/article/details/8034075" target="_blank" rel="external">杭电1023——Train Problem II</a><br><a href="http://blog.csdn.net/jtlyuan/article/details/7440591" target="_blank" rel="external">2012腾讯实习笔试中看到的Catalan数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;这篇文章主要介绍了卡特兰数、推导和应用&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>通过递归的矩阵向量空间预测组合语义</title>
    <link href="http://yoursite.com/2017/08/06/05-%20Matrix-Vector/"/>
    <id>http://yoursite.com/2017/08/06/05- Matrix-Vector/</id>
    <published>2017-08-06T16:31:25.000Z</published>
    <updated>2017-09-15T09:31:20.263Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>关于论文的总结<br><a id="more"></a></excerpt></p><p><a href="http://www.socher.org/index.php/Main/SemanticCompositionalityThroughRecursiveMatrix-VectorSpaces" target="_blank" rel="external">Semantic Compositionality Through Recursive Matrix-Vector Spaces</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>单字矢量空间模型已经在学习词汇信息方面非常成功。但是，它们无法捕捉到更长的短语的位置意义，这样就阻碍了它们对语言的深入理解。我们介绍一种递归神经网络（RNN）模型，该模型学习任意句法类型和长度的短语和句子的组合向量表示。我们的模型为解析树中的每个节点分配向量和矩阵：向量捕获组成部分的固有含义，而矩阵捕获它如何改变相邻单词或短语的含义。这种矩阵向量RNN可以学习命题逻辑的运算符和自然语言的含义。该模型在三个不同的实验中获得最显著的表现：预测副词形容词对的细粒度情感分布;对电影评论的情感标签进行分类，并使用他们之间的句法路径对名词之间的因果关系或主题信息进行分类。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>语义词向量空间是许多有用的自然语言应用的核心，例如搜索查询扩展（Jones et al。2006），信息检索的事实提取（Pas¸caet al。2006）和消歧的文本自动注释带有的维基百科链接（Ratinov et al。2011）等等（Turney和Pantel。2010）。在这些模型中，单词的含义被编码为从单词及其相邻单词的共现统计中计算出的向量。这些向量已经表明它们与人类对词相似性的判断有很好的相关性（Griffiths et al。2007）。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="http://upload-images.jianshu.io/upload_images/1531909-4e65add68e8839c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="方法.png"></p><h2 id="二分法解析树"><a href="#二分法解析树" class="headerlink" title="二分法解析树"></a>二分法解析树</h2><p><img src="http://upload-images.jianshu.io/upload_images/1531909-2cabd92402d45add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="二分法解析树.png"></p><blockquote><p>The song was composed by as famous Indian musician</p></blockquote><h2 id="递归矩阵向量模型"><a href="#递归矩阵向量模型" class="headerlink" title="递归矩阵向量模型"></a>递归矩阵向量模型</h2><p><img src="http://upload-images.jianshu.io/upload_images/1531909-b198ecca3d36e95f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="递归矩阵向量模型.png"></p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ul><li>用预先训练的50维词向量初始化所有的单词向量</li><li>将矩阵初始化为X=I+ε，其中I�是实体矩阵<h3 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h3><img src="http://upload-images.jianshu.io/upload_images/1531909-3e8bfbfc08f12ad6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="组合.png"><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2>我们通过在每个父节点顶部添加一个softmax分类器来训练向量表示，以一种情感分类或一些关系分类<br><img src="http://upload-images.jianshu.io/upload_images/1531909-b414b509852fda04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="softmax.png"><blockquote><p>其中W label∈R K×n是权重矩阵。如果有K个标签，则d∈RK是K维多项式分布</p></blockquote></li></ul><p>我们将t（x）∈RK×1表示为节点x处的目标分布向量,t（x）具有0-1编码：t（x）处的条目为1，其余条目为0.后计算d（x）和t（x）之间的交叉熵误差。<br><img src="http://upload-images.jianshu.io/upload_images/1531909-978a597c1be47c4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="交叉熵.png"><br>并将目标函数定义为所有训练数据上的E（x）之和：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-dbbcadf4712bd6db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="QQ截图20170807151929.png"></p><blockquote><p>其中θ=（W，W M，W label，L，L M）是我们应该学习的模型参数的集合。 λ是正则化参数的向量.L和L M分别是字矢量和字矩阵的集合。</p></blockquote><h2 id="语义关系分类"><a href="#语义关系分类" class="headerlink" title="语义关系分类"></a>语义关系分类</h2><ul><li>我们首先在解析树之间找到我们要分类的关系的两个单词之间的路径。</li><li>然后，我们选择路径的最高节点，并使用该节点的向量作为特征对关系进行分类。</li><li>最后，我们将MV-RNN模型应用于由两个单词所跨越的子树。</li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1531909-1d0a4d2d89815601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="语义关系分类.png"></p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>我们对以下数据集进行了实验：</p><ul><li>SemEval 2010 Task 8<br>有9个有序的关系（有两个方向）和一个无向的其他类，所以一共有19个类。 这些关系有：信息主题，因果关系，工具代理。 如果关系中的单词的顺序正确，则对将其计为正确。<br><img src="http://upload-images.jianshu.io/upload_images/1531909-55c1e5bd2ff97e51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SemEval 2010 Task8.png"><ol><li>Accuracy (calculated for the above confusion matrix)= 2094/2717 =77.07％</li><li>F1_score= 82.51％</li><li>我们还使用根据“SemEval 2007 Task 4”的代码要求修改的不同数据集来执行测试并使用以前的培训模型</li><li>该实验的F1得分为40.08％，忽略方向性。</li></ol></li></ul><h2 id="与其他办法的对比"><a href="#与其他办法的对比" class="headerlink" title="与其他办法的对比"></a>与其他办法的对比</h2><p><img src="http://upload-images.jianshu.io/upload_images/1531909-9a1608415535eec8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="对比.png"><br>结果的改善也是由于其他方法的一些常见缺点。 例如：<br>•许多方法用无序的单词列表来表示文本，而情绪不仅取决于单词的含义，而且还取决于它们的顺序。<br>•使用的功能是手动开发的，不一定会捕获该单词的所有功能。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul><li>我们的模型建立在语法上合理的解析树上，可以处理组合现象。</li><li>我们的模型的主要新颖性是矩阵向量表示与递归神经网络的组合。</li><li>它可以学习一个单词的意义向量，以及该单词如何修改其邻居（通过其矩阵）。</li><li>MV-RNN将有吸引力的理论性能与大型噪声数据集的良好性能相结合。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;关于论文的总结&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Laravel博客实战</title>
    <link href="http://yoursite.com/2017/08/04/04-Laravel%E5%8D%9A%E5%AE%A2%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/08/04/04-Laravel博客实战/</id>
    <published>2017-08-04T17:31:25.000Z</published>
    <updated>2017-09-15T09:31:20.263Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>用了5天时间，参考后盾网老师的Laravel实战视频做的博客<br><a id="more"></a></excerpt></p><blockquote><p>最近在学习Laravel，参考的课程是后盾网地Laravel5.2博客项目实战，地址是<br><a href="http://bbs.houdunwang.com/forum-247-1.html" target="_blank" rel="external">Laravel 5.2开发实</a></p></blockquote><p>下面整个项目的开发过程：</p><h1 id="laravel-blog"><a href="#laravel-blog" class="headerlink" title="laravel-blog"></a>laravel-blog</h1><p>基于laravel5.2的博客</p><h2 id="day1（7月31）："><a href="#day1（7月31）：" class="headerlink" title="day1（7月31）："></a>day1（7月31）：</h2><ol><li>后台模板引入</li><li>验证码</li><li>表单验证</li><li>后台权限和密码更改</li><li>文章分类</li></ol><h2 id="day2（8月01）"><a href="#day2（8月01）" class="headerlink" title="day2（8月01）:"></a>day2（8月01）:</h2><ol><li>文章多级分类以及父分类</li><li>ajax修改排序</li><li>文章分类添加</li><li>文章分类编辑</li><li>文章分类ajax异步删除</li></ol><h2 id="day3（8月02）"><a href="#day3（8月02）" class="headerlink" title="day3（8月02）:"></a>day3（8月02）:</h2><ol><li>文章添加以及百度编辑器Ueditor嵌入</li><li>文章缩略图上传之uploadify（HTML5版本）的引入</li><li>文章分页列表</li><li>文章编辑</li><li>文章删除</li></ol><h2 id="day4（8月03）"><a href="#day4（8月03）" class="headerlink" title="day4（8月03）:"></a>day4（8月03）:</h2><ol><li>数据库迁移以及数据填充</li><li>友情链接增删改查</li><li>自定义导航</li><li>前台文章首页、列表页、文章模板</li><li>前台模板数据共享</li></ol><h2 id="day5（8月04）"><a href="#day5（8月04）" class="headerlink" title="day5（8月04）"></a>day5（8月04）</h2><ol><li>配置项模块的创建</li><li>最新文章以及点击排行</li><li>公共侧边栏模板继承</li><li>文章页面信息以及详情</li><li>文章上一篇下一篇以及相关文章</li></ol><p><a href="https://github.com/yanqiangmiffy/laravel-blog" target="_blank" rel="external">项目地址</a></p><hr><h1 id="最终的效果"><a href="#最终的效果" class="headerlink" title="最终的效果"></a>最终的效果</h1><p><img src="http://upload-images.jianshu.io/upload_images/1531909-c088728dbc0aeef0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="前台.png"><br><img src="http://upload-images.jianshu.io/upload_images/1531909-5cf281d1d80cacd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="文章详情页.png"><br><img src="http://upload-images.jianshu.io/upload_images/1531909-4c956a5d9193e149.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="管理页面.png"></p><hr><h1 id="踩的坑"><a href="#踩的坑" class="headerlink" title="踩的坑"></a>踩的坑</h1><blockquote><h2 id="关于session"><a href="#关于session" class="headerlink" title="关于session"></a>关于session</h2></blockquote><p>Laravel采用了另一套session机制，默认情况下session没有被打开，而有些情况下，我们引入的类需要开启session。比如引入验证码之后，需要把验证码字符存入session。</p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-6a2c57ce7d3d74e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="error.png"></p><p>此时可以在入口文件index.php打开session即可</p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-b55f04107b59b2cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="session.png"></p><blockquote><h2 id="csrf验证"><a href="#csrf验证" class="headerlink" title="csrf验证"></a>csrf验证</h2></blockquote><p>在使用Laravel框架开发网站的时候，我们最好从头到底按照框架规范进行设计<br><img src="http://upload-images.jianshu.io/upload_images/1531909-4922d1a661748114.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在进行表单验证时，需要加上csrf token</p><p><img src="http://upload-images.jianshu.io/upload_images/1531909-f85e842836ad3b1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><blockquote><p>return-&gt;back()-&gt;with()</p></blockquote><p>return back()-&gt;with(‘msg’,’验证码错误’);重定向至前一个页面，但传入的值用session(‘msg’)无法取到</p><p><code>项目路由配置时，所有路由是配置在一个总的路由分组中，对这个分组添加了web中间件。删掉这个中间件或者去掉这个路由分组，问题得到解决</code></p><blockquote><h2 id="时区设置"><a href="#时区设置" class="headerlink" title="时区设置"></a>时区设置</h2></blockquote><p>默认时区采用的是UTC，需要手动改成东八区。PRC在config下的app.php文件里：<br><img src="http://upload-images.jianshu.io/upload_images/1531909-e3fc42505ac364ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="时区.png"></p><blockquote><h2 id="5-this与静态函数"><a href="#5-this与静态函数" class="headerlink" title="5.this与静态函数"></a>5.this与静态函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">/* public static function tree()</div><div class="line">    &#123;</div><div class="line">        $category=Category::all();</div><div class="line">        return (new Category)-&gt;getTree($category,&apos;cate_name&apos;,&apos;cate_id&apos;,&apos;cate_pid&apos;);</div><div class="line">    &#125;*/</div></pre></td></tr></table></figure></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">public function tree()</div><div class="line">  &#123;</div><div class="line">      $category = $this-&gt;orderBy(&apos;cate_order&apos;,&apos;asc&apos;)-&gt;get();</div><div class="line">      return $this-&gt;getTree($category, &apos;cate_name&apos;, &apos;cate_id&apos;, &apos;cate_pid&apos;);</div><div class="line">  &#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;用了5天时间，参考后盾网老师的Laravel实战视频做的博客&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用Keras构建卷积神经网络预测“阿三”的年龄</title>
    <link href="http://yoursite.com/2017/08/03/03AgeDetection/"/>
    <id>http://yoursite.com/2017/08/03/03AgeDetection/</id>
    <published>2017-08-03T17:45:25.000Z</published>
    <updated>2017-09-15T09:31:20.263Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>用Keras构建基本的前馈神经网络以及借助卷积层逐步优化预测结果，从海量图片中预测印度人们的年龄。目前结果为（0.750904）<br><a id="more"></a></excerpt></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>我们的任务是从一个人的面部特征来预测他的年龄(用“Young”“Middle ”“Old”表示)，我们训练的数据集大约有19906多张照片及其每张图片对应的年龄（全是阿三的头像。。。），测试集有6636张图片，首先我们加载数据集，然后我们通过深度学习框架Keras建立、编译、训练模型，预测出6636张人物头像对应的年龄</p><h1 id="引入所需要模块"><a href="#引入所需要模块" class="headerlink" title="引入所需要模块"></a>引入所需要模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div></pre></td></tr></table></figure><h1 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root_dir=os.path.abspath(<span class="string">'E:/data/age'</span>)</div><div class="line">train=pd.read_csv(os.path.join(root_dir,<span class="string">'train.csv'</span>))</div><div class="line">test=pd.read_csv(os.path.join(root_dir,<span class="string">'test.csv'</span>))</div><div class="line"></div><div class="line">print(train.head())</div><div class="line">print(test.head())</div></pre></td></tr></table></figure><pre><code>          ID   Class0    377.jpg  MIDDLE1  17814.jpg   YOUNG2  21283.jpg  MIDDLE3  16496.jpg   YOUNG4   4487.jpg  MIDDLE          ID0  25321.jpg1    989.jpg2  19277.jpg3  13093.jpg4   5367.jpg</code></pre><h2 id="随机读取一张图片试下（☺）"><a href="#随机读取一张图片试下（☺）" class="headerlink" title="随机读取一张图片试下（☺）"></a>随机读取一张图片试下（☺）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">i=random.choice(train.index)</div><div class="line">img_name=train.ID[i]</div><div class="line">print(img_name)</div><div class="line">img=Image.open(os.path.join(root_dir,<span class="string">'Train'</span>,img_name))</div><div class="line">img.show()</div><div class="line">print(train.Class[i])</div></pre></td></tr></table></figure><pre><code>20188.jpgMIDDLE</code></pre><h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><p>我们随机打开几张图片之后，可以发现图片之间的差别比较大。大家感受下：</p><ol><li><p>质量好的图片：</p><ul><li>Middle:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022332/mid3.png" alt="**Middle**"></li><li>Young:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022132/y2.png" alt="**Young**"></li><li>Old:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022442/old1.png" alt="**Old**"></li></ul></li><li>质量差的：<ul><li>Middle:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022313/mid1.png" alt="**Middle**"></li></ul></li></ol><p>下面是我们需要面临的问题：</p><ol><li>图片的尺寸差别：有的图片的尺寸是66x46,而另一张图片尺寸为102x87</li><li>人物面貌角度不同：<ul><li>侧脸：<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022102/side1.png" alt=""></li><li>正脸：<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022113/try1.png" alt=""></li></ul></li><li>图片质量不一（直接上图）:<br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022450/pixel1.png" alt="插图"></li><li>亮度和对比度的差异<br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022151/contra1.png" alt="亮度"><br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022200/contra2.png" alt="对比度"><br>现在，我们只专注下图片尺寸处理，将每一张图片尺寸重置为32x32</li></ol><h2 id="格式化图片尺寸和将图片转换成numpy数组"><a href="#格式化图片尺寸和将图片转换成numpy数组" class="headerlink" title="格式化图片尺寸和将图片转换成numpy数组"></a>格式化图片尺寸和将图片转换成numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">temp=[]</div><div class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> train.ID:</div><div class="line">    img_path=os.path.join(root_dir,<span class="string">'Train'</span>,img_name)</div><div class="line">    img=Image.open(img_path)</div><div class="line">    img=img.resize((<span class="number">32</span>,<span class="number">32</span>))</div><div class="line">    array=np.array(img)</div><div class="line">    temp.append(array.astype(<span class="string">'float32'</span>))</div><div class="line">train_x=np.stack(temp)</div><div class="line">print(train_x.shape)</div><div class="line">print(train_x.ndim)</div></pre></td></tr></table></figure><pre><code>(19906, 32, 32, 3)4</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">temp=[]</div><div class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> test.ID:</div><div class="line">    img_path=os.path.join(root_dir,<span class="string">'Test'</span>,img_name)</div><div class="line">    img=Image.open(img_path)</div><div class="line">    img=img.resize((<span class="number">32</span>,<span class="number">32</span>))</div><div class="line">    array=np.array(img)</div><div class="line">    temp.append(array.astype(<span class="string">'float32'</span>))</div><div class="line">test_x=np.stack(temp)</div><div class="line">print(test_x.shape)</div></pre></td></tr></table></figure><pre><code>(6636, 32, 32, 3)</code></pre><p>另外我们再归一化图像，这样会使模型训练的更快</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">train_x = train_x / <span class="number">255.</span></div><div class="line">test_x = test_x / <span class="number">255.</span></div></pre></td></tr></table></figure><p>我们看下图片年龄大致分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.Class.value_counts(normalize=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><pre><code>MIDDLE    0.542751YOUNG     0.336883OLD       0.120366Name: Class, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test[<span class="string">'Class'</span>] = <span class="string">'MIDDLE'</span></div><div class="line">test.to_csv(<span class="string">'sub01.csv'</span>, index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>将目标变量处理虚拟列，能够使模型更容易接受识别它</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> keras</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line">lb=LabelEncoder()</div><div class="line">train_y=lb.fit_transform(train.Class)</div><div class="line">print(train_y)</div><div class="line">train_y=keras.utils.np_utils.to_categorical(train_y)</div><div class="line">print(train_y)</div><div class="line">print(train_y.shape)</div></pre></td></tr></table></figure><pre><code>[0 2 0 ..., 0 0 0][[ 1.  0.  0.] [ 0.  0.  1.] [ 1.  0.  0.] ...,  [ 1.  0.  0.] [ 1.  0.  0.] [ 1.  0.  0.]](19906, 3)</code></pre><h1 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#构建神经网络</span></div><div class="line">input_num_units=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div><div class="line">hidden_num_units=<span class="number">500</span></div><div class="line">output_num_units=<span class="number">3</span></div><div class="line">epochs=<span class="number">5</span></div><div class="line">batch_size=<span class="number">128</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Flatten,InputLayer</div><div class="line">model=Sequential(&#123;</div><div class="line">    InputLayer(input_shape=input_num_units),</div><div class="line">    Flatten(),</div><div class="line">    Dense(units=hidden_num_units,activation=<span class="string">'relu'</span>),</div><div class="line">    Dense(input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>),units=output_num_units,activation=<span class="string">'softmax'</span>)</div><div class="line">&#125;)</div><div class="line">model.summary()</div></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_23 (InputLayer)        (None, 32, 32, 3)         0         _________________________________________________________________flatten_23 (Flatten)         (None, 3072)              0         _________________________________________________________________dense_45 (Dense)             (None, 500)               1536500   _________________________________________________________________dense_46 (Dense)             (None, 3)                 1503      =================================================================Total params: 1,538,003Trainable params: 1,538,003Non-trainable params: 0_________________________________________________________________</code></pre><h1 id="编译模型"><a href="#编译模型" class="headerlink" title="编译模型"></a>编译模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])</span></div><div class="line">model.compile(optimizer=<span class="string">'sgd'</span>,loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,verbose=<span class="number">1</span>)</div></pre></td></tr></table></figure><pre><code>Epoch 1/519906/19906 [==============================] - 4s - loss: 0.8878 - acc: 0.5809     Epoch 2/519906/19906 [==============================] - 4s - loss: 0.8420 - acc: 0.6077     Epoch 3/519906/19906 [==============================] - 4s - loss: 0.8210 - acc: 0.6214     Epoch 4/519906/19906 [==============================] - 4s - loss: 0.8149 - acc: 0.6194     Epoch 5/519906/19906 [==============================] - 4s - loss: 0.8042 - acc: 0.6305     &lt;keras.callbacks.History at 0x1d3803e6278&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(train_x, train_y, batch_size=batch_size,epochs=epochs,verbose=<span class="number">1</span>, validation_split=<span class="number">0.2</span>)</div></pre></td></tr></table></figure><pre><code>Train on 15924 samples, validate on 3982 samplesEpoch 1/515924/15924 [==============================] - 3s - loss: 0.7970 - acc: 0.6375 - val_loss: 0.7854 - val_acc: 0.6396Epoch 2/515924/15924 [==============================] - 3s - loss: 0.7919 - acc: 0.6378 - val_loss: 0.7767 - val_acc: 0.6519Epoch 3/515924/15924 [==============================] - 3s - loss: 0.7870 - acc: 0.6404 - val_loss: 0.7754 - val_acc: 0.6534Epoch 4/515924/15924 [==============================] - 3s - loss: 0.7806 - acc: 0.6439 - val_loss: 0.7715 - val_acc: 0.6524Epoch 5/515924/15924 [==============================] - 3s - loss: 0.7755 - acc: 0.6519 - val_loss: 0.7970 - val_acc: 0.6346&lt;keras.callbacks.History at 0x1d3800a4eb8&gt;</code></pre><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>我们使用最基本的模型来处理这个年龄预测结果，并且最终的预测结果为0.6375。接下来，从以下角度尝试优化：</p><ol><li>使用更好的神经网络模型</li><li>增加训练次数</li><li>将图片进行灰度处理（因为对于本问题而言，图片颜色不是一个特别重要的特征。）</li></ol><h1 id="optimize1-使用卷积神经网络"><a href="#optimize1-使用卷积神经网络" class="headerlink" title="optimize1 使用卷积神经网络"></a>optimize1 使用卷积神经网络</h1><p><code>添加卷积层之后，预测准确率有所上涨，从6.3到6.7；最开始epochs轮数是5，训练轮数增加到10，此时准确率为6.87；然后将训练轮数增加到20，结果没有发生变化。</code></p><h2 id="Conv2D层"><a href="#Conv2D层" class="headerlink" title="Conv2D层"></a>Conv2D层</h2><p><code>keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding=&#39;valid&#39;, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)</code></p><ul><li>filters:输出的维度</li><li>strides:卷积的步长</li></ul><p>更多关于Conv2D的介绍请看<a href="http://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/#conv2d" target="_blank" rel="external">Keras文档Conv2D层</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#参数初始化</span></div><div class="line">filters=<span class="number">10</span></div><div class="line">filtersize=(<span class="number">5</span>,<span class="number">5</span>)</div><div class="line"></div><div class="line">epochs =<span class="number">10</span></div><div class="line">batchsize=<span class="number">128</span></div><div class="line"></div><div class="line">input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line">model = Sequential()</div><div class="line"></div><div class="line">model.add(keras.layers.InputLayer(input_shape=input_shape))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line">model.add(keras.layers.Flatten())</div><div class="line"></div><div class="line">model.add(keras.layers.Dense(units=<span class="number">3</span>, input_dim=<span class="number">50</span>,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x, train_y, epochs=epochs, batch_size=batchsize,validation_split=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">model.summary()</div></pre></td></tr></table></figure><pre><code>Train on 13934 samples, validate on 5972 samplesEpoch 1/1013934/13934 [==============================] - 9s - loss: 0.8986 - acc: 0.5884 - val_loss: 0.8352 - val_acc: 0.6271Epoch 2/1013934/13934 [==============================] - 9s - loss: 0.8141 - acc: 0.6281 - val_loss: 0.7886 - val_acc: 0.6474Epoch 3/1013934/13934 [==============================] - 9s - loss: 0.7788 - acc: 0.6504 - val_loss: 0.7706 - val_acc: 0.6551Epoch 4/1013934/13934 [==============================] - 9s - loss: 0.7638 - acc: 0.6577 - val_loss: 0.7559 - val_acc: 0.6626Epoch 5/1013934/13934 [==============================] - 9s - loss: 0.7484 - acc: 0.6679 - val_loss: 0.7457 - val_acc: 0.6710Epoch 6/1013934/13934 [==============================] - 9s - loss: 0.7346 - acc: 0.6723 - val_loss: 0.7490 - val_acc: 0.6780Epoch 7/1013934/13934 [==============================] - 9s - loss: 0.7217 - acc: 0.6804 - val_loss: 0.7298 - val_acc: 0.6795Epoch 8/1013934/13934 [==============================] - 9s - loss: 0.7162 - acc: 0.6826 - val_loss: 0.7248 - val_acc: 0.6792Epoch 9/1013934/13934 [==============================] - 9s - loss: 0.7082 - acc: 0.6892 - val_loss: 0.7202 - val_acc: 0.6890Epoch 10/1013934/13934 [==============================] - 9s - loss: 0.7001 - acc: 0.6940 - val_loss: 0.7226 - val_acc: 0.6885_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_6 (InputLayer)         (None, 32, 32, 3)         0         _________________________________________________________________conv2d_6 (Conv2D)            (None, 28, 28, 10)        760       _________________________________________________________________max_pooling2d_6 (MaxPooling2 (None, 14, 14, 10)        0         _________________________________________________________________flatten_6 (Flatten)          (None, 1960)              0         _________________________________________________________________dense_6 (Dense)              (None, 3)                 5883      =================================================================Total params: 6,643Trainable params: 6,643Non-trainable params: 0_________________________________________________________________</code></pre><h1 id="optimize2-增加神经网络的层数"><a href="#optimize2-增加神经网络的层数" class="headerlink" title="optimize2 增加神经网络的层数"></a>optimize2 增加神经网络的层数</h1><p>我们在模型中多添加几层并且提高卷几层的输出维度，这次结果得到显著提升：0.750904</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#参数初始化</span></div><div class="line">filters1=<span class="number">50</span></div><div class="line">filters2=<span class="number">100</span></div><div class="line">filters3=<span class="number">100</span></div><div class="line"></div><div class="line">filtersize=(<span class="number">5</span>,<span class="number">5</span>)</div><div class="line"></div><div class="line">epochs =<span class="number">10</span></div><div class="line">batchsize=<span class="number">128</span></div><div class="line"></div><div class="line">input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line"></div><div class="line">model.add(keras.layers.InputLayer(input_shape=input_shape))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters1, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters2, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters3, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.Flatten())</div><div class="line"></div><div class="line">model.add(keras.layers.Dense(units=<span class="number">3</span>, input_dim=<span class="number">50</span>,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x, train_y, epochs=epochs, batch_size=batchsize,validation_split=<span class="number">0.3</span>)</div><div class="line">model.summary()</div></pre></td></tr></table></figure><pre><code>Train on 13934 samples, validate on 5972 samplesEpoch 1/1013934/13934 [==============================] - 44s - loss: 0.8613 - acc: 0.5985 - val_loss: 0.7778 - val_acc: 0.6586Epoch 2/1013934/13934 [==============================] - 44s - loss: 0.7493 - acc: 0.6697 - val_loss: 0.7545 - val_acc: 0.6808Epoch 3/1013934/13934 [==============================] - 43s - loss: 0.7079 - acc: 0.6877 - val_loss: 0.7150 - val_acc: 0.6947Epoch 4/1013934/13934 [==============================] - 43s - loss: 0.6694 - acc: 0.7061 - val_loss: 0.6496 - val_acc: 0.7261Epoch 5/1013934/13934 [==============================] - 43s - loss: 0.6274 - acc: 0.7295 - val_loss: 0.6683 - val_acc: 0.7125Epoch 6/1013934/13934 [==============================] - 43s - loss: 0.5950 - acc: 0.7462 - val_loss: 0.6194 - val_acc: 0.7400Epoch 7/1013934/13934 [==============================] - 43s - loss: 0.5562 - acc: 0.7655 - val_loss: 0.5981 - val_acc: 0.7465Epoch 8/1013934/13934 [==============================] - 43s - loss: 0.5165 - acc: 0.7852 - val_loss: 0.6458 - val_acc: 0.7354Epoch 9/1013934/13934 [==============================] - 46s - loss: 0.4826 - acc: 0.7986 - val_loss: 0.6206 - val_acc: 0.7467Epoch 10/1013934/13934 [==============================] - 45s - loss: 0.4530 - acc: 0.8130 - val_loss: 0.5984 - val_acc: 0.7569_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_15 (InputLayer)        (None, 32, 32, 3)         0         _________________________________________________________________conv2d_31 (Conv2D)           (None, 28, 28, 50)        3800      _________________________________________________________________max_pooling2d_23 (MaxPooling (None, 14, 14, 50)        0         _________________________________________________________________conv2d_32 (Conv2D)           (None, 10, 10, 100)       125100    _________________________________________________________________max_pooling2d_24 (MaxPooling (None, 5, 5, 100)         0         _________________________________________________________________conv2d_33 (Conv2D)           (None, 1, 1, 100)         250100    _________________________________________________________________flatten_15 (Flatten)         (None, 100)               0         _________________________________________________________________dense_7 (Dense)              (None, 3)                 303       =================================================================Total params: 379,303Trainable params: 379,303Non-trainable params: 0_________________________________________________________________</code></pre><h1 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">pred=model.predict_classes(test_x)</div><div class="line">pred=lb.inverse_transform(pred)</div><div class="line">print(pred)</div><div class="line">test[<span class="string">'Class'</span>]=pred</div><div class="line">test.to_csv(<span class="string">'sub02.csv'</span>,index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><pre><code>6636/6636 [==============================] - 7s     [&apos;MIDDLE&apos; &apos;YOUNG&apos; &apos;MIDDLE&apos; ..., &apos;MIDDLE&apos; &apos;MIDDLE&apos; &apos;YOUNG&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">i = random.choice(train.index)</div><div class="line">img_name = train.ID[i]</div><div class="line"></div><div class="line">img=Image.open(os.path.join(root_dir,<span class="string">'Train'</span>,img_name))</div><div class="line">img.show()</div><div class="line">pred = model.predict_classes(train_x)</div><div class="line">print(<span class="string">'Original:'</span>, train.Class[i], <span class="string">'Predicted:'</span>, lb.inverse_transform(pred[i]))</div></pre></td></tr></table></figure><pre><code>19872/19906 [============================&gt;.] - ETA: 0sOriginal: MIDDLE Predicted: MIDDLE</code></pre><h1 id="继续探讨"><a href="#继续探讨" class="headerlink" title="继续探讨"></a>继续探讨</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;用Keras构建基本的前馈神经网络以及借助卷积层逐步优化预测结果，从海量图片中预测印度人们的年龄。目前结果为（0.750904）&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://yoursite.com/2017/08/02/02%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>http://yoursite.com/2017/08/02/02动态规划/</id>
    <published>2017-08-02T16:00:25.000Z</published>
    <updated>2017-09-15T09:31:20.263Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>文中主要总结动态规划的01背包问题以及相关实例，然后通过c++解决问题<br><a id="more"></a></excerpt></p><h1 id="一、背包问题"><a href="#一、背包问题" class="headerlink" title="一、背包问题"></a>一、背包问题</h1><h2 id="01背包问题"><a href="#01背包问题" class="headerlink" title="01背包问题"></a>01背包问题</h2><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p>有N件物品和一个容积为<code>V</code>的背包。第<code>i</code>件物品的体积是<code>c[i]</code>，价值是<code>w[i]</code>。求解将哪些物品装入背包可使价值总和最大。</p></blockquote><h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><blockquote><p>这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。<br>用子问题定义状态：即<code>f[i][v]</code>表示前<code>i</code>件物品恰放入一个容量为<code>v</code>的背包可以获得的最大价值。则其状态转移方程便是：</p><ul><li>二维方程：<code>f[i][v]=max(f[i-1][v],f[i-1][v-c[i]]+w[i])</code></li><li>一维方程：<code>f[v]=max(f[v],f[v-c[i]]+w[i])</code><br><strong>状态转移方程解释</strong>：“将前<code>i</code>件物品放入容量为v的背包中”这个子问题，若只考虑第<code>i</code>件物品放或不放，那么就可以转化为一个只牵扯前<code>i-1</code>件物品的问题。如果不放第i件物品，那么问题就转化为“前<code>i-1</code>件物品放入容量为v的背包中”，价值为<code>f[i-1][v]</code>；如果放第i件物品，那么问题就转化为“前i-1件物品放入剩下的容量为<code>v-c[i]</code>的背包中”，此时能获得的最大价值就是<code>f[i-1][v-c[i]]</code>再加上通过放入第i件物品获得的价值<code>w[i]</code>。</li></ul></blockquote><h2 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h2><p><img src="https://raw.githubusercontent.com/arkulo56/thought/master/images/algorithm/beibao.png" alt="图解"><br><a href="http://www.jianshu.com/p/48f2dd394608" target="_blank" rel="external">参考</a></p><blockquote><p>有了这张图和上面总结的公式，我们就可以很清晰的理解01背包算法了</p><ol><li>e2单元格：当只有一件物品e，包的容量是2时，装不进去，所以最大值为0</li><li>a8单元格：物品包括a、b、c、d、e，容量为8时，F[i-1,j]=F[b,8]=9，F[i-1,j-Wi]+Pi=F[b,6]+6=9+6=15，两种情况取最大值，因此这里的最大值是15</li></ol></blockquote><p>##实例1 采药(RQNOJ15)；</p><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>辰辰是个天资聪颖的孩子，他的梦想是成为世界上最伟大的医师。为此，他想拜附近最有威望的医师为师。医师为了判断他的资质，给他出了一个难题。医师把他带到一个到处都是草药的山洞里对他说：“孩子，这个山洞里有一些不同的草药，采每一株都需要一些时间，每一株也有它自身的价值。我会给你一段时间，在这段时间里，你可以采到一些草药。如果你是一个聪明的孩子，你应该可以让采到的草药的总价值最大。”<br>　　如果你是辰辰，你能完成这个任务吗？</p><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><p>　输入的第一行有两个整数T（1 &lt;= T &lt;= 1000）和M（1 &lt;= M &lt;= 100），用一个空格隔开，T代表总共能够用来采药的时间，M代表山洞里的草药的数目。接下来的M行每行包括两个在1到100之间（包括1和100）的整数，分别表示采摘某株草药的时间和这株草药的价值。</p><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><p>输出包括一行，这一行只包含一个整数，表示在规定的时间内，可以采到的草药的最大总价值。</p><h3 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h3><p>70 3<br>71 100<br>69 1<br>1 2</p><h3 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h3><p>3</p><h3 id="参考程序1"><a href="#参考程序1" class="headerlink" title="参考程序1"></a>参考程序1</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> ti[<span class="number">101</span>],money[<span class="number">101</span>];</div><div class="line"><span class="keyword">int</span> f[<span class="number">1001</span>];</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> t,m,i,j;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;t&gt;&gt;m;</div><div class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=m;i++)&#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;ti[i]&gt;&gt;money[i];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=m;i++)&#123;</div><div class="line">        <span class="keyword">for</span>(j=t;j&gt;=ti[i];j--)&#123;</div><div class="line">            <span class="keyword">if</span>(f[j-ti[i]]+money[i]&gt;f[j])&#123;<span class="comment">//把最大值赋值给f[t]</span></div><div class="line">                f[j]=f[j-ti[i]]+money[i];</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">cout</span>&lt;&lt;f[t];</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="参考程序2"><a href="#参考程序2" class="headerlink" title="参考程序2"></a>参考程序2</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">define</span>  V 1500</span></div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> f[<span class="number">10</span>][V];<span class="comment">//全局变量，自动初始化为0</span></div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> weight[<span class="number">10</span>];</div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> value[<span class="number">10</span>];</div><div class="line"><span class="meta">#<span class="meta-keyword">define</span>  max(x,y)   (x)&gt;(y)?(x):(y)</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">int</span> N,M;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;M;<span class="comment">//背包容量</span></div><div class="line">    <span class="built_in">cin</span>&gt;&gt;N;<span class="comment">//药品个数</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N; i++)</div><div class="line">    &#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;weight[i]&gt;&gt;value[i];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">        &#123;</div><div class="line">            <span class="keyword">if</span> (weight[i]&lt;=j)</div><div class="line">            &#123;</div><div class="line">                f[i][j]=max(f[i<span class="number">-1</span>][j],f[i<span class="number">-1</span>][j-weight[i]]+value[i]);</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span></div><div class="line">                f[i][j]=f[i<span class="number">-1</span>][j];</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="built_in">cout</span>&lt;&lt;f[N][M]&lt;&lt;<span class="built_in">endl</span>;<span class="comment">//输出最优解</span></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;文中主要总结动态规划的01背包问题以及相关实例，然后通过c++解决问题&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="http://yoursite.com/2017/08/01/01%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2017/08/01/01排序问题/</id>
    <published>2017-08-01T16:00:25.000Z</published>
    <updated>2017-09-15T09:31:20.263Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>这篇文章主要介绍了啊哈算法的排序问题，包括桶排序、快速排序、冒泡排序<br><a id="more"></a></excerpt></p><h1 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p><code>这个算法好比有11个桶，编号从0~10。每出现一个数，就在对应编号的桶里放一个小旗子。最后只要数数每个桶中有几个小旗子就可以了。例如2号桶中有2个旗子，表示数字2出现了2次。</code></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p><code>班上有5个同学，输入5个同学的分数（满分是10分），按从大到小输出5个同学的分数</code></p><h2 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h2><p><code>5 3 5 2 8</code></p><h2 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h2><p><code>8 5 5 3 2</code></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> a[<span class="number">11</span>]=&#123;<span class="number">0</span>&#125;,t;<span class="comment">//将数组元素初始化为0</span></div><div class="line">    <span class="comment">//int a[11]=&#123;1&#125;;第一个元素为1，其他元素为0</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;<span class="comment">//循环输入5个数</span></div><div class="line">        <span class="built_in">cin</span>&gt;&gt;t;</div><div class="line">        a[t]++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)&#123;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;a[i];j++)&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="string">"  "</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    getchar();getchar();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><h2 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h2><blockquote><p>通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p></blockquote><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> n,a[<span class="number">101</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">quicksort</span><span class="params">(<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> temp,i ,j ,t;<span class="comment">//temp存基数</span></div><div class="line">    <span class="keyword">if</span>(left&gt;right)&#123;</div><div class="line">        <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line">    temp=a[left];</div><div class="line">    i=left;</div><div class="line">    j=right;</div><div class="line">    <span class="keyword">while</span>(i!=j)&#123;</div><div class="line">        <span class="comment">//顺序很重要，要先从右往左查找</span></div><div class="line">        <span class="keyword">while</span>(a[j]&gt;=temp &amp;&amp; i&lt;j)&#123;</div><div class="line">            j--;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//然后从左往右找</span></div><div class="line">        <span class="keyword">while</span>(a[i]&lt;=temp&amp;&amp;i&lt;j)&#123;</div><div class="line">            i++;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//交换两个数的位置</span></div><div class="line">        <span class="keyword">if</span>(i&lt;j)&#123;<span class="comment">//当哨兵没有相遇时</span></div><div class="line">            t=a[i];</div><div class="line">            a[i]=a[j];</div><div class="line">            a[j]=t;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//最终基数归位</span></div><div class="line">    a[left]=a[i];</div><div class="line">    a[i]=temp;</div><div class="line">    quicksort(left,i<span class="number">-1</span>);</div><div class="line">    quicksort(i+<span class="number">1</span>,right);</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;n;</div><div class="line">    <span class="comment">//输入数据</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;a[i];</div><div class="line">    &#125;</div><div class="line">    quicksort(<span class="number">0</span>,n<span class="number">-1</span>);<span class="comment">//快速排序</span></div><div class="line">    <span class="comment">//输出数据</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;a[i]&lt;&lt;<span class="string">" "</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;这篇文章主要介绍了啊哈算法的排序问题，包括桶排序、快速排序、冒泡排序&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>HDU1009-FatMouse&#39; Trade</title>
    <link href="http://yoursite.com/2017/07/29/HDU1009-FatMouse&#39;%20Trade/"/>
    <id>http://yoursite.com/2017/07/29/HDU1009-FatMouse&#39; Trade/</id>
    <published>2017-07-29T14:54:03.000Z</published>
    <updated>2017-09-15T09:31:20.294Z</updated>
    
    <content type="html"><![CDATA[<excerpt in="" index="" |="" 首页摘要=""> <p>HDU 1009 FatMouse’ Trade<br><a id="more"></a></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>FatMouse prepared M pounds of cat food, ready to trade with the cats guarding the warehouse containing his favorite food, JavaBean.</p><p>The warehouse has N rooms. The i-th room contains J[i] pounds of JavaBeans and requires F[i] pounds of cat food. FatMouse does not have to trade for all the JavaBeans in the room, instead, he may get J[i]<em> a% pounds of JavaBeans if he pays F[i]</em> a% pounds of cat food. Here a is a real number. Now he is assigning this homework to you: tell him the maximum amount of JavaBeans he can obtain.</p><h1 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h1><p>The input consists of multiple test cases. Each test case begins with a line containing two non-negative integers M and N. Then N lines follow, each contains two non-negative integers J[i] and F[i] respectively. The last test case is followed by two -1’s. All integers are not greater than 1000.</p><h1 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h1><p>For each test case, print in a single line a real number accurate up to 3 decimal places, which is the maximum amount of JavaBeans that FatMouse can obtain.</p><h1 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">5 3</div><div class="line">7 2</div><div class="line">4 3</div><div class="line">5 2</div><div class="line">20 3</div><div class="line">25 18</div><div class="line">24 15</div><div class="line">15 10</div><div class="line">-1 -1</div></pre></td></tr></table></figure><h1 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">13.333</div><div class="line">31.500</div></pre></td></tr></table></figure><h1 id="解题报告"><a href="#解题报告" class="headerlink" title="解题报告"></a>解题报告</h1><p>大意：一只老鼠有M磅的猫粮，另外有一只猫控制了老鼠的N个房间，这些房间里面放了老鼠爱吃的绿豆，给出每个房间的绿豆数量，和这个房间的绿豆所需要的猫粮数，现在要求老鼠用这M磅的猫粮最多能换到多少它爱吃的绿豆？</p><p>贪心题，由于所有的绿豆都是一样的，所以如果老鼠想要换到最多的绿豆，便可以换猫控制的房间里面最便宜的绿豆，也就是说先换取单位数量的绿豆所需要最少的猫粮的房间里的绿豆，这样就可以保证换到的绿豆是最多的。具体实现可以用一个结构体，里面保存每个房间里面有的绿豆的数量和换取这个房间的绿豆时所需要的猫粮的数量和换取这个房间的 单位重量的绿豆所需要的猫粮数（以下简称单价），然后再按照单价升序给这些结构体排一次序，这时就可以从最便宜的绿豆开始换了。</p><h1 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iomanip&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">house</span>&#123;</span></div><div class="line">    <span class="keyword">int</span> bean_num;<span class="comment">//每个房间含有的豆子数量</span></div><div class="line">    <span class="keyword">int</span> cost;<span class="comment">//获取bean_num个豆子，所需要的猫粮数</span></div><div class="line">    <span class="keyword">double</span> rate;<span class="comment">//性价比</span></div><div class="line">&#125;h[<span class="number">1005</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(house a,house b)</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(a.rate!=b.rate)</div><div class="line">    <span class="keyword">return</span> a.rate&gt;b.rate;</div><div class="line">    <span class="keyword">else</span></div><div class="line">    <span class="keyword">return</span> a.bean_num&lt;b.bean_num;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> m,n,i;</div><div class="line">    <span class="keyword">double</span> gains;</div><div class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;m&gt;&gt;n&amp;&amp;m!=<span class="number">-1</span>&amp;&amp;n!=<span class="number">-1</span>)&#123;</div><div class="line">        gains=<span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="built_in">cin</span>&gt;&gt;h[i].bean_num&gt;&gt;h[i].cost;</div><div class="line">            h[i].rate=h[i].bean_num*<span class="number">1.0</span>/h[i].cost;</div><div class="line">        &#125;</div><div class="line">        sort(h,h+n,cmp);</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="keyword">if</span>(m&gt;h[i].cost)&#123;</div><div class="line">                m-=h[i].cost;</div><div class="line">                gains+=h[i].bean_num;</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                gains+=h[i].rate*m;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;setiosflags(ios::fixed)&lt;&lt;setprecision(<span class="number">3</span>)&lt;&lt;gains&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;HDU 1009 FatMouse’ Trade&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习算法的基本知识（使用Python和R代码）</title>
    <link href="http://yoursite.com/2017/07/26/Python28-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BD%BF%E7%94%A8Python%E5%92%8CR%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>http://yoursite.com/2017/07/26/Python28-机器学习算法的基本知识（使用Python和R代码）/</id>
    <published>2017-07-26T15:51:25.000Z</published>
    <updated>2017-09-15T09:31:20.310Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是<a href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/" target="_blank" rel="external">原文</a>的译文，然后自己对其中做了一些修改和添加内容（随机森林和降维算法）。文章简洁地介绍了机器学习的主要算法和一些伪代码，对于初学者有很大帮助，是一篇不错的总结文章，后期可以通过文中提到的算法展开去做一些实际问题。<br><a id="more"></a></excerpt></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><hr><p><code>Google的自驾车和机器人得到了很多新闻，但公司的真正未来是机器学习，这种技术使计算机变得更智能，更个性化。</code><em>-Eric Schmidt (Google Chairman)</em></p><hr><p>我们可能生活在人类历史上最具影响力的时期——计算从大型主机到PC移动到云计算的时期。 但是使这段时期有意义的不是发生了什么，而是在未来几年里我们的方式。</p><p>这个时期令像我这样的一个人兴奋的就是，随着计算机的推动，工具和技术的民主化。 今天，作为数据科学家，我可以每小时为几个玩偶构建具有复杂算法的数据处理机。 但到达这里并不容易，我已经度过了许多黑暗的日日夜夜。</p><h1 id="谁可以从本指南中获益最多"><a href="#谁可以从本指南中获益最多" class="headerlink" title="谁可以从本指南中获益最多"></a>谁可以从本指南中获益最多</h1><p><strong>我今天发布的可能是我创造的最有价值的指南。</strong></p><p>创建本指南背后的理念是简化全球有抱负的数据科学家和机器学习爱好者的旅程。 本指南能够使你在研究机器学习问题的过程中获取经验。 我提供了关于各种机器学习算法以及R＆Python代码的高级理解以及运行它们，这些应该足以使你得心顺手。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Newl-Machine-Learning-Algorithms.jpg" alt="machine learning"><br>我故意跳过了这些技术背后的统计数据，因为你不需要在开始时就了解它们。 所以，如果你正在寻找对这些算法的统计学理解，你应该看看别的文章。 但是，如果你正在寻找并开始构建机器学习项目，那么这篇文章给你带来极大好处。</p><h1 id="3类机器学习算法（广义上）"><a href="#3类机器学习算法（广义上）" class="headerlink" title="3类机器学习算法（广义上）"></a>3类机器学习算法（广义上）</h1><ol><li><p>监督学习<br>工作原理：该算法由一组目标/结果变量（或因变量）组成，该变量将根据给定的一组预测变量（独立变量）进行预测。 使用这些变量集，我们生成一个将输入映射到所需输出的函数。 训练过程继续进行执行，直到模型达到培训数据所需的准确度水平。 监督学习的例子：回归，决策树，随机森林，KNN，逻辑回归等</p></li><li><p>无监督学习<br>如何工作：在这个算法中，我们没有任何目标或结果变量来预测/估计。 用于不同群体的群体聚类和用于不同群体的客户进行特定干预。 无监督学习的例子：Apriori算法，K-means。</p></li><li><p>加强学习：<br>工作原理：使用这种算法，机器受到学习和训练，作出具体决定。 它以这种方式工作：机器暴露在一个环境中，它连续不断地使用试错。 该机器从过去的经验中学习，并尝试捕获最好的知识，以做出准确的业务决策。 加强学习示例：马尔可夫决策过程</p></li></ol><h1 id="常见机器学习算法"><a href="#常见机器学习算法" class="headerlink" title="常见机器学习算法"></a>常见机器学习算法</h1><p>以下是常用机器学习算法的列表。 这些算法几乎可以应用于任何数据问题：</p><ul><li>线性回归</li><li>逻辑回归</li><li>决策树</li><li>SVM</li><li>朴素贝叶斯</li><li>KNN</li><li>K-Means</li><li>随机森林</li><li>降维算法</li><li>Gradient Boost＆Adaboost</li></ul><h1 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h1><p>它用于基于连续变量来估计实际价值（房屋成本，电话数量，总销售额等）。在这里，我们通过拟合最佳线来建立独立变量和因变量之间的关系。这个最佳拟合线被称为回归线，由线性方程<code>Y = a * X + b</code>表示。</p><p>理解线性回归的最好方法是回想童年的经历。比如，你要求五年级的孩子通过体重来从小到大排序班里的学生，而事先不告诉学生们的体重！你认为孩子会做什么？他/她很可能在身高和体格上分析人物的体重，并使用这些可视参数的组合进行排列。这是现实生活中的线性回归！孩子实际上已经弄清楚，身高和体格将有一个关系与体重相关联，看起来就像上面的等式。</p><p>在这个方程式中：</p><p><code>Y-因变量</code><br><code>a - 斜率</code><br><code>X - 自变量</code><br><code>b - 截距</code><br>这些系数a和b是基于最小化数据点和回归线之间的距离的平方差之和导出的。</p><p>看下面的例子。这里我们确定了线性方程<code>y = 0.2811x + 13.9</code>的最佳拟合线。现在使用这个方程，我们可以找到一个人（身高已知）的体重。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Linear_Regression.png" alt="线性回归"><br>线性回归主要有两种类型：简单线性回归和多元线性回归。 简单线性回归的特征在于一个自变量。 而且，多元线性回归（顾名思义）的特征是多个（多于1个）自变量。 在找到最佳拟合线的同时，可以拟合多项式或曲线回归线，这些被称为多项式或曲线回归。</p><h2 id="Python-Code"><a href="#Python-Code" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line"><span class="comment"># x_train=input_variables_values_training_datasets</span></div><div class="line">x_train=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(x_train)</div><div class="line"><span class="comment"># y_train=target_variables_values_training_datasets</span></div><div class="line">y_train=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(y_train)</div><div class="line"></div><div class="line"><span class="comment"># x_test=input_variables_values_test_datasets</span></div><div class="line">x_test=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(x_test)</div><div class="line"></div><div class="line"><span class="comment"># Create linear regression object</span></div><div class="line">linear = linear_model.LinearRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear.fit(x_train, y_train)</div><div class="line">linear.score(x_train, y_train)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, linear.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, linear.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= linear.predict(x_test)</div><div class="line">print(<span class="string">'predicted:\n'</span>,predicted)</div></pre></td></tr></table></figure><pre><code>[[ 0.98267731  0.23364069  0.35133775  0.92826309] [ 0.80538991  0.05637806  0.87662175  0.3960776 ] [ 0.54686738  0.6816495   0.99747716  0.32531085] [ 0.19189509  0.87105462  0.88158122  0.25056621]][[ 0.55541608  0.56859636  0.40616234  0.14683524] [ 0.09937835  0.63874553  0.92062536  0.32798326] [ 0.87174236  0.779044    0.79119392  0.06912842] [ 0.87907434  0.53175367  0.01371655  0.11414196]][[ 0.37568516  0.17267374  0.51647046  0.04774661] [ 0.38573914  0.85335136  0.11647555  0.0758696 ] [ 0.67559384  0.57535368  0.88579261  0.26278658] [ 0.13829782  0.28328756  0.51170484  0.04260013]]Coefficient:  [[ 0.55158868  1.45901817  0.31224322  0.49538173] [ 0.6995448   0.40804135  0.59938423  0.09084578] [ 1.79010371  0.21674532  1.60972012 -0.046387  ] [-0.31562917 -0.53767439 -0.16141312 -0.2154683 ]]Intercept:  [-0.89705102 -0.50908061 -1.9260686   0.83934127]predicted: [[-0.25297601  0.13808785 -0.38696891  0.53426883] [ 0.63472658  0.18566989 -0.86662193  0.22361739] [ 0.72181277  0.75309881  0.82170796  0.11715048] [-0.22656611  0.01383581 -0.79537442  0.55159912]]</code></pre><h2 id="R-Code"><a href="#R-Code" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train &lt;- input_variables_values_training_datasets</div><div class="line">y_train &lt;- target_variables_values_training_datasets</div><div class="line">x_test &lt;- input_variables_values_test_datasets</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear &lt;- lm(y_train ~ ., data = x)</div><div class="line">summary(linear)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(linear,x_test)</div></pre></td></tr></table></figure><h1 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2.逻辑回归"></a>2.逻辑回归</h1><p>不要因为它的名字而感到困惑，逻辑回归是一个分类算法而不是回归算法。它用于基于给定的一组自变量来估计离散值（二进制值，如0/1，是/否，真/假）。简单来说，它通过将数据拟合到logit函数来预测事件发生的概率。因此，它也被称为logit回归。由于它预测概率，其输出值在0和1之间（如预期的那样）。</p><p>再次，让我们通过一个简单的例子来尝试理解这一点。</p><p>假设你的朋友给你一个难题解决。只有2个结果场景 - 你能解决和不能解决。现在想象，你正在被许多猜谜或者简单测验，来试图理解你擅长的科目。这项研究的结果将是这样的结果 - 如果给你一个10级的三角形问题，那么你有70％可能会解决这个问题。另外一个例子，如果是五级的历史问题，得到答案的概率只有30％。这就是逻辑回归为你提供的结果。</p><p>对数学而言，结果的对数几率被建模为预测变量的线性组合。</p><p><code>odds= p/ (1-p) = probability of event occurrence / probability of not event occurrenceln(odds) = ln(p/(1-p))logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk</code></p><p>以上，p是感兴趣特征的概率。 它选择最大化观察样本值的可能性的参数，而不是最小化平方误差的总和（如在普通回归中）。</p><p>现在，你可能会问，为什么要采用log？ 为了简单起见，让我们来说，这是复制阶梯函数的最好的数学方法之一。 我可以进一步详细介绍，但这将会打破这篇文章的目的。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Logistic_Regression.png" alt="逻辑回归"></p><h2 id="Python-Code-1"><a href="#Python-Code-1" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, model.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, model.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-1"><a href="#R-Code-1" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</div><div class="line">summary(logistic)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(logistic,x_test)</div></pre></td></tr></table></figure><h1 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3.决策树"></a>3.决策树</h1><p>这是我最喜欢的算法之一，我经常使用它。 它是一种主要用于分类问题的监督学习算法，令人惊讶的是，它可以适用于分类和连·续因变量。 在该算法中，我们将群体分为两个或多个均匀集合。 这是基于最重要的属性/自变量来做出的并将它们分为不同的组。关于决策树的更多细节，你可以阅读<a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="external">决策树简介</a></p><p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png" alt="决策树"><br>在上图中，您可以看到根据多个属性将群体分为四个不同的群组，以确定用户“是否可以玩”。为了 将人口分为不同的特征群体，它使用了诸如Gini，信息增益，卡方，熵等各种技术。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg" alt="JezzBall"><br>了解决策树如何运作的最佳方法是播放Jezzball - 微软的经典游戏（下图）。 大体上就是，来一起在屏幕上滑动手指，筑起墙壁，掩住移动的球吧。</p><h2 id="Python-Code-2"><a href="#Python-Code-2" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create tree object </span></div><div class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>) </div><div class="line"><span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></div><div class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-2"><a href="#R-Code-2" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(rpart)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># grow tree </span></div><div class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure><h1 id="4-SVM-支持向量机"><a href="#4-SVM-支持向量机" class="headerlink" title="4.SVM(支持向量机)"></a>4.SVM(支持向量机)</h1><p>这是一种分类方法。 在这个算法中，我们将每个数据项目绘制为n维空间中的一个点（其中n是拥有的特征数），每个特征的值是特定坐标的值。</p><p>例如，如果我们有一个人的“高度”和“头发长度”这两个特征，我们首先将这两个变量绘制在二维空间中，其中每个点都有两个坐标（这些坐标称为支持向量）<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM1-850x575.png" alt="支持向量机"><br>现在，我们将找到一些可以将数据分割成两类的线。 而我们想要的线，就是使得两组数据中最近点到分割线的距离最长的线。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2-850x578.png" alt="最佳分割直线"><br>在上述示例中，将数据分成两个不同分类的组的线是黑线，因为两个最接近的点距离线最远（红线也可以，但不是一最远）。 这条线是我们的分类器， 然后根据测试数据位于线路两边的位置，我们可以将新数据分类为什么类别。</p><h2 id="Python-Code-3"><a href="#Python-Code-3" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object </span></div><div class="line">model = svm.svc() <span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-3"><a href="#R-Code-3" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-svm(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure><h1 id="5-朴素贝叶斯"><a href="#5-朴素贝叶斯" class="headerlink" title="5. 朴素贝叶斯"></a>5. 朴素贝叶斯</h1><p>它是基于贝叶斯定理的分类技术，假设预测因子之间是独立的。 简单来说，朴素贝叶斯分类器假设类中特定特征的存在与任何其他特征的存在无关。 例如，如果果实是红色，圆形，直径约3英寸，则果实可能被认为是苹果。 即使这些特征依赖于彼此或其他特征的存在，一个朴素的贝叶斯分类器将考虑的是所有属性来单独地贡献这个果实是苹果的概率。</p><p>朴素贝叶斯模型易于构建，对于非常大的数据集尤其有用。 除了简单之外，朴素贝叶斯也被称为超高级分类方法。</p><p>贝叶斯定理提供了一种由P（c），P（x）和P（x | c）计算概率P（c | x）的方法。 看下面的等式：<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule.png" alt="朴素贝叶斯"></p><ul><li>其中：<ul><li>P（c | x）是在x条件下c发生的概率。</li><li>P（c）是c发生的概率。</li><li>P（x | c）在c条件下x发生的概率。</li><li>P（x）是x发生的概率。</li></ul></li></ul><h2 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h2><p>让我们用一个例子来理解它。 下面我有一个天气和相应的目标变量“玩游戏”的训练数据集。 现在，我们需要根据天气条件对玩家是否玩游戏进行分类。 我们按照以下步骤执行。</p><p>步骤1：将数据集转换为频率表</p><p>步骤2：通过发现像“Overcast”概率= 0.29和播放概率为0.64的概率来创建似然表。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41-850x310.png" alt="例子"><br>步骤3：现在，使用朴素贝叶斯方程来计算每个类的概率。 其中概率最高的情况就是是预测的结果。</p><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>如果天气晴朗，玩家会玩游戏，这个说法是正确的吗？</p><p>我们可以使用上述方法解决，所以P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p><p>这里，P（Sunny | Yes）= 3/9 = 0.33，P（Sunny）= 5/14 = 0.36，P（Yes）= 9/14 = 0.64</p><p>现在，P（Yes | Sunny）= 0.33 * 0.64 / 0.36 = 0.60，该事件发生的概率还是比较高的。</p><p>朴素贝叶斯使用类似的方法根据各种属性预测不同分类的概率，该算法主要用于文本分类和具有多个类的问题。</p><h2 id="Python-Code-4"><a href="#Python-Code-4" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object model = GaussianNB() </span></div><div class="line"><span class="comment"># there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-4"><a href="#R-Code-4" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure><h1 id="6-KNN-K-近邻算法"><a href="#6-KNN-K-近邻算法" class="headerlink" title="6. KNN (K-近邻算法)"></a>6. KNN (K-近邻算法)</h1><p>它可以用于分类和回归问题, 然而，它在行业中被广泛地应用于分类问题。 K-近邻算法用于存储所有训练样本集（所有已知的案列），并通过其k个邻近数据多数投票对新的数据（或者案列）进行分类。通常，选择k个最近邻数据中出现次数最多的分类作为新数据的分类。</p><p>这些计算机的距离函数可以是欧几里德，曼哈顿，闵可夫斯基和汉明距离。 前三个函数用于连续函数，第四个函数用于分类变量。 如果K = 1，则简单地将该情况分配给其最近邻的类。 有时，选择K在执行KNN建模时是一个难点。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png" alt="K-近邻算法"><br>KNN可以轻松映射到我们的现实生活中。 如果你想了解一个人，你没有任何信息，你可能想知道先去了解他的亲密的朋友和他活动的圈子，从而获得他/她的信息！</p><p>选择KNN之前要考虑的事项：</p><ul><li>KNN在计算上是昂贵的</li><li>变量应该被归一化，否则更高的范围变量可以偏移它</li><li>在进行KNN之前，预处理阶段的工作更像去除离群值、噪声值</li></ul><h2 id="Python-Code-5"><a href="#Python-Code-5" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">KNeighborsClassifier(n_neighbors=<span class="number">6</span>) <span class="comment"># default value for n_neighbors is 5</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-5"><a href="#R-Code-5" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(knn)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-knn(y_train ~ ., data = x,k=<span class="number">5</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure><h1 id="7-K-Means"><a href="#7-K-Means" class="headerlink" title="7. K-Means"></a>7. K-Means</h1><p>它是解决聚类问题的一种无监督算法。 其过程遵循一种简单而简单的方式，通过一定数量的聚类（假设k个聚类）对给定的数据集进行分类。 集群内的数据点与对等组是同构的和异构的。</p><p>尝试从油墨印迹中找出形状？（见下图） k means 与这个活动相似， 你通过墨水渍形状来判断有多少群体存在！<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph-284x300.jpg" alt="K-Means"><br>下面两点感觉原文解释的不是很清楚，自己然后查了下国内的解释方法</p><h2 id="K-means如何形成集群"><a href="#K-means如何形成集群" class="headerlink" title="K-means如何形成集群"></a>K-means如何形成集群</h2><ul><li>（1） 从 n个数据对象任意选择 k 个对象作为初始聚类中心；</li><li>（2） 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</li><li>（3） 重新计算每个（有变化）聚类的均值（中心对象）</li><li>（4） 循环（2）到（3）直到每个聚类不再发生变化为止<a href="https://baike.baidu.com/item/K-means/4934806?fr=aladdin" target="_blank" rel="external">参考</a></li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="http://cms.csdnimg.cn/articlev1/uploads/allimg/120703/091301K62-1.jpg" alt="K-Means例子"><br>从上图中，我们可以看到，<code>A，B，C，D，E</code>是五个在图中点。而灰色的点是我们的种子点，也就是我们用来找点群的点。有两个种子点，所以<code>K=2</code>。</p><p>然后，<code>K-Means</code>的算法如下：</p><ol><li>随机在图中取K（这里K=2）个种子点。</li><li>然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群。（上图中，我们可以看到A，B属于上面的种子点，C，D，E属于下面中部的种子点）</li><li>接下来，我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）</li><li>然后重复第2）和第3）步，直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A，B，C，下面的种子点聚合了D，E）。<a href="http://www.csdn.net/article/2012-07-03/2807073-k-means" target="_blank" rel="external">参考</a></li></ol><h2 id="K值如何确定"><a href="#K值如何确定" class="headerlink" title="K值如何确定"></a>K值如何确定</h2><p>在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。<a href="http://www.cnblogs.com/dudumiaomiao/p/5839905.html" target="_blank" rel="external">参考</a></p><p>我们知道随着群集数量的增加，该值不断减少，但是如果绘制结果，则可能会发现平方距离的总和急剧下降到k的某个值，然后再慢一些。 在这里，我们可以找到最佳聚类数。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Kmenas-850x429.png" alt="k值"></p><h2 id="Python-Code-6"><a href="#Python-Code-6" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-6"><a href="#R-Code-6" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">library(cluster)</div><div class="line">fit &lt;- kmeans(X, <span class="number">3</span>) <span class="comment"># 5 cluster solution</span></div></pre></td></tr></table></figure><h1 id="8-Random-Forest（随机树林）"><a href="#8-Random-Forest（随机树林）" class="headerlink" title="8. Random Forest（随机树林）"></a>8. Random Forest（随机树林）</h1><p>随机森林(Random Forest)是一个包含多个决策树的分类器， 其输出的类别由个别树输出类别的众数而定。（相当于许多不同领域的专家对数据进行分类判断，然后投票）<br><img src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=e0a6ac59104c510fbac9ea4801304e48/960a304e251f95cab62ae027c3177f3e66095247.jpg" alt="随机树林"><br>感觉原文没有将什么实质内容，给大家推进这一篇<a href="https://www.zybuluo.com/hshustc/note/179319" target="_blank" rel="external">Random Forest入门</a></p><h1 id="9-降维算法"><a href="#9-降维算法" class="headerlink" title="9. 降维算法"></a>9. 降维算法</h1><p>在过去的4-5年中，数据挖掘在每个可能的阶段都呈指数级增长。 公司/政府机构/研究机构不仅有新的来源，而且他们正在非常详细地挖掘数据。</p><p>例如：电子商务公司正在捕获更多关于客户的细节，例如人口统计，网络爬网历史，他们喜欢或不喜欢的内容，购买历史记录，反馈信息等等，给予他们个性化的关注，而不是离你最近的杂货店主。</p><p>作为数据科学家，我们提供的数据还包括许多功能，这对建立良好的稳健模型是非常有用的，但是有一个挑战。 你如何识别出1000或2000年高度重要的变量？ 在这种情况下，维数降低算法可以帮助我们与决策树，随机森林，PCA，因子分析，基于相关矩阵，缺失值比等的其他算法一起使用。<br>要了解更多有关此算法的信息，您可以阅读<a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/" target="_blank" rel="external"> “Beginners Guide To Learn Dimension Reduction Techniques“.</a></p><h2 id="Python-Code-7"><a href="#Python-Code-7" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</div><div class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></div><div class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></div><div class="line"><span class="comment"># For Factor analysis</span></div><div class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></div><div class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></div><div class="line">train_reduced = pca.fit_transform(train)</div><div class="line"><span class="comment">#Reduced the dimension of test dataset</span></div><div class="line">test_reduced = pca.transform(test)</div></pre></td></tr></table></figure><p>For more detail on this, please refer  <a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions" target="_blank" rel="external">this link</a>.</p><h2 id="R-Code-7"><a href="#R-Code-7" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">library(stats)</div><div class="line">pca &lt;- princomp(train, cor = TRUE)</div><div class="line">train_reduced  &lt;- predict(pca,train)</div><div class="line">test_reduced  &lt;- predict(pca,test)</div></pre></td></tr></table></figure><h1 id="10-Gradient-Boosting-amp-AdaBoost"><a href="#10-Gradient-Boosting-amp-AdaBoost" class="headerlink" title="10. Gradient Boosting &amp; AdaBoost"></a>10. Gradient Boosting &amp; AdaBoost</h1><p>当我们处理大量数据以预测高预测能力时，GBM＆AdaBoost是更加强大的算法。 Boosting是一种综合学习算法，它结合了几个基本估计器的预测，以提高单个估计器的鲁棒性。 它将多个弱或平均预测值组合到一个强大的预测变量上。 这些提升算法在数据科学比赛中总是能够很好地运行，如Kaggle，AV Hackathon，CrowdAnalytix。<br>More: <a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="external">Know about Gradient and AdaBoost in detail</a></p><h2 id="Python-Code-8"><a href="#Python-Code-8" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></div><div class="line">model= GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure><h2 id="R-Code-8"><a href="#R-Code-8" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">library(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = <span class="number">4</span>, repeats = <span class="number">4</span>)</div><div class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = FALSE)</div><div class="line">predicted= predict(fit,x_test,type= <span class="string">"prob"</span>)[,<span class="number">2</span>]</div></pre></td></tr></table></figure><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>现在我相信，你会有一个常用的机器学习算法的想法。 我在写这篇文章和提供R和Python中的代码的唯一意图就是让你马上开始。 如果您想要掌握机器学习，请将算法运用实际问题，体会其中的乐趣</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文&lt;/a&gt;的译文，然后自己对其中做了一些修改和添加内容（随机森林和降维算法）。文章简洁地介绍了机器学习的主要算法和一些伪代码，对于初学者有很大帮助，是一篇不错的总结文章，后期可以通过文中提到的算法展开去做一些实际问题。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Kera实例：预测白酒和红酒的质量</title>
    <link href="http://yoursite.com/2017/07/26/Keras04-%E7%99%BD%E9%85%92%E5%92%8C%E7%BA%A2%E9%85%92%E5%AE%9E%E4%BE%8B/"/>
    <id>http://yoursite.com/2017/07/26/Keras04-白酒和红酒实例/</id>
    <published>2017-07-26T15:51:25.000Z</published>
    <updated>2017-09-15T09:31:20.294Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是益智的教程，参考之后动手进行实践了一遍。编译环境windows10+python3.5<br><a id="more"></a></excerpt></p><p><a href="https://jizhi.im/course/dl_keras/2" target="_blank" rel="external">参考</a></p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment">#读取数据</span></div><div class="line">red=pd.read_csv(<span class="string">'winequality-red.csv'</span>,sep=<span class="string">';'</span>)</div><div class="line">white=pd.read_csv(<span class="string">'winequality-white.csv'</span>,sep=<span class="string">';'</span>)</div><div class="line"><span class="comment">#输出数据</span></div><div class="line">print(red.info)</div><div class="line">print(white.info)</div></pre></td></tr></table></figure><pre><code>&lt;bound method DataFrame.info of       fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \0               7.4             0.700         0.00             1.9      0.076   1               7.8             0.880         0.00             2.6      0.098   2               7.8             0.760         0.04             2.3      0.092   3              11.2             0.280         0.56             1.9      0.075   4               7.4             0.700         0.00             1.9      0.076   5               7.4             0.660         0.00             1.8      0.075   6               7.9             0.600         0.06             1.6      0.069   7               7.3             0.650         0.00             1.2      0.065   8               7.8             0.580         0.02             2.0      0.073   9               7.5             0.500         0.36             6.1      0.071   10              6.7             0.580         0.08             1.8      0.097   11              7.5             0.500         0.36             6.1      0.071   12              5.6             0.615         0.00             1.6      0.089   13              7.8             0.610         0.29             1.6      0.114   14              8.9             0.620         0.18             3.8      0.176   15              8.9             0.620         0.19             3.9      0.170   16              8.5             0.280         0.56             1.8      0.092   17              8.1             0.560         0.28             1.7      0.368   18              7.4             0.590         0.08             4.4      0.086   19              7.9             0.320         0.51             1.8      0.341   20              8.9             0.220         0.48             1.8      0.077   21              7.6             0.390         0.31             2.3      0.082   22              7.9             0.430         0.21             1.6      0.106   23              8.5             0.490         0.11             2.3      0.084   24              6.9             0.400         0.14             2.4      0.085   25              6.3             0.390         0.16             1.4      0.080   26              7.6             0.410         0.24             1.8      0.080   27              7.9             0.430         0.21             1.6      0.106   28              7.1             0.710         0.00             1.9      0.080   29              7.8             0.645         0.00             2.0      0.082   ...             ...               ...          ...             ...        ...   1569            6.2             0.510         0.14             1.9      0.056   1570            6.4             0.360         0.53             2.2      0.230   1571            6.4             0.380         0.14             2.2      0.038   1572            7.3             0.690         0.32             2.2      0.069   1573            6.0             0.580         0.20             2.4      0.075   1574            5.6             0.310         0.78            13.9      0.074   1575            7.5             0.520         0.40             2.2      0.060   1576            8.0             0.300         0.63             1.6      0.081   1577            6.2             0.700         0.15             5.1      0.076   1578            6.8             0.670         0.15             1.8      0.118   1579            6.2             0.560         0.09             1.7      0.053   1580            7.4             0.350         0.33             2.4      0.068   1581            6.2             0.560         0.09             1.7      0.053   1582            6.1             0.715         0.10             2.6      0.053   1583            6.2             0.460         0.29             2.1      0.074   1584            6.7             0.320         0.44             2.4      0.061   1585            7.2             0.390         0.44             2.6      0.066   1586            7.5             0.310         0.41             2.4      0.065   1587            5.8             0.610         0.11             1.8      0.066   1588            7.2             0.660         0.33             2.5      0.068   1589            6.6             0.725         0.20             7.8      0.073   1590            6.3             0.550         0.15             1.8      0.077   1591            5.4             0.740         0.09             1.7      0.089   1592            6.3             0.510         0.13             2.3      0.076   1593            6.8             0.620         0.08             1.9      0.068   1594            6.2             0.600         0.08             2.0      0.090   1595            5.9             0.550         0.10             2.2      0.062   1596            6.3             0.510         0.13             2.3      0.076   1597            5.9             0.645         0.12             2.0      0.075   1598            6.0             0.310         0.47             3.6      0.067         free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \0                    11.0                  34.0  0.99780  3.51       0.56   1                    25.0                  67.0  0.99680  3.20       0.68   2                    15.0                  54.0  0.99700  3.26       0.65   3                    17.0                  60.0  0.99800  3.16       0.58   4                    11.0                  34.0  0.99780  3.51       0.56   5                    13.0                  40.0  0.99780  3.51       0.56   6                    15.0                  59.0  0.99640  3.30       0.46   7                    15.0                  21.0  0.99460  3.39       0.47   8                     9.0                  18.0  0.99680  3.36       0.57   9                    17.0                 102.0  0.99780  3.35       0.80   10                   15.0                  65.0  0.99590  3.28       0.54   11                   17.0                 102.0  0.99780  3.35       0.80   12                   16.0                  59.0  0.99430  3.58       0.52   13                    9.0                  29.0  0.99740  3.26       1.56   14                   52.0                 145.0  0.99860  3.16       0.88   15                   51.0                 148.0  0.99860  3.17       0.93   16                   35.0                 103.0  0.99690  3.30       0.75   17                   16.0                  56.0  0.99680  3.11       1.28   18                    6.0                  29.0  0.99740  3.38       0.50   19                   17.0                  56.0  0.99690  3.04       1.08   20                   29.0                  60.0  0.99680  3.39       0.53   21                   23.0                  71.0  0.99820  3.52       0.65   22                   10.0                  37.0  0.99660  3.17       0.91   23                    9.0                  67.0  0.99680  3.17       0.53   24                   21.0                  40.0  0.99680  3.43       0.63   25                   11.0                  23.0  0.99550  3.34       0.56   26                    4.0                  11.0  0.99620  3.28       0.59   27                   10.0                  37.0  0.99660  3.17       0.91   28                   14.0                  35.0  0.99720  3.47       0.55   29                    8.0                  16.0  0.99640  3.38       0.59   ...                   ...                   ...      ...   ...        ...   1569                 15.0                  34.0  0.99396  3.48       0.57   1570                 19.0                  35.0  0.99340  3.37       0.93   1571                 15.0                  25.0  0.99514  3.44       0.65   1572                 35.0                 104.0  0.99632  3.33       0.51   1573                 15.0                  50.0  0.99467  3.58       0.67   1574                 23.0                  92.0  0.99677  3.39       0.48   1575                 12.0                  20.0  0.99474  3.26       0.64   1576                 16.0                  29.0  0.99588  3.30       0.78   1577                 13.0                  27.0  0.99622  3.54       0.60   1578                 13.0                  20.0  0.99540  3.42       0.67   1579                 24.0                  32.0  0.99402  3.54       0.60   1580                  9.0                  26.0  0.99470  3.36       0.60   1581                 24.0                  32.0  0.99402  3.54       0.60   1582                 13.0                  27.0  0.99362  3.57       0.50   1583                 32.0                  98.0  0.99578  3.33       0.62   1584                 24.0                  34.0  0.99484  3.29       0.80   1585                 22.0                  48.0  0.99494  3.30       0.84   1586                 34.0                  60.0  0.99492  3.34       0.85   1587                 18.0                  28.0  0.99483  3.55       0.66   1588                 34.0                 102.0  0.99414  3.27       0.78   1589                 29.0                  79.0  0.99770  3.29       0.54   1590                 26.0                  35.0  0.99314  3.32       0.82   1591                 16.0                  26.0  0.99402  3.67       0.56   1592                 29.0                  40.0  0.99574  3.42       0.75   1593                 28.0                  38.0  0.99651  3.42       0.82   1594                 32.0                  44.0  0.99490  3.45       0.58   1595                 39.0                  51.0  0.99512  3.52       0.76   1596                 29.0                  40.0  0.99574  3.42       0.75   1597                 32.0                  44.0  0.99547  3.57       0.71   1598                 18.0                  42.0  0.99549  3.39       0.66         alcohol  quality  0         9.4        5  1         9.8        5  2         9.8        5  3         9.8        6  4         9.4        5  5         9.4        5  6         9.4        5  7        10.0        7  8         9.5        7  9        10.5        5  10        9.2        5  11       10.5        5  12        9.9        5  13        9.1        5  14        9.2        5  15        9.2        5  16       10.5        7  17        9.3        5  18        9.0        4  19        9.2        6  20        9.4        6  21        9.7        5  22        9.5        5  23        9.4        5  24        9.7        6  25        9.3        5  26        9.5        5  27        9.5        5  28        9.4        5  29        9.8        6  ...       ...      ...  1569     11.5        6  1570     12.4        6  1571     11.1        6  1572      9.5        5  1573     12.5        6  1574     10.5        6  1575     11.8        6  1576     10.8        6  1577     11.9        6  1578     11.3        6  1579     11.3        5  1580     11.9        6  1581     11.3        5  1582     11.9        5  1583      9.8        5  1584     11.6        7  1585     11.5        6  1586     11.4        6  1587     10.9        6  1588     12.8        6  1589      9.2        5  1590     11.6        6  1591     11.6        6  1592     11.0        6  1593      9.5        6  1594     10.5        5  1595     11.2        6  1596     11.0        6  1597     10.2        5  1598     11.0        6  [1599 rows x 12 columns]&gt;&lt;bound method DataFrame.info of       fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \0               7.0             0.270         0.36           20.70      0.045   1               6.3             0.300         0.34            1.60      0.049   2               8.1             0.280         0.40            6.90      0.050   3               7.2             0.230         0.32            8.50      0.058   4               7.2             0.230         0.32            8.50      0.058   5               8.1             0.280         0.40            6.90      0.050   6               6.2             0.320         0.16            7.00      0.045   7               7.0             0.270         0.36           20.70      0.045   8               6.3             0.300         0.34            1.60      0.049   9               8.1             0.220         0.43            1.50      0.044   10              8.1             0.270         0.41            1.45      0.033   11              8.6             0.230         0.40            4.20      0.035   12              7.9             0.180         0.37            1.20      0.040   13              6.6             0.160         0.40            1.50      0.044   14              8.3             0.420         0.62           19.25      0.040   15              6.6             0.170         0.38            1.50      0.032   16              6.3             0.480         0.04            1.10      0.046   17              6.2             0.660         0.48            1.20      0.029   18              7.4             0.340         0.42            1.10      0.033   19              6.5             0.310         0.14            7.50      0.044   20              6.2             0.660         0.48            1.20      0.029   21              6.4             0.310         0.38            2.90      0.038   22              6.8             0.260         0.42            1.70      0.049   23              7.6             0.670         0.14            1.50      0.074   24              6.6             0.270         0.41            1.30      0.052   25              7.0             0.250         0.32            9.00      0.046   26              6.9             0.240         0.35            1.00      0.052   27              7.0             0.280         0.39            8.70      0.051   28              7.4             0.270         0.48            1.10      0.047   29              7.2             0.320         0.36            2.00      0.033   ...             ...               ...          ...             ...        ...   4868            5.8             0.230         0.31            4.50      0.046   4869            6.6             0.240         0.33           10.10      0.032   4870            6.1             0.320         0.28            6.60      0.021   4871            5.0             0.200         0.40            1.90      0.015   4872            6.0             0.420         0.41           12.40      0.032   4873            5.7             0.210         0.32            1.60      0.030   4874            5.6             0.200         0.36            2.50      0.048   4875            7.4             0.220         0.26            1.20      0.035   4876            6.2             0.380         0.42            2.50      0.038   4877            5.9             0.540         0.00            0.80      0.032   4878            6.2             0.530         0.02            0.90      0.035   4879            6.6             0.340         0.40            8.10      0.046   4880            6.6             0.340         0.40            8.10      0.046   4881            5.0             0.235         0.27           11.75      0.030   4882            5.5             0.320         0.13            1.30      0.037   4883            4.9             0.470         0.17            1.90      0.035   4884            6.5             0.330         0.38            8.30      0.048   4885            6.6             0.340         0.40            8.10      0.046   4886            6.2             0.210         0.28            5.70      0.028   4887            6.2             0.410         0.22            1.90      0.023   4888            6.8             0.220         0.36            1.20      0.052   4889            4.9             0.235         0.27           11.75      0.030   4890            6.1             0.340         0.29            2.20      0.036   4891            5.7             0.210         0.32            0.90      0.038   4892            6.5             0.230         0.38            1.30      0.032   4893            6.2             0.210         0.29            1.60      0.039   4894            6.6             0.320         0.36            8.00      0.047   4895            6.5             0.240         0.19            1.20      0.041   4896            5.5             0.290         0.30            1.10      0.022   4897            6.0             0.210         0.38            0.80      0.020         free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \0                    45.0                 170.0  1.00100  3.00       0.45   1                    14.0                 132.0  0.99400  3.30       0.49   2                    30.0                  97.0  0.99510  3.26       0.44   3                    47.0                 186.0  0.99560  3.19       0.40   4                    47.0                 186.0  0.99560  3.19       0.40   5                    30.0                  97.0  0.99510  3.26       0.44   6                    30.0                 136.0  0.99490  3.18       0.47   7                    45.0                 170.0  1.00100  3.00       0.45   8                    14.0                 132.0  0.99400  3.30       0.49   9                    28.0                 129.0  0.99380  3.22       0.45   10                   11.0                  63.0  0.99080  2.99       0.56   11                   17.0                 109.0  0.99470  3.14       0.53   12                   16.0                  75.0  0.99200  3.18       0.63   13                   48.0                 143.0  0.99120  3.54       0.52   14                   41.0                 172.0  1.00020  2.98       0.67   15                   28.0                 112.0  0.99140  3.25       0.55   16                   30.0                  99.0  0.99280  3.24       0.36   17                   29.0                  75.0  0.98920  3.33       0.39   18                   17.0                 171.0  0.99170  3.12       0.53   19                   34.0                 133.0  0.99550  3.22       0.50   20                   29.0                  75.0  0.98920  3.33       0.39   21                   19.0                 102.0  0.99120  3.17       0.35   22                   41.0                 122.0  0.99300  3.47       0.48   23                   25.0                 168.0  0.99370  3.05       0.51   24                   16.0                 142.0  0.99510  3.42       0.47   25                   56.0                 245.0  0.99550  3.25       0.50   26                   35.0                 146.0  0.99300  3.45       0.44   27                   32.0                 141.0  0.99610  3.38       0.53   28                   17.0                 132.0  0.99140  3.19       0.49   29                   37.0                 114.0  0.99060  3.10       0.71   ...                   ...                   ...      ...   ...        ...   4868                 42.0                 124.0  0.99324  3.31       0.64   4869                  8.0                  81.0  0.99626  3.19       0.51   4870                 29.0                 132.0  0.99188  3.15       0.36   4871                 20.0                  98.0  0.98970  3.37       0.55   4872                 50.0                 179.0  0.99622  3.14       0.60   4873                 33.0                 122.0  0.99044  3.33       0.52   4874                 16.0                 125.0  0.99282  3.49       0.49   4875                 18.0                  97.0  0.99245  3.12       0.41   4876                 34.0                 117.0  0.99132  3.36       0.59   4877                 12.0                  82.0  0.99286  3.25       0.36   4878                  6.0                  81.0  0.99234  3.24       0.35   4879                 68.0                 170.0  0.99494  3.15       0.50   4880                 68.0                 170.0  0.99494  3.15       0.50   4881                 34.0                 118.0  0.99540  3.07       0.50   4882                 45.0                 156.0  0.99184  3.26       0.38   4883                 60.0                 148.0  0.98964  3.27       0.35   4884                 68.0                 174.0  0.99492  3.14       0.50   4885                 68.0                 170.0  0.99494  3.15       0.50   4886                 45.0                 121.0  0.99168  3.21       1.08   4887                  5.0                  56.0  0.98928  3.04       0.79   4888                 38.0                 127.0  0.99330  3.04       0.54   4889                 34.0                 118.0  0.99540  3.07       0.50   4890                 25.0                 100.0  0.98938  3.06       0.44   4891                 38.0                 121.0  0.99074  3.24       0.46   4892                 29.0                 112.0  0.99298  3.29       0.54   4893                 24.0                  92.0  0.99114  3.27       0.50   4894                 57.0                 168.0  0.99490  3.15       0.46   4895                 30.0                 111.0  0.99254  2.99       0.46   4896                 20.0                 110.0  0.98869  3.34       0.38   4897                 22.0                  98.0  0.98941  3.26       0.32           alcohol  quality  0      8.800000        6  1      9.500000        6  2     10.100000        6  3      9.900000        6  4      9.900000        6  5     10.100000        6  6      9.600000        6  7      8.800000        6  8      9.500000        6  9     11.000000        6  10    12.000000        5  11     9.700000        5  12    10.800000        5  13    12.400000        7  14     9.700000        5  15    11.400000        7  16     9.600000        6  17    12.800000        8  18    11.300000        6  19     9.500000        5  20    12.800000        8  21    11.000000        7  22    10.500000        8  23     9.300000        5  24    10.000000        6  25    10.400000        6  26    10.000000        6  27    10.500000        6  28    11.600000        6  29    12.300000        7  ...         ...      ...  4868  10.800000        6  4869   9.800000        6  4870  11.450000        7  4871  12.050000        6  4872   9.700000        5  4873  11.900000        6  4874  10.000000        6  4875   9.700000        6  4876  11.600000        7  4877   8.800000        5  4878   9.500000        4  4879   9.533333        6  4880   9.533333        6  4881   9.400000        6  4882  10.700000        5  4883  11.500000        6  4884   9.600000        5  4885   9.550000        6  4886  12.150000        7  4887  13.000000        7  4888   9.200000        5  4889   9.400000        6  4890  11.800000        6  4891  10.600000        6  4892   9.700000        5  4893  11.200000        6  4894   9.600000        5  4895   9.400000        6  4896  12.800000        7  4897  11.800000        6  [4898 rows x 12 columns]&gt;</code></pre><p>在读取数据集时，红酒和白酒是分别存在于两个DataFrame变量中的，为了方便分类任务，需要将两个变量进行合并。下面对数据作预处理，然后就可以开始搭建自己的神经网络了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将红酒数据集添加一列‘type=1’</span></div><div class="line">red[<span class="string">'type'</span>]=<span class="number">1</span></div><div class="line"><span class="comment">#将白酒数据集添加“type=0”</span></div><div class="line">white[<span class="string">'type'</span>]=<span class="number">0</span></div><div class="line"><span class="comment">#将“white”、增补到“red”之后</span></div><div class="line">wines=red.append(white,ignore_index=<span class="keyword">True</span>)</div><div class="line">wines</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2017/07/27/59798df875dd8.png" alt="wines.png"></p><h1 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h1><p>现在我们已经有了完整数据集，可以再做一些更深入的数据挖掘。协方差矩阵图像就是一种很好的方法，可以直观地展示变量之间的相关性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">corr=wines.corr()</div><div class="line">sns.heatmap(corr,</div><div class="line">           xticklabels=corr.columns.values,</div><div class="line">           yticklabels=corr.columns.values)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2017/07/27/59798d8ad45f3.png" alt="output_7_0.png"></p><h1 id="训练集与测试集"><a href="#训练集与测试集" class="headerlink" title="训练集与测试集"></a>训练集与测试集</h1><p>大多数分类数据，都不是每个类别的样本恰好一样多，这种不平衡就会导致一些分类上的问题。（比如一个数据集里，两个类别的数量比例是<code>7:3</code>，那只要算法全部猜测为多的那一类，也能得到<code>70%</code>的正确率。）这样我们就需要让两个类别的酒都在训练集里出现，而且数量要基本一致，这样才不会产生偏差。</p><p>酒质量的这个数据及就是不平衡的，但我们先不做额外处理，之后可以再衡量分类性能是否有所下降，借助下采样或上采样等方式。现在，先导入<code>sklearn.model_selection</code>里的<code>train_test_split</code>方法，来把数据和标签分配到变量<code>X</code>和<code>y</code>当中。我们还需要调用<code>ravel()</code>函数把数据“展平”，以适应之后的函数输入格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#从Scikit-learn中导入train_test_split模块</span></div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#指定特征变量列</span></div><div class="line">X=wines.iloc[:,<span class="number">0</span>:<span class="number">11</span>]</div><div class="line"><span class="comment">#指定标签列，展平多维数组</span></div><div class="line">y=np.ravel(wines.type)</div><div class="line"><span class="comment">#将数据分割为训练集和测试集</span></div><div class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.33</span>,random_state=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>至此我们已经准备好构建第一个神经网络了，但是还有一件事值得留意，那就是数据的标准化。</p><h1 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h1><p>当有些数据值相隔甚远的时候，就需要进行标准化处理。<code>Scikit-Learn</code>提供了很强力且快捷的方式：从<code>sklearn.preprocessing</code>模块导入<code>StandardScaler</code>工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line">scaler=StandardScaler().fit(X_train)</div><div class="line">X_train=scaler.transform(X_train)</div><div class="line">X_test=scaler.transform(X_test)</div></pre></td></tr></table></figure><h1 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h1><p>在真正开始建模之前，回顾我们一开始的问题：能否根据化学性质，如挥发性酸度或硫酸盐，预测酒是红酒还是白酒？因为这里有两个分类：红or白，所以是个二分类<code>(binary classification)</code>问题，本质上相当于0/1, yes/no。因为神经网络只能处理数值信息，所以之前已经将红/白编码成了0/1。</p><p>多层感知器是一种擅长二分类的神经网络，在本教程开头已经介绍过，多层感知器通常是全连接的，也就是简单地把若干全连接层堆砌起来。在激活函数的选择上，基于熟悉Keras和神经网络的目的，可以使用最最普遍的ReLU函数。</p><p>那么如何开始着手构建呢？一个快捷的方法是使用Keras的序贯模型(Sequential model)：层的线性堆叠。我们可以轻松地创建模型，再把层实例传递给模型，具体的命令是:<code>model=Sequential()</code>。</p><p>现在来想一想多层感知器的结构：输入层，若干隐藏层和输出层。当你构建自己的模型时，必须清楚定义输入形状，模型需要知道输入形状，所以你会发现<code>input_shape, input_dim, input_length</code>或<code>batch_size</code>等。</p><p>全连接层在<code>Keras</code>里称为<code>Dense</code>层，执行了以下操作<code>output = activation(dot(inputs, units) + bias)</code>。注意如果没有激活函数的话，Dense层就只包含两个线性操作：点乘、求和。</p><p>在第一层当中，<code>activation</code>参数取值<code>relu</code>，之后定义了<code>input_shape=(11, )</code>，因为有11个特征。第一个隐藏层含有16个神经元，所以Dense()的units参数等于16，也就是说模型的输出形状为(*, 16)。units代表的就是权重矩阵，内有对应每个输入节点的权重值。因为没有将use_bias设为TRUE，所以暂时没有偏置项，这也是可行的。</p><p>第二个隐藏层同样使用relu激活函数，这层的输出数组形状为<code>(*, 8)</code>。最后的输出Dense层尺寸为1，用sigmoid激活函数，所以最终的输出结果是一个0-1之间的概率，对应的是样本属于标签1，即红酒的概率。</p><p>请在下方的代码区域搭建神经网络，要求：</p><ul><li>使用Sequential()模型</li><li>共有3层，且第一层的输入参数为(11,)</li><li>输出层使用sigmoid激活函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#导入Sequential模型和Dense层</span></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line">model=Sequential()</div><div class="line"></div><div class="line"><span class="comment">#隐藏层1</span></div><div class="line">model.add(Dense(<span class="number">16</span>,activation=<span class="string">'relu'</span>,input_shape=(<span class="number">11</span>,)))</div><div class="line"><span class="comment">#隐藏层2</span></div><div class="line">model.add(Dense(<span class="number">8</span>,activation=<span class="string">'relu'</span>))</div><div class="line"><span class="comment">#输出层</span></div><div class="line">model.add(Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>))</div></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><p>总的来讲，关于神经网络的架构，有两个关键的决策：</p><ol><li>多少层？</li><li>每层多少个单元？<br>在这个例子中，我们第一层有16个单元，也就是在学习数据表征时的自由度，更多的隐藏单元可以学习更复杂的表征，但是计算消耗也更大，而且容易过拟合<code>(overfitting)</code>。当模型过于复杂的时候，就会出现过拟合：把一些随机的误差或噪音也当作特征，换言之就是训练数据被拟合的“太好了”。所以当我们并没有足够多数据的时候，最好还是用相对小的神经网络，层数也不要太多。</li></ol><p>如果想要获取所建模型的信息，可以使用<code>output_shape</code>或<code>summary()</code>函数，喜爱main列举了几种常用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#输出形状</span></div><div class="line">model.output_shape</div><div class="line"><span class="comment">#模型总览</span></div><div class="line">model.summary()</div><div class="line"><span class="comment">#详细参数</span></div><div class="line">model.get_config()</div><div class="line"><span class="comment">#权重矩阵</span></div><div class="line">model.get_weights()</div></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_1 (Dense)              (None, 16)                192       _________________________________________________________________dense_2 (Dense)              (None, 8)                 136       _________________________________________________________________dense_3 (Dense)              (None, 1)                 9         =================================================================Total params: 337Trainable params: 337Non-trainable params: 0_________________________________________________________________[array([[ -4.98264432e-02,  -8.99875760e-02,   1.66897923e-01,           3.89293462e-01,   2.48389035e-01,   1.94905251e-01,           3.81554663e-02,  -1.70459509e-01,  -4.62478936e-01,           9.45781171e-02,  -9.45084095e-02,   4.50080931e-02,          -2.01654226e-01,  -2.18820870e-02,  -3.53524268e-01,          -3.39704037e-01],        [  4.67661113e-01,   6.37504160e-02,  -2.29388103e-01,          -5.40849864e-02,   2.22171873e-01,   2.39076287e-01,          -3.60502452e-01,  -3.84893119e-01,   1.26932710e-01,           3.79719436e-02,   3.56621891e-01,   1.69539779e-01,           4.34244841e-01,   4.50510353e-01,   2.42370367e-02,          -2.50114679e-01],        [ -3.73600125e-01,  -2.06571698e-01,  -1.06325597e-01,           1.82575583e-02,   9.36785340e-03,  -7.66809583e-02,           3.23935062e-01,   3.03234130e-01,   1.04181617e-01,          -3.18242192e-01,   2.15769619e-01,  -2.10983753e-02,           1.22898072e-01,   3.79836261e-02,  -2.06408739e-01,           1.86543435e-01],        [ -1.59280300e-02,   2.84385353e-01,  -1.80770189e-01,          -6.91838861e-02,  -4.28074747e-01,  -3.27124178e-01,           1.92455947e-02,   4.65576321e-01,   2.14139491e-01,           2.47457176e-01,   9.40738022e-02,  -2.64835954e-01,          -3.01520914e-01,  -2.66410232e-01,   2.50897020e-01,          -2.39203826e-01],        [  9.09360349e-02,  -2.52071738e-01,   1.81674153e-01,           4.17934448e-01,  -4.57543045e-01,   4.53864366e-01,           1.57245368e-01,  -3.64349395e-01,   3.86538893e-01,          -1.76164597e-01,  -5.79869747e-02,  -2.85525113e-01,          -1.39552027e-01,   5.49268723e-03,  -3.44688624e-01,          -2.01445311e-01],        [ -3.61947805e-01,  -4.36158180e-02,   2.21010417e-01,          -4.11448449e-01,   1.11243278e-01,  -1.96210444e-01,          -3.63108486e-01,   3.47647637e-01,   7.67233074e-02,          -4.12058502e-01,  -2.14669198e-01,  -3.62275094e-01,          -1.37348175e-02,   1.43671960e-01,  -1.09374881e-01,          -1.29260212e-01],        [  1.84318751e-01,   1.69243068e-01,   2.64439911e-01,          -3.27584505e-01,  -3.12709033e-01,   2.97704428e-01,           1.93249792e-01,   2.26672620e-01,  -2.32822448e-01,          -3.53965074e-01,   3.30718786e-01,   8.20287764e-02,           1.41222507e-01,  -4.48238492e-01,  -1.47753030e-01,          -4.31054354e-01],        [ -3.64983499e-01,   2.66292900e-01,   8.03867280e-02,          -3.78615826e-01,  -3.46475422e-01,   1.89222127e-01,           2.69394010e-01,   2.37171561e-01,  -3.25533509e-01,           3.10469061e-01,   1.54059440e-01,   4.10036236e-01,           3.57707292e-01,  -4.47573662e-02,  -3.61494094e-01,           2.87418455e-01],        [ -3.18877876e-01,   2.47041434e-01,  -2.29884654e-01,           8.18514526e-02,   2.36380666e-01,  -3.12529325e-01,           2.58298367e-01,  -3.12896848e-01,   4.36720461e-01,           8.30825865e-02,  -1.53442502e-01,   2.92674035e-01,           2.43945867e-01,  -3.45032215e-01,   9.18445289e-02,          -2.73343891e-01],        [  1.14024431e-01,  -1.97158337e-01,   2.65030652e-01,          -3.90317142e-01,  -5.33969104e-02,  -1.00827187e-01,           1.35453552e-01,  -2.08345950e-02,  -3.05458009e-01,           3.28467578e-01,   3.91551107e-01,   3.88602704e-01,           4.19867784e-01,   1.98601454e-01,  -2.90410578e-01,           2.18321770e-01],        [  3.94538552e-01,  -3.01331282e-04,  -4.59927320e-01,           3.52448225e-03,  -2.55332798e-01,  -7.66898394e-02,           1.71944499e-02,   2.51493305e-01,  -6.00979328e-02,           4.07272190e-01,  -1.14112884e-01,  -4.47229087e-01,          -1.85045898e-02,   2.91900188e-01,   4.34516460e-01,          -3.59144658e-01]], dtype=float32), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,         0.,  0.,  0.], dtype=float32), array([[ 0.27758062,  0.32696116, -0.12059772, -0.2686069 ,  0.08139598,          0.38036656,  0.32520974,  0.19151318],        [ 0.43766093,  0.29809725, -0.4557929 , -0.18581784,  0.08751357,         -0.39931965, -0.09964991,  0.17332137],        [-0.2620455 , -0.24762535,  0.35845268, -0.13336289,  0.04007018,         -0.39839149,  0.01755929,  0.11646259],        [-0.28185141, -0.41674638,  0.07205951,  0.46127093,  0.42340422,         -0.12234998, -0.32808745,  0.49965596],        [-0.26166177, -0.4406935 ,  0.3176899 ,  0.32351041, -0.06424642,          0.41437888,  0.36301064,  0.2036624 ],        [-0.27416241, -0.35417187,  0.26924002,  0.32288253, -0.16948187,         -0.35796487, -0.04283953, -0.44096291],        [-0.01216853, -0.30725086, -0.38324308,  0.19532835, -0.30979538,          0.18932819,  0.26240873, -0.4475528 ],        [-0.1612885 ,  0.19788098, -0.19374907, -0.06785023,  0.21359551,          0.3040458 ,  0.39540446,  0.23423409],        [ 0.01686943,  0.07593989,  0.00735629,  0.25039053,  0.25843036,         -0.23249888,  0.02778065, -0.30911994],        [-0.1596216 , -0.25759542, -0.19575047, -0.02004528,  0.22266507,         -0.1529597 , -0.2789892 ,  0.12094378],        [ 0.19889224,  0.44975781,  0.11675143, -0.16397417,  0.25484574,          0.36306274,  0.48795998,  0.47419429],        [-0.45383811,  0.13647282, -0.2559135 ,  0.05184174,  0.02903581,          0.17449057, -0.27694225,  0.13545072],        [ 0.29954553,  0.2175715 , -0.04698312,  0.05174255,  0.25326657,          0.12707448, -0.45172453,  0.41674447],        [-0.34929419,  0.17539358,  0.35529578,  0.26315773, -0.06466413,         -0.19027662, -0.204934  , -0.33771062],        [-0.30182111, -0.01916206, -0.07562017, -0.34805727, -0.27742755,          0.18699825, -0.30500996, -0.43830144],        [-0.45377958, -0.09787893,  0.16146803, -0.07033706, -0.08875155,          0.04072464, -0.32710898, -0.18625259]], dtype=float32), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32), array([[-0.47236764],        [-0.39696497],        [ 0.32774436],        [ 0.39144981],        [-0.42509505],        [-0.5582419 ],        [ 0.58168077],        [ 0.20806301]], dtype=float32), array([ 0.], dtype=float32)]</code></pre><h1 id="编译和拟合"><a href="#编译和拟合" class="headerlink" title="编译和拟合"></a>编译和拟合</h1><p>现在是时候编译我们的模型并针对数据进行拟合了，相应的函数是compile()和fit()：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(X_train,y_train,epochs=<span class="number">5</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">2</span>)</div></pre></td></tr></table></figure><pre><code>Epoch 1/53s - loss: 0.0807 - acc: 0.9809Epoch 2/53s - loss: 0.0291 - acc: 0.9949Epoch 3/53s - loss: 0.0239 - acc: 0.9959Epoch 4/53s - loss: 0.0211 - acc: 0.9961Epoch 5/53s - loss: 0.0187 - acc: 0.9966&lt;keras.callbacks.History at 0x2aebe9a4d68&gt;</code></pre><p>在编译<code>(compile)</code>过程中，我们为模型指定了adam优化器和<code>binary_crossentropy</code>损失函数。将<code>[&#39;accuracy&#39;]</code>传给参数metrics还可以监测训练过程中的准确度。optimizer和loss是编译模型需要的另外两个参数，最流行的几种优化算法有：随即梯度下降<code>(Stochastic Gradient Descent, SGD)</code>，<code>ADAM</code>和<code>RMSprop</code>。根据所选算法不同，调整的参数也会有不同，不如学习率或者动量(momentum)。损失函数的选择取决于面对的任务：比如回归问题一般用均方误差(<code>Mean Squared Error, MSE)</code>。而在这个二分类的例子中，我们用<code>binary_crossentropy</code>；对于多分类任务，可以使用<code>categorical_crossentropy</code>。</p><p>之后我们对所有<code>X_train</code>和<code>y_trai</code>n的样本迭代训练了5个来回，批次规模为1个样本。<code>verbose</code>则是为了设置输出内容。我们用特定的迭代回数训练模型，一次迭代(epoch)就是把所有训练集筛过一遍，然后对照测试集。批规模<code>(batch size)</code>则定义了每次在网络里传播的样本数量，这样做也是为了在内存有限的情况下优化效率。</p><h2 id="预测值"><a href="#预测值" class="headerlink" title="预测值"></a>预测值</h2><p>下面把训练的模型投入实战，你可以对测试集数据，预测每个样本的标签，只需调用predict()，把结果赋值给变量y_pred:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = model.predict(X_test)</div></pre></td></tr></table></figure><h2 id="评价模型"><a href="#评价模型" class="headerlink" title="评价模型"></a>评价模型</h2><p>现在我们已经建立了模型，并且用于对此前未见的数据做预测，之后肯定还要衡量平价一下整个模型的表现。可以直接拿y_pred和y_test去比较看看中了几个，或者使用其他更高级的度量衡。对这个实例，我们调用evaluate()函数，传递测试数据+测试标签即可得到全局得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">score=model.evaluate(X_test,y_test,verbose=<span class="number">2</span>)</div><div class="line">print(score)</div></pre></td></tr></table></figure><pre><code>[0.022412170111003608, 0.9944055944055944]</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是益智的教程，参考之后动手进行实践了一遍。编译环境windows10+python3.5&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Keras实现简单的手写数字识别</title>
    <link href="http://yoursite.com/2017/07/24/Keras02-MNIST%E6%89%8B%E5%86%99%E5%AE%9E%E4%BE%8B/"/>
    <id>http://yoursite.com/2017/07/24/Keras02-MNIST手写实例/</id>
    <published>2017-07-24T14:54:03.000Z</published>
    <updated>2017-09-15T09:31:20.294Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>Keras实现简单的手写数字识别：构建模型、编译模型、训练数据、输出<br><a id="more"></a></excerpt></p><p><a href="http://www.cnblogs.com/yqtm/p/6924939.html" target="_blank" rel="external">参考</a><br>文中代码有点小bug,加以改正。顺带才了下数据集的坑</p><h2 id="导入需要的函数和包"><a href="#导入需要的函数和包" class="headerlink" title="导入需要的函数和包"></a>导入需要的函数和包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Activation,Dropout</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#从s3.amazonaws.com/img-datasets/mnist.npz下载数据太慢了。挂了代理，结果程序运行崩溃，只好写一个加载本地的文件函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path=<span class="string">'mnist.npz'</span>)</span>:</span></div><div class="line">    f=np.load(path)</div><div class="line">    x_train,y_train=f[<span class="string">'x_train'</span>],f[<span class="string">'y_train'</span>]</div><div class="line">    x_test,y_test=f[<span class="string">'x_test'</span>],f[<span class="string">'y_test'</span>]</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">return</span> (x_train,y_train),(x_test,y_test)</div></pre></td></tr></table></figure><p>Sequential是序贯模型，Dense是用于添加模型的层数，SGD是用于模型变异的时候优化器参数,<br>mnist是用于加载手写识别的数据集，需要在网上下载,下面是mnist.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">from ..utils.data_utils import get_file</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"></div><div class="line">def load_data(path=&apos;mnist.npz&apos;):</div><div class="line">    &quot;&quot;&quot;Loads the MNIST dataset.</div><div class="line"></div><div class="line">    # Arguments</div><div class="line">        path: path where to cache the dataset locally</div><div class="line">            (relative to ~/.keras/datasets).</div><div class="line"></div><div class="line">    # Returns</div><div class="line">        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    path = get_file(path, origin=&apos;https://s3.amazonaws.com/img-datasets/mnist.npz&apos;)</div><div class="line">    f = np.load(path)</div><div class="line">    x_train, y_train = f[&apos;x_train&apos;], f[&apos;y_train&apos;]</div><div class="line">    x_test, y_test = f[&apos;x_test&apos;], f[&apos;y_test&apos;]</div><div class="line">    f.close()</div><div class="line">    return (x_train, y_train), (x_test, y_test)</div></pre></td></tr></table></figure><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">model=Sequential()</div><div class="line">model.add(Dense(<span class="number">500</span>,input_shape=(<span class="number">784</span>,)))<span class="comment">#输入层</span></div><div class="line">model.add(Activation(<span class="string">'tanh'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"></div><div class="line">model.add(Dense(<span class="number">500</span>))<span class="comment">#隐藏层</span></div><div class="line">model.add(Activation(<span class="string">'tanh'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"></div><div class="line">model.add(Dense(<span class="number">10</span>))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure><ol><li>Dense()设定该层的结构，第一个参数表示输出的个数，第二个参数是接受的输入数据的格式。第一层中需要指定输入的格式，在之后的增加的层中输入层节点数默认是上一层的输出个数</li><li>Activation()指定预定义激活函数：softmax，elu、softplus、softsign、relu、、sigmoid、hard_sigmoid、linear<br></li><li>Dropout()用于指定每层丢掉的信息百分比。</li></ol><h2 id="编译模型"><a href="#编译模型" class="headerlink" title="编译模型"></a>编译模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sgd=SGD(lr=<span class="number">0.01</span>,decay=<span class="number">1e-6</span>,momentum=<span class="number">0.9</span>,nesterov=<span class="keyword">True</span>)<span class="comment">#设定学习效率等参数</span></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=sgd)</div><div class="line"><span class="comment">#model.compile(loss = 'categorical_crossentropy', optimizer=sgd, class_mode='categorical') #使用交叉熵作为loss</span></div></pre></td></tr></table></figure><p>调用model.compile()之前初始化一个优化器对象，然后传入该函数,使用优化器sgd来编译模型，用来指定学习效率等参数。编译时指定loss函数，这里使用交叉熵函数作为loss函数。</p><p><em>SGD</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)</div></pre></td></tr></table></figure><p>随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesterov动量</p><p>参数</p><ul><li><code>lr</code>：大于0的浮点数，学习率</li><li><code>momentum</code>：大于0的浮点数，动量参数</li><li><code>decay</code>：大于0的浮点数，每次更新后的学习率衰减值</li><li><code>nesterov</code>：布尔值，确定是否使用Nesterov动量</li></ul><h2 id="读取训练集和测试集"><a href="#读取训练集和测试集" class="headerlink" title="读取训练集和测试集"></a>读取训练集和测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">(x_train,y_train),(x_test,y_test)=load_data()<span class="comment">#直接加载本地文件</span></div><div class="line"><span class="comment">#(x_train,y_train),(x_test,y_test)=mnist.load_data()#不使用mnist提供的load_data函数，</span></div><div class="line">X_train=x_train.reshape(x_train.shape[<span class="number">0</span>],x_train.shape[<span class="number">1</span>]*x_train.shape[<span class="number">2</span>])</div><div class="line">X_test=x_test.reshape(x_test.shape[<span class="number">0</span>],x_test.shape[<span class="number">1</span>]*x_test.shape[<span class="number">2</span>])</div><div class="line">Y_train=(np.arange(<span class="number">10</span>)==y_train[:,<span class="keyword">None</span>]).astype(int)<span class="comment">#将index转换成一个one_hot矩阵</span></div><div class="line">Y_test=(np.arange(<span class="number">10</span>)==y_test[:,<span class="keyword">None</span>]).astype(int)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">print(x_train.shape)</div><div class="line">print(x_train)</div><div class="line">print(x_test.shape)</div><div class="line">print(<span class="string">"y_train:"</span>,y_train,len(y_train))</div><div class="line">print(y_train[:<span class="keyword">None</span>])</div><div class="line">print(y_train[:,<span class="keyword">None</span>]==np.arange(<span class="number">10</span>))</div><div class="line">print(np.arange(<span class="number">10</span>))</div></pre></td></tr></table></figure><pre><code>(60000, 28, 28)[[[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]] [[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]] [[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]] ...,  [[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]] [[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]] [[0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  ...,   [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]  [0 0 0 ..., 0 0 0]]](10000, 28, 28)y_train: [5 0 4 ..., 5 6 8] 60000[5 0 4 ..., 5 6 8][[False False False ..., False False False] [ True False False ..., False False False] [False False False ..., False False False] ...,  [False False False ..., False False False] [False False False ..., False False False] [False False False ..., False  True False]][0 1 2 3 4 5 6 7 8 9]</code></pre><ol><li>读取minst数据集，通过reshape()函数转换数据的格式。</li><li>如果我们打印x_train.shape会发现它是(60000,28,28)，即一共60000个数据，每个数据是28*28的图片。通过reshape转换为(60000,784)的线性张量。</li><li>如果我们打印y_train会发现它是一组表示每张图片的表示数字的数组，通过numpy的arange()和astype()函数将每个数字转换为一组长度为10的张量，代表的数字的位置是1，其它位置为0.</li></ol><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train,Y_train,batch_size=<span class="number">200</span>,epochs=<span class="number">100</span>,shuffle=<span class="keyword">True</span>,verbose=<span class="number">1</span>,validation_split=<span class="number">0.3</span>)</div></pre></td></tr></table></figure><pre><code>Train on 42000 samples, validate on 18000 samplesEpoch 1/10042000/42000 [==============================] - 5s - loss: 1.2457 - val_loss: 0.5666Epoch 2/10042000/42000 [==============================] - 4s - loss: 0.9481 - val_loss: 0.4958Epoch 3/10042000/42000 [==============================] - 4s - loss: 0.8623 - val_loss: 0.4659Epoch 4/10042000/42000 [==============================] - 4s - loss: 0.8145 - val_loss: 0.4691Epoch 5/10042000/42000 [==============================] - 4s - loss: 0.7788 - val_loss: 0.4342Epoch 6/10042000/42000 [==============================] - 4s - loss: 0.7225 - val_loss: 0.4105Epoch 7/10042000/42000 [==============================] - 4s - loss: 0.7338 - val_loss: 0.3970Epoch 8/10042000/42000 [==============================] - 4s - loss: 0.6848 - val_loss: 0.3961Epoch 9/10042000/42000 [==============================] - 4s - loss: 0.6693 - val_loss: 0.3875Epoch 10/10042000/42000 [==============================] - 4s - loss: 0.6544 - val_loss: 0.3751Epoch 11/10042000/42000 [==============================] - 4s - loss: 0.6276 - val_loss: 0.3681Epoch 12/10042000/42000 [==============================] - 4s - loss: 0.6605 - val_loss: 0.3660Epoch 13/10042000/42000 [==============================] - 4s - loss: 0.6487 - val_loss: 0.3515Epoch 14/10042000/42000 [==============================] - 4s - loss: 0.6426 - val_loss: 0.3646Epoch 15/10042000/42000 [==============================] - 4s - loss: 0.6292 - val_loss: 0.3424Epoch 16/10042000/42000 [==============================] - 4s - loss: 0.6074 - val_loss: 0.3378Epoch 17/10042000/42000 [==============================] - 4s - loss: 0.5844 - val_loss: 0.3320Epoch 18/10042000/42000 [==============================] - 4s - loss: 0.5753 - val_loss: 0.3363Epoch 19/10042000/42000 [==============================] - 4s - loss: 0.5570 - val_loss: 0.3199Epoch 20/10042000/42000 [==============================] - 4s - loss: 0.5452 - val_loss: 0.3108Epoch 21/10042000/42000 [==============================] - 4s - loss: 0.5320 - val_loss: 0.3108Epoch 22/10042000/42000 [==============================] - 4s - loss: 0.5354 - val_loss: 0.3024Epoch 23/10042000/42000 [==============================] - 4s - loss: 0.5172 - val_loss: 0.2973Epoch 24/10042000/42000 [==============================] - 4s - loss: 0.5222 - val_loss: 0.3037Epoch 25/10042000/42000 [==============================] - 4s - loss: 0.5208 - val_loss: 0.2940Epoch 26/10042000/42000 [==============================] - 4s - loss: 0.5154 - val_loss: 0.2948Epoch 27/10042000/42000 [==============================] - 4s - loss: 0.5258 - val_loss: 0.2918Epoch 28/10042000/42000 [==============================] - 4s - loss: 0.5033 - val_loss: 0.2889Epoch 29/10042000/42000 [==============================] - 4s - loss: 0.4962 - val_loss: 0.2828Epoch 30/10042000/42000 [==============================] - 4s - loss: 0.4848 - val_loss: 0.2761Epoch 31/10042000/42000 [==============================] - 4s - loss: 0.4884 - val_loss: 0.2881Epoch 32/10042000/42000 [==============================] - 4s - loss: 0.4873 - val_loss: 0.2794Epoch 33/10042000/42000 [==============================] - 4s - loss: 0.4823 - val_loss: 0.2686Epoch 34/10042000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2788Epoch 35/10042000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2732Epoch 36/10042000/42000 [==============================] - 4s - loss: 0.4786 - val_loss: 0.2880Epoch 37/10042000/42000 [==============================] - 4s - loss: 0.4829 - val_loss: 0.2729Epoch 38/10042000/42000 [==============================] - 4s - loss: 0.4744 - val_loss: 0.2731Epoch 39/10042000/42000 [==============================] - 4s - loss: 0.4564 - val_loss: 0.2698Epoch 40/10042000/42000 [==============================] - 4s - loss: 0.4614 - val_loss: 0.2629Epoch 41/10042000/42000 [==============================] - 4s - loss: 0.4673 - val_loss: 0.2586Epoch 42/10042000/42000 [==============================] - 4s - loss: 0.4666 - val_loss: 0.2524Epoch 43/10042000/42000 [==============================] - 4s - loss: 0.4545 - val_loss: 0.2682Epoch 44/10042000/42000 [==============================] - 4s - loss: 0.4550 - val_loss: 0.2653Epoch 45/10042000/42000 [==============================] - 4s - loss: 0.4426 - val_loss: 0.2537Epoch 46/10042000/42000 [==============================] - 4s - loss: 0.4322 - val_loss: 0.2523Epoch 47/10042000/42000 [==============================] - 4s - loss: 0.4541 - val_loss: 0.2552Epoch 48/10042000/42000 [==============================] - 4s - loss: 0.4465 - val_loss: 0.2493Epoch 49/10042000/42000 [==============================] - 4s - loss: 0.4366 - val_loss: 0.2445Epoch 50/10042000/42000 [==============================] - 4s - loss: 0.4362 - val_loss: 0.2458Epoch 51/10042000/42000 [==============================] - 4s - loss: 0.4388 - val_loss: 0.2446Epoch 52/10042000/42000 [==============================] - 4s - loss: 0.4440 - val_loss: 0.2551Epoch 53/10042000/42000 [==============================] - 4s - loss: 0.4278 - val_loss: 0.2469Epoch 54/10042000/42000 [==============================] - 4s - loss: 0.4185 - val_loss: 0.2416Epoch 55/10042000/42000 [==============================] - 4s - loss: 0.4086 - val_loss: 0.2332Epoch 56/10042000/42000 [==============================] - 4s - loss: 0.4005 - val_loss: 0.2407Epoch 57/10042000/42000 [==============================] - 4s - loss: 0.4064 - val_loss: 0.2396Epoch 58/10042000/42000 [==============================] - 4s - loss: 0.4063 - val_loss: 0.2384Epoch 59/10042000/42000 [==============================] - 4s - loss: 0.4020 - val_loss: 0.2358Epoch 60/10042000/42000 [==============================] - 4s - loss: 0.4008 - val_loss: 0.2332Epoch 61/10042000/42000 [==============================] - 4s - loss: 0.4045 - val_loss: 0.2338Epoch 62/10042000/42000 [==============================] - 4s - loss: 0.4153 - val_loss: 0.2346Epoch 63/10042000/42000 [==============================] - 4s - loss: 0.4102 - val_loss: 0.2279Epoch 64/10042000/42000 [==============================] - 4s - loss: 0.4013 - val_loss: 0.2337Epoch 65/10042000/42000 [==============================] - 4s - loss: 0.3945 - val_loss: 0.2312Epoch 66/10042000/42000 [==============================] - 4s - loss: 0.3917 - val_loss: 0.2243Epoch 67/10042000/42000 [==============================] - 4s - loss: 0.3780 - val_loss: 0.2219Epoch 68/10042000/42000 [==============================] - 4s - loss: 0.3781 - val_loss: 0.2249Epoch 69/10042000/42000 [==============================] - 4s - loss: 0.3755 - val_loss: 0.2192Epoch 70/10042000/42000 [==============================] - 4s - loss: 0.3814 - val_loss: 0.2164Epoch 71/10042000/42000 [==============================] - 4s - loss: 0.3843 - val_loss: 0.2197Epoch 72/10042000/42000 [==============================] - 4s - loss: 0.3835 - val_loss: 0.2228Epoch 73/10042000/42000 [==============================] - 4s - loss: 0.3908 - val_loss: 0.2281Epoch 74/10042000/42000 [==============================] - 4s - loss: 0.3881 - val_loss: 0.2185Epoch 75/10042000/42000 [==============================] - 4s - loss: 0.3870 - val_loss: 0.2108Epoch 76/10042000/42000 [==============================] - 4s - loss: 0.3731 - val_loss: 0.2112Epoch 77/10042000/42000 [==============================] - 4s - loss: 0.3685 - val_loss: 0.2069Epoch 78/10042000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.2059Epoch 79/10042000/42000 [==============================] - 4s - loss: 0.3626 - val_loss: 0.2073Epoch 80/10042000/42000 [==============================] - 4s - loss: 0.3594 - val_loss: 0.2053Epoch 81/10042000/42000 [==============================] - 4s - loss: 0.3489 - val_loss: 0.2001Epoch 82/10042000/42000 [==============================] - 4s - loss: 0.3521 - val_loss: 0.2007Epoch 83/10042000/42000 [==============================] - 4s - loss: 0.3488 - val_loss: 0.2029Epoch 84/10042000/42000 [==============================] - 4s - loss: 0.3531 - val_loss: 0.1984Epoch 85/10042000/42000 [==============================] - 4s - loss: 0.3545 - val_loss: 0.2034Epoch 86/10042000/42000 [==============================] - 4s - loss: 0.3559 - val_loss: 0.2053Epoch 87/10042000/42000 [==============================] - 4s - loss: 0.3551 - val_loss: 0.2019Epoch 88/10042000/42000 [==============================] - 4s - loss: 0.3538 - val_loss: 0.2043Epoch 89/10042000/42000 [==============================] - 4s - loss: 0.3498 - val_loss: 0.2050Epoch 90/10042000/42000 [==============================] - 4s - loss: 0.3566 - val_loss: 0.2076Epoch 91/10042000/42000 [==============================] - 4s - loss: 0.3573 - val_loss: 0.2052Epoch 92/10042000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.1994Epoch 93/10042000/42000 [==============================] - 4s - loss: 0.3561 - val_loss: 0.2004Epoch 94/10042000/42000 [==============================] - 4s - loss: 0.3473 - val_loss: 0.2015Epoch 95/10042000/42000 [==============================] - 4s - loss: 0.3463 - val_loss: 0.1951Epoch 96/10042000/42000 [==============================] - 4s - loss: 0.3485 - val_loss: 0.1985Epoch 97/10042000/42000 [==============================] - 4s - loss: 0.3357 - val_loss: 0.1994Epoch 98/10042000/42000 [==============================] - 4s - loss: 0.3399 - val_loss: 0.1965Epoch 99/10042000/42000 [==============================] - 4s - loss: 0.3408 - val_loss: 0.1931Epoch 100/10042000/42000 [==============================] - 4s - loss: 0.3366 - val_loss: 0.1956&lt;keras.callbacks.History at 0x2a5fdb3d278&gt;</code></pre><ul><li>batch_size表示每个训练块包含的数据个数，</li><li>epochs表示训练的次数，</li><li>shuffle表示是否每次训练后将batch打乱重排，</li><li>verbose表示是否输出进度log，</li><li>validation_split指定验证集占比</li></ul><h2 id="输出测试结果"><a href="#输出测试结果" class="headerlink" title="输出测试结果"></a>输出测试结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"test set"</span>)</div><div class="line">scores = model.evaluate(X_test,Y_test,batch_size=<span class="number">200</span>,verbose=<span class="number">1</span>)</div><div class="line">print(<span class="string">""</span>)</div><div class="line">print(<span class="string">"The test loss is %f"</span> % scores)</div><div class="line">result = model.predict(X_test,batch_size=<span class="number">200</span>,verbose=<span class="number">1</span>)</div><div class="line"></div><div class="line">result_max = np.argmax(result, axis = <span class="number">1</span>)</div><div class="line">test_max = np.argmax(Y_test, axis = <span class="number">1</span>)</div><div class="line"></div><div class="line">result_bool = np.equal(result_max, test_max)</div><div class="line">true_num = np.sum(result_bool)</div><div class="line">print(<span class="string">""</span>)</div><div class="line">print(<span class="string">"The accuracy of the model is %f"</span> % (true_num/len(result_bool)))</div></pre></td></tr></table></figure><pre><code>test set 8800/10000 [=========================&gt;....] - ETA: 0sThe test loss is 0.18595810000/10000 [==============================] - 0s     The accuracy of the model is 0.943400</code></pre><ul><li>model.evaluate()计算了测试集中的识别的loss值。</li><li>通过model.predict()，我们可以得到对于测试集中每个数字的识别结果，每个数字对应一个表示每个数字都是多少概率的长度为10的张量。</li><li><p>通过np.argmax()，我们得到每个数字的识别结果和期望的识别结果</p></li><li><p>通过np.equal()，我们得到每个数字是否识别正确</p></li><li><p>通过np.sum()得到识别正确的总的数字个数</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;Keras实现简单的手写数字识别：构建模型、编译模型、训练数据、输出&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用Python和R语言从头开始理解和编写神经网络</title>
    <link href="http://yoursite.com/2017/07/24/Python26-%E4%BD%BF%E7%94%A8Python%E5%92%8CR%E8%AF%AD%E8%A8%80%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E7%90%86%E8%A7%A3%E5%92%8C%E7%BC%96%E5%86%99%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2017/07/24/Python26-使用Python和R语言从头开始理解和编写神经网络/</id>
    <published>2017-07-24T14:46:25.000Z</published>
    <updated>2017-09-15T09:31:20.294Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是<a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" target="_blank" rel="external">原文</a>的翻译过来的，自己在学习和阅读之后觉得文章非常不错，文章结构清晰，由浅入深、从理论到代码实现，最终将神经网络的概念和工作流程呈现出来。自己将其翻译成中文，以便以后阅读和复习和网友参考。因时间（文字纯手打加配图）紧促和翻译水平有限，文章有不足之处请大家指正。<br><a id="more"></a></excerpt></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>你可以通过两种方式学习和实践一个概念：</p><ul><li>选项1：您可以了解一个特定主题的整个理论，然后寻找应用这些概念的方法。所以，你阅读整个算法的工作原理，背后的数学知识、假设理论、局限，然后去应用它。这样学习稳健但是需要花费大量的时间去准备。</li><li>选项2：从简单的基础开始，并就此主题研究直觉上的知识。接下来，选择一个问题并开始解决它。在解决问题的同时了解这些概念，保持调整并改善您对此问题的理解。所以，你去了解如何应用一个算法——实践并应用它。一旦你知道如何应用它，请尝试使用不同的参数和测试值，极限值去测试算法和继续优化对算法的理解。</li></ul><p>我更喜欢选项2，并采取这种方法来学习任何新的话题。我可能无法告诉你算法背后的整个数学，但我可以告诉你直觉上的知识以及基于实验和理解来应用算法的最佳场景。</p><p>在与其他人交流的过程中，我发现人们不用花时间来发展这种直觉，所以他们能够以正确的方式努力地去解决问题。</p><p>在本文中，我将从头开始讨论一个神经网络的构建，更多地关注研究这种直觉上的知识来实现神经网络。我们将在“Python”和“R”中编写代码。读完本篇文章后，您将了解神经网络如何工作，如何初始化权重，以及如何使用反向传播进行更新。</p><p>让我们开始吧</p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul><li>神经网络背后的简单直觉知识</li><li>多层感知器及其基础知识</li><li>涉及神经网络方法的步骤</li><li>可视化神经网络工作方法的步骤</li><li>使用Numpy（Python）实现NN</li><li>使用R实现NN</li><li>[可选]反向传播算法的数学观点</li></ul><h1 id="神经网络背后的直观知识"><a href="#神经网络背后的直观知识" class="headerlink" title="神经网络背后的直观知识"></a>神经网络背后的直观知识</h1><p>如果您是开发人员或了解一种工作——知道如何在代码中调试错误。您可以通过改变输入或条件来触发各种测试用例，并查找输出，输出的变化提供了一个提示：在代码中，去哪里寻找bug？ - 哪个模块要检查，哪些行要阅读。找到bug后，您进行更改并继续运行，直到您能够运行正确的代码或者实现应用程序。</p><p>神经网络的工作方式非常相似。它需要多个输入，通过来自多个隐藏层的多个神经元进行处理，并使用输出层返回结果。这个结果估计过程在技术上被称为“前向传播”。</p><p>接下来，我们将结果与实际输出进行比较。任务是使神经网络的输出接近实际（期望的）输出。在这些神经元中，每一个都会对最终输出产生一些误差，你如何减少这些误差呢？</p><p>我们尝试最小化那些对错误“贡献”更多的神经元的值和权重，并且在返回到神经网络的神经元并发现误差在哪里时发生。这个过程被称为“向后传播”。</p><p>为了减少迭代次数来实现最小化误差，神经网络通常使用称为“梯度下降”的算法，来快速有效地优化任务。</p><p>的确 ，这就是神经网络如何工作的！我知道这是一个非常简单的表示，但它可以帮助您以简单的方式理解事物。</p><h1 id="多层感知器及其基础知识"><a href="#多层感知器及其基础知识" class="headerlink" title="多层感知器及其基础知识"></a>多层感知器及其基础知识</h1><p>就像原子是形成地球上任何物质的基础 - 神经网络的基本形成单位是感知器。 那么，什么是感知器呢？</p><p>感知器可以被理解为需要多个输入并产生一个输出的任何东西。 例如，看下面的图片<br><img src="https://i.loli.net/2017/07/24/59756c063bbec.png" alt="感知器" title="感知器"><br>上述结构需要三个输入并产生一个输出，下一个逻辑问题是输入和输出之间的关系是什么？让我们从基本的方式着手，寻求更复杂的方法。</p><p>下面我讨论了三种创建输入输出关系的方法：</p><ol><li>通过直接组合输入和计算基于阈值的输出。例如：取x1 = 0，x2 = 1，x3 = 1并设置阈值= 0。因此，如果<code>x1 + x2 + x3&gt; 0</code>，则输出为1，否则为0.可以看出，在这种情况下，感知器会将输出计算为1。</li><li>接下来，让我们为输入添加权重。权重重视输入。例如，您分别为x1，x2和x3分配w1 = 2，w2 = 3和w3 = 4。为了计算输出，我们将输入与相应权重相乘，并将其与阈值进行比较，如w1 <em> x1 + w2 </em> x2 + w3 * x3&gt;阈值。与x1和x2相比，这些权重对于x3显得更重要。</li><li>最后，让我们添加偏置量：每个感知器也有一个偏置量，可以被认为是感知器多么灵活。它与某种线性函数y = ax + b的常数b类似，它允许我们上下移动线以适应数据更好的预测。假设没有b，线将始终通过原点（0，0），并且可能会得到较差的拟合。例如，感知器可以具有两个输入，在这种情况下，它需要三个权重。每个输入一个，偏置一个。现在输入的线性表示将如下所示：w1 <em> x1 + w2 </em> x2 + w3 <em> x3 + 1 </em> b。</li></ol><p>但是，上面所讲的感知器之间的关系都是线性的，并没有那么有趣。所以，人们认为将感知器演化成现在所谓的人造神经元，对于输入和偏差，神经元将使用非线性变换（激活函数）。</p><h1 id="什么是激活函数？"><a href="#什么是激活函数？" class="headerlink" title="什么是激活函数？"></a>什么是激活函数？</h1><p>激活函数将加权输入<code>（w1 * x1 + w2 * x2 + w3 * x3 + 1 * b）</code>的和作为参数，并返回神经元的输出。<br><img src="https://i.loli.net/2017/07/24/59757bc8a16d3.png" alt="激活函数" title="激活函数"></p><p>在上式中，我们用x0表示1，w0表示b。</p><p>激活函数主要用于进行非线性变换，使我们能够拟合非线性假设或估计复杂函数。 有多种激活功能，如：<code>“Sigmoid”</code>，<code>“Tanh”</code>，<code>ReLu</code>等等。</p><h1 id="前向传播，反向传播和训练次数-epochs"><a href="#前向传播，反向传播和训练次数-epochs" class="headerlink" title="前向传播，反向传播和训练次数(epochs)"></a>前向传播，反向传播和训练次数(epochs)</h1><p>到目前为止，我们已经计算了输出，这个过程被称为“正向传播”。 但是如果估计的输出远离实际输出（非常大的误差）怎么办？ 下面正是我们在神经网络中所做的：基于错误更新偏差和权重。 这种权重和偏差更新过程被称为“反向传播”。</p><p>反向传播（BP）算法通过确定输出处的损耗（或误差），然后将其传播回网络来工作， 更新权重以最小化每个神经元产生的错误。 最小化误差的第一步是确定每个节点w.r.t.的梯度（Derivatives），最终实现输出。 要获得反向传播的数学视角，请参阅下面的部分。</p><p>这一轮的前向和后向传播迭代被称为一个训练迭代也称为“Epoch”。<code>ps:e（一）poch（波）的意思;一个epoch是指把所有训练数据完整的过一遍</code></p><h1 id="多层感知器"><a href="#多层感知器" class="headerlink" title="多层感知器"></a>多层感知器</h1><p>现在，我们来看看多层感知器。 到目前为止，我们已经看到只有一个由3个输入节点组成的单层，即x1，x2和x3，以及由单个神经元组成的输出层。 但是，出于实际，单层网络只能做到这一点。 如下所示，MLP由层叠在输入层和输出层之间的许多隐层组成。<br><img src="https://i.loli.net/2017/07/24/59757fa6a6e02.png" alt="多层感知器" title="多层感知器"><br>上面的图像只显示一个单一的隐藏层，但实际上可以包含多个隐藏层。 在MLP的情况下要记住的另一点是，所有层都完全连接，即层中的每个节点（输入和输出层除外）连接到上一层和下一层中的每个节点。让我们继续下一个主题，即神经网络的训练算法（最小化误差）。 在这里，我们将看到最常见的训练算法称为梯度下降。</p><h1 id="全批量梯度下降和随机梯度下降"><a href="#全批量梯度下降和随机梯度下降" class="headerlink" title="全批量梯度下降和随机梯度下降"></a>全批量梯度下降和随机梯度下降</h1><p>Gradient Descent的第二个变体通过使用相同的更新算法执行更新MLP的权重的相同工作，但差异在于用于更新权重和偏差的训练样本的数量。</p><p>全部批量梯度下降算法作为名称意味着使用所有的训练数据点来更新每个权重一次，而随机渐变使用1个或更多（样本），但从不使整个训练数据更新权重一次。</p><p>让我们用一个简单的例子来理解这个10个数据点的数据集，它们有两个权重w1和w2。</p><ul><li><p>全批：您可以使用10个数据点（整个训练数据），并计算w1（Δw1）的变化和w2（Δw2）的变化，并更新w1和w2。</p></li><li><p>SGD：使用第一个数据点并计算w1（Δw1）的变化，并改变w2（Δw2）并更新w1和w2。 接下来，当您使用第二个数据点时，您将处理更新的权重</p></li></ul><h1 id="神经网络方法的步骤"><a href="#神经网络方法的步骤" class="headerlink" title="神经网络方法的步骤"></a>神经网络方法的步骤</h1><p><img src="https://i.loli.net/2017/07/24/59757fa6a6e02.png" alt="多层感知器" title="多层感知器"><br>我们来看一步一步地构建神经网络的方法（MLP与一个隐藏层，类似于上图所示的架构）。 在输出层，我们只有一个神经元，因为我们正在解决二进制分类问题（预测0或1）。 我们也可以有两个神经元来预测两个类的每一个。</p><p>先看一下广泛的步骤：</p><ol><li><p>我们输入和输出</p><ul><li>X作为输入矩阵</li><li>y作为输出矩阵</li></ul></li><li><p>我们用随机值初始化权重和偏差（这是一次启动，在下一次迭代中，我们将使用更新的权重和偏差）。 让我们定义：</p><ul><li>wh作为权重矩阵隐藏层</li><li>bh作为隐藏层的偏置矩阵</li><li>wout作为输出层的权重矩阵</li><li>bout作为偏置矩阵作为输出层</li></ul></li><li><p>我们将输入和权重的矩阵点积分配给输入和隐藏层之间的边，然后将隐层神经元的偏差添加到相应的输入，这被称为线性变换：</p><p>   <code>hidden_layer_input= matrix_dot_product(X,wh) + bh</code></p></li><li>使用激活函数（Sigmoid）执行非线性变换。 Sigmoid将返回输出1/(1 + exp(-x)).<br>   <code>hiddenlayer_activations = sigmoid(hidden_layer_input)</code></li><li><p>对隐藏层激活进行线性变换（取矩阵点积，并加上输出层神经元的偏差），然后应用激活函数（再次使用Sigmoid，但是根据您的任务可以使用任何其他激活函数 ）来预测输出</p><p>   <code>output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout</code></p><p>   <code>output = sigmoid(output_layer_input)</code></p><p><strong>所有上述步骤被称为“前向传播”（Forward Propagation）</strong></p></li><li>将预测与实际输出进行比较，并计算误差梯度（实际预测值）。 误差是均方损失= ((Y-t)^2)/2<br>   <code>E = y – output</code></li><li><p>计算隐藏和输出层神经元的斜率/斜率（为了计算斜率，我们计算每个神经元的每层的非线性激活x的导数）。 S形梯度可以返回 <code>x * (1 – x)</code>.</p><p>   <code>slope_output_layer = derivatives_sigmoid(output)</code></p><p>   <code>slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</code></p></li><li>计算输出层的变化因子（delta），取决于误差梯度乘以输出层激活的斜率<br>   <code>d_output = E * slope_output_layer</code></li><li>在这一步，错误将传播回网络，这意味着隐藏层的错误。 为此，我们将采用输出层三角形的点积与隐藏层和输出层之间的边缘的重量参数（wout.T）。<br>   <code>Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)</code></li><li>计算隐层的变化因子（delta），将隐层的误差乘以隐藏层激活的斜率<br>  <code>d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</code></li><li><p>在输出和隐藏层更新权重：网络中的权重可以从为训练示例计算的错误中更新。<br>   <code>wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate</code></p><p>   <code>wh =  wh + matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate</code><br>learning_rate：权重更新的量由称为学习率的配置参数控制）</p></li><li><p>在输出和隐藏层更新偏差：网络中的偏差可以从该神经元的聚合错误中更新。</p><ul><li>bias at output_layer =bias at output_layer + sum of delta of output_layer at row-wise * learning_rate</li><li><p>bias at hidden_layer =bias at hidden_layer + sum of delta of output_layer at row-wise * learning_rate  </p><p><code>bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate</code></p><p><code>bout = bout + sum(d_output, axis=0)*learning_rate</code></p></li></ul><p><strong>从6到12的步骤被称为“向后传播”(Backward Propagation)</strong></p></li></ol><p>一个正向和反向传播迭代被认为是一个训练周期。 如前所述，我们什么时候训练第二次，然后更新权重和偏差用于正向传播。</p><p>以上，我们更新了隐藏和输出层的权重和偏差，我们使用了全批量梯度下降算法。</p><h1 id="神经网络方法的可视化步骤"><a href="#神经网络方法的可视化步骤" class="headerlink" title="神经网络方法的可视化步骤"></a>神经网络方法的可视化步骤</h1><p>我们将重复上述步骤，可视化输入，权重，偏差，输出，误差矩阵，以了解神经网络（MLP）的工作方法。</p><ul><li><p><strong>注意：</strong></p><ul><li>对于良好的可视化图像，我有2或3个位置的十进制小数位。</li><li>黄色填充的细胞代表当前活动细胞</li><li>橙色单元格表示用于填充当前单元格值的输入</li></ul></li></ul><ul><li>步骤1：读取输入和输出<br><img src="https://i.loli.net/2017/07/24/597588fe182b4.png" alt="Step 1"></li><li>步骤2：用随机值初始化权重和偏差（有初始化权重和偏差的方法，但是现在用随机值初始化）<br><img src="https://i.loli.net/2017/07/24/597589475b8bb.png" alt="Step 2"></li><li>步骤3：计算隐层输入： <br><br><code>hidden_layer_input= matrix_dot_product(X,wh) + bh</code><br><img src="https://i.loli.net/2017/07/24/597589a62c037.png" alt="Step 3"></li><li>步骤4：对隐藏的线性输入进行非线性变换 <br><br><code>hiddenlayer_activations = sigmoid(hidden_layer_input)</code><br><img src="https://i.loli.net/2017/07/24/59758a0111ed8.png" alt="Step 4"></li><li><p>步骤5：在输出层执行隐层激活的线性和非线性变换 <br><br><code>output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout</code> <br><br><code>output = sigmoid(output_layer_input)</code><br><img src="https://i.loli.net/2017/07/24/59758a58893ba.png" alt="Step 5"></p></li><li><p>步骤6：计算输出层的误差（E）梯度 <br><br><code>E = y-output</code><br><img src="https://i.loli.net/2017/07/24/59758ad4a72ff.png" alt="Step 6"></p></li><li>步骤7：计算输出和隐藏层的斜率 <br><br><code>Slope_output_layer= derivatives_sigmoid(output)</code> <br><br><code>Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</code><br><img src="https://i.loli.net/2017/07/24/59758b26893ef.png" alt="py26-10.png"></li><li>步骤8：计算输出层的增量 <br><br><code>d_output = E * slope_output_layer*lr</code><br><img src="https://i.loli.net/2017/07/24/59758b61227a9.png" alt="py26-11.png"></li><li>步骤9：计算隐藏层的误差 <br><br><code>Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)</code><br><img src="https://i.loli.net/2017/07/24/59758ba276123.png" alt="py26-12.png"></li><li>步骤10：计算隐藏层的增量 <br><br><code>d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</code><br><img src="https://i.loli.net/2017/07/24/59758bd705865.png" alt="py26-13.png"></li><li>步骤11：更新输出和隐藏层的权重 <br><br><code>wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate</code> <br><br><code>wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate</code><br><img src="https://i.loli.net/2017/07/24/59758c13cc478.png" alt="py26-14.png"></li><li>步骤12：更新输出和隐藏层的偏置量<br><br><code>bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate</code><br><br><code>bout = bout + sum(d_output, axis=0)*learning_rate</code><br><img src="https://i.loli.net/2017/07/24/59758c71210be.png" alt="py26-15.png"></li></ul><p>以上，您可以看到仍然有一个很好的误差而不接近于实际目标值，因为我们已经完成了一次训练迭代。 如果我们多次训练模型，那么这将是一个非常接近的实际结果。 我完成了数千次迭代，我的结果接近实际的目标值（<code>[[0.98032096] [0.96845624] [0.04532167]]</code>）。</p><h1 id="使用Numpy（Python）实现NN"><a href="#使用Numpy（Python）实现NN" class="headerlink" title="使用Numpy（Python）实现NN"></a>使用Numpy（Python）实现NN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment">#Input array</span></div><div class="line">X=np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</div><div class="line"></div><div class="line"><span class="comment">#Output</span></div><div class="line">y=np.array([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</div><div class="line"></div><div class="line"><span class="comment">#Sigmoid Function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span> <span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</div><div class="line"></div><div class="line"><span class="comment">#Derivative of Sigmoid Function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">derivatives_sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> - x)</div><div class="line"></div><div class="line"><span class="comment">#Variable initialization</span></div><div class="line">epoch=<span class="number">5000</span> <span class="comment">#Setting training iterations</span></div><div class="line">lr=<span class="number">0.1</span> <span class="comment">#Setting learning rate</span></div><div class="line">inputlayer_neurons = X.shape[<span class="number">1</span>] <span class="comment">#number of features in data set</span></div><div class="line">hiddenlayer_neurons = <span class="number">3</span> <span class="comment">#number of hidden layers neurons</span></div><div class="line">output_neurons = <span class="number">1</span> <span class="comment">#number of neurons at output layer</span></div><div class="line"></div><div class="line"><span class="comment">#weight and bias initialization</span></div><div class="line">wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))</div><div class="line">bh=np.random.uniform(size=(<span class="number">1</span>,hiddenlayer_neurons))</div><div class="line">wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))</div><div class="line">bout=np.random.uniform(size=(<span class="number">1</span>,output_neurons))</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</div><div class="line">    <span class="comment">#Forward Propogation</span></div><div class="line">    hidden_layer_input1=np.dot(X,wh)</div><div class="line">    hidden_layer_input=hidden_layer_input1 + bh</div><div class="line">    hiddenlayer_activations = sigmoid(hidden_layer_input)</div><div class="line">    output_layer_input1=np.dot(hiddenlayer_activations,wout)</div><div class="line">    output_layer_input= output_layer_input1+ bout</div><div class="line">    output = sigmoid(output_layer_input)</div><div class="line">    <span class="comment">#Backpropagation</span></div><div class="line">    E = y-output</div><div class="line">    slope_output_layer = derivatives_sigmoid(output)</div><div class="line">    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</div><div class="line">    d_output = E * slope_output_layer</div><div class="line">    Error_at_hidden_layer = d_output.dot(wout.T)</div><div class="line">    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</div><div class="line">    wout += hiddenlayer_activations.T.dot(d_output) *lr</div><div class="line">    bout += np.sum(d_output, axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>) *lr</div><div class="line">    wh += X.T.dot(d_hiddenlayer) *lr</div><div class="line">    bh += np.sum(d_hiddenlayer, axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>) *lr</div><div class="line"></div><div class="line">print(<span class="string">"output of Forward Propogation:\n&#123;&#125;"</span>.format(output))</div><div class="line">print(<span class="string">"wout,bout of Backpropagation:\n&#123;&#125;,\n&#123;&#125;"</span>.format(wout,bout))</div></pre></td></tr></table></figure><pre><code>output of Forward Propogation:[[ 0.98497471] [ 0.96956956] [ 0.0416628 ]]wout,bout of Backpropagation:[[ 3.34342103] [-1.97924327] [ 3.90636787]],[[-1.71231223]]</code></pre><h1 id="在R中实现NN"><a href="#在R中实现NN" class="headerlink" title="在R中实现NN"></a>在R中实现NN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># input matrix</span></div><div class="line">X=matrix(c(<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>),nrow = <span class="number">3</span>, ncol=<span class="number">4</span>,byrow = TRUE)</div><div class="line"></div><div class="line"><span class="comment"># output matrix</span></div><div class="line">Y=matrix(c(<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),byrow=FALSE)</div><div class="line"></div><div class="line"><span class="comment">#sigmoid function</span></div><div class="line">sigmoid&lt;-function(x)&#123;</div><div class="line"><span class="number">1</span>/(<span class="number">1</span>+exp(-x))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># derivative of sigmoid function</span></div><div class="line">derivatives_sigmoid&lt;-function(x)&#123;</div><div class="line">x*(<span class="number">1</span>-x)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># variable initialization</span></div><div class="line">epoch=<span class="number">5000</span></div><div class="line">lr=<span class="number">0.1</span></div><div class="line">inputlayer_neurons=ncol(X)</div><div class="line">hiddenlayer_neurons=<span class="number">3</span></div><div class="line">output_neurons=<span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment">#weight and bias initialization</span></div><div class="line">wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=<span class="number">0</span>,sd=<span class="number">1</span>), inputlayer_neurons, hiddenlayer_neurons)</div><div class="line">bias_in=runif(hiddenlayer_neurons)</div><div class="line">bias_in_temp=rep(bias_in, nrow(X))</div><div class="line">bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE)</div><div class="line">wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=<span class="number">0</span>,sd=<span class="number">1</span>), hiddenlayer_neurons, output_neurons)</div><div class="line"></div><div class="line">bias_out=runif(output_neurons)</div><div class="line">bias_out_temp=rep(bias_out,nrow(X))</div><div class="line">bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE)</div><div class="line"><span class="comment"># forward propagation</span></div><div class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:epoch)&#123;</div><div class="line"></div><div class="line">hidden_layer_input1= X%*%wh</div><div class="line">hidden_layer_input=hidden_layer_input1+bh</div><div class="line">hidden_layer_activations=sigmoid(hidden_layer_input)</div><div class="line">output_layer_input1=hidden_layer_activations%*%wout</div><div class="line">output_layer_input=output_layer_input1+bout</div><div class="line">output= sigmoid(output_layer_input)</div><div class="line"></div><div class="line"><span class="comment"># Back Propagation</span></div><div class="line"></div><div class="line">E=Y-output</div><div class="line">slope_output_layer=derivatives_sigmoid(output)</div><div class="line">slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)</div><div class="line">d_output=E*slope_output_layer</div><div class="line">Error_at_hidden_layer=d_output%*%t(wout)</div><div class="line">d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer</div><div class="line">wout= wout + (t(hidden_layer_activations)%*%d_output)*lr</div><div class="line">bout= bout+rowSums(d_output)*lr</div><div class="line">wh = wh +(t(X)%*%d_hiddenlayer)*lr</div><div class="line">bh = bh + rowSums(d_hiddenlayer)*lr</div><div class="line"></div><div class="line">&#125;</div><div class="line">output</div></pre></td></tr></table></figure><h1 id="可选-反向传播算法的数学理解"><a href="#可选-反向传播算法的数学理解" class="headerlink" title="[可选]反向传播算法的数学理解"></a>[可选]反向传播算法的数学理解</h1><p>设Wi为输入层和隐层之间的权重。 Wh是隐层和输出层之间的权重。</p><p>现在，<code>h =σ（u）=σ（WiX）</code>，即h是u的函数，u是Wi和X的函数。这里我们将我们的函数表示为σ</p><p><code>Y =σ（u&#39;）=σ（Whh）</code>，即Y是u’的函数，u’是Wh和h的函数。</p><p>我们将不断参考上述方程来计算偏导数。</p><p>我们主要感兴趣的是找到两个项：∂E/∂Wi和∂E/∂Wh即改变输入和隐藏层之间权重的误差变化，改变隐层和输出之间权重的变化 层。</p><p>但是为了计算这两个偏导数，我们将需要使用部分微分的链规则，因为E是Y的函数，Y是u’的函数，u’是Wi的函数。</p><p>让我们把这个属性很好的用于计算梯度。</p><p>`∂E/∂Wh = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂Wh), ……..(1)</p><p>We know E is of the form E=(Y-t)2/2.</p><p>So, (∂E/∂Y)= (Y-t)`</p><p>现在，σ是一个S形函数，并具有σ（1-σ）形式的有意义的区分。 我敦促读者在他们身边进行验证。<br>所以, <code>(∂Y/∂u’)= ∂( σ(u’)/ ∂u’= σ(u’)(1- σ(u’))</code>.</p><p>但是, <code>σ(u’)=Y, So</code>,</p><p><code>(∂Y/∂u’)=Y(1-Y)</code></p><p>现在得出, <code>( ∂u’/∂Wh)= ∂( Whh)/ ∂Wh = h</code></p><p>取代等式（1）中的值我们得到，</p><p><code>∂E/∂Wh = (Y-t). Y(1-Y).h</code></p><p>所以，现在我们已经计算了隐层和输出层之间的梯度。 现在是计算输入层和隐藏层之间的梯度的时候了。<br><code>∂E/∂Wi =(∂ E/∂ h). (∂h/∂u).( ∂u/∂Wi)</code><br>但是，<code>(∂ E/∂ h) = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h)</code>. 在上述方程中替换这个值得到：</p><p><code>∂E/∂Wi =[(∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h)]. (∂h/∂u).( ∂u/∂Wi)……………(2)</code><br>那么，首先计算隐层和输出层之间的梯度有什么好处？</p><p>如等式（2）所示，我们已经计算出∂E/∂Y和∂Y/∂u’节省了空间和计算时间。 我们会在一段时间内知道为什么这个算法称为反向传播算法。</p><p>让我们计算公式（2）中的未知导数。</p><p><code>∂u’/∂h = ∂(Whh)/ ∂h = Wh</code></p><p><code>∂h/∂u = ∂( σ(u)/ ∂u= σ(u)(1- σ(u))</code></p><p>但是, <code>σ(u)=h, So,</code></p><p><code>(∂Y/∂u)=h(1-h)</code></p><p>得出, <code>∂u/∂Wi = ∂(WiX)/ ∂Wi = X</code></p><p>取代等式（2）中的所有这些值，我们得到：<br><br><code>∂E/∂Wi = [(Y-t). Y(1-Y).Wh].h(1-h).X</code></p><p>所以现在，由于我们已经计算了两个梯度，所以权重可以更新为:</p><p><code>Wh = Wh + η . ∂E/∂Wh</code></p><p> <code>Wi = Wi + η . ∂E/∂Wi</code></p><p>其中η是学习率。</p><p>所以回到这个问题：为什么这个算法叫做反向传播算法？</p><p>原因是：如果您注意到∂E/∂Wh和∂E/∂Wi的最终形式，您将看到术语（Yt）即输出错误，这是我们开始的，然后将其传播回输入 层重量更新。</p><p>那么，这个数学在哪里适合代码？</p><p><code>hiddenlayer_activations= H</code></p><p><code>E = Y-t</code></p><p><code>Slope_output_layer = Y（1-Y）</code></p><p><code>lr =η</code></p><p><code>slope_hidden_layer = h（1-h）</code></p><p><code>wout = Wh</code></p><p>现在，您可以轻松地将代码与数学联系起来。</p><h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>本文主要从头开始构建神经网络，并了解其基本概念。 我希望你现在可以理解神经网络的工作，如前向和后向传播的工作，优化算法（全批次和随机梯度下降），如何更新权重和偏差，Excel中每个步骤的可视化以及建立在python和R的代码.</p><p>因此，在即将到来的文章中，我将解释在Python中使用神经网络的应用，并解决与以下问题相关的现实生活中的挑战：</p><ol><li>计算机视觉</li><li>言语</li><li>自然语言处理</li></ol><p>我在写这篇文章的时候感到很愉快，并希望从你的反馈中学习。 你觉得这篇文章有用吗？ 感谢您的建议/意见。 请随时通过以下意见提出您的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">（转载请注明来源）</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文&lt;/a&gt;的翻译过来的，自己在学习和阅读之后觉得文章非常不错，文章结构清晰，由浅入深、从理论到代码实现，最终将神经网络的概念和工作流程呈现出来。自己将其翻译成中文，以便以后阅读和复习和网友参考。因时间（文字纯手打加配图）紧促和翻译水平有限，文章有不足之处请大家指正。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
