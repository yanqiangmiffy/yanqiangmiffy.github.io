<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yanqiangmiffy</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-08-03T09:53:05.086Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>致Great</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Keras构建卷积神经网络预测“阿三”的年龄</title>
    <link href="http://yoursite.com/2017/08/03/03AgeDetection/"/>
    <id>http://yoursite.com/2017/08/03/03AgeDetection/</id>
    <published>2017-08-03T17:45:25.000Z</published>
    <updated>2017-08-03T09:53:05.086Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>用Keras构建基本的前馈神经网络以及借助卷积层逐步优化预测结果，从海量图片中预测印度人们的年龄。目前结果为（0.750904）<br><a id="more"></a></excerpt></p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>我们的任务是从一个人的面部特征来预测他的年龄(用“Young”“Middle ”“Old”表示)，我们训练的数据集大约有19906多张照片及其每张图片对应的年龄（全是阿三的头像。。。），测试集有6636张图片，首先我们加载数据集，然后我们通过深度学习框架Keras建立、编译、训练模型，预测出6636张人物头像对应的年龄</p>
<h1 id="引入所需要模块"><a href="#引入所需要模块" class="headerlink" title="引入所需要模块"></a>引入所需要模块</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div></pre></td></tr></table></figure>
<h1 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root_dir=os.path.abspath(<span class="string">'E:/data/age'</span>)</div><div class="line">train=pd.read_csv(os.path.join(root_dir,<span class="string">'train.csv'</span>))</div><div class="line">test=pd.read_csv(os.path.join(root_dir,<span class="string">'test.csv'</span>))</div><div class="line"></div><div class="line">print(train.head())</div><div class="line">print(test.head())</div></pre></td></tr></table></figure>
<pre><code>          ID   Class
0    377.jpg  MIDDLE
1  17814.jpg   YOUNG
2  21283.jpg  MIDDLE
3  16496.jpg   YOUNG
4   4487.jpg  MIDDLE
          ID
0  25321.jpg
1    989.jpg
2  19277.jpg
3  13093.jpg
4   5367.jpg
</code></pre><h2 id="随机读取一张图片试下（☺）"><a href="#随机读取一张图片试下（☺）" class="headerlink" title="随机读取一张图片试下（☺）"></a>随机读取一张图片试下（☺）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">i=random.choice(train.index)</div><div class="line">img_name=train.ID[i]</div><div class="line">print(img_name)</div><div class="line">img=Image.open(os.path.join(root_dir,<span class="string">'Train'</span>,img_name))</div><div class="line">img.show()</div><div class="line">print(train.Class[i])</div></pre></td></tr></table></figure>
<pre><code>20188.jpg
MIDDLE
</code></pre><h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><p>我们随机打开几张图片之后，可以发现图片之间的差别比较大。大家感受下：</p>
<ol>
<li><p>质量好的图片：</p>
<ul>
<li>Middle:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022332/mid3.png" alt="**Middle**"></li>
<li>Young:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022132/y2.png" alt="**Young**"></li>
<li>Old:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022442/old1.png" alt="**Old**"></li>
</ul>
</li>
<li>质量差的：<ul>
<li>Middle:<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022313/mid1.png" alt="**Middle**"></li>
</ul>
</li>
</ol>
<p>下面是我们需要面临的问题：</p>
<ol>
<li>图片的尺寸差别：有的图片的尺寸是66x46,而另一张图片尺寸为102x87</li>
<li>人物面貌角度不同：<ul>
<li>侧脸：<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022102/side1.png" alt=""></li>
<li>正脸：<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022113/try1.png" alt=""></li>
</ul>
</li>
<li>图片质量不一（直接上图）:<br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022450/pixel1.png" alt="插图"></li>
<li>亮度和对比度的差异<br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022151/contra1.png" alt="亮度"><br> <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/27022200/contra2.png" alt="对比度"><br>现在，我们只专注下图片尺寸处理，将每一张图片尺寸重置为32x32</li>
</ol>
<h2 id="格式化图片尺寸和将图片转换成numpy数组"><a href="#格式化图片尺寸和将图片转换成numpy数组" class="headerlink" title="格式化图片尺寸和将图片转换成numpy数组"></a>格式化图片尺寸和将图片转换成numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">temp=[]</div><div class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> train.ID:</div><div class="line">    img_path=os.path.join(root_dir,<span class="string">'Train'</span>,img_name)</div><div class="line">    img=Image.open(img_path)</div><div class="line">    img=img.resize((<span class="number">32</span>,<span class="number">32</span>))</div><div class="line">    array=np.array(img)</div><div class="line">    temp.append(array.astype(<span class="string">'float32'</span>))</div><div class="line">train_x=np.stack(temp)</div><div class="line">print(train_x.shape)</div><div class="line">print(train_x.ndim)</div></pre></td></tr></table></figure>
<pre><code>(19906, 32, 32, 3)
4
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">temp=[]</div><div class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> test.ID:</div><div class="line">    img_path=os.path.join(root_dir,<span class="string">'Test'</span>,img_name)</div><div class="line">    img=Image.open(img_path)</div><div class="line">    img=img.resize((<span class="number">32</span>,<span class="number">32</span>))</div><div class="line">    array=np.array(img)</div><div class="line">    temp.append(array.astype(<span class="string">'float32'</span>))</div><div class="line">test_x=np.stack(temp)</div><div class="line">print(test_x.shape)</div></pre></td></tr></table></figure>
<pre><code>(6636, 32, 32, 3)
</code></pre><p>另外我们再归一化图像，这样会使模型训练的更快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">train_x = train_x / <span class="number">255.</span></div><div class="line">test_x = test_x / <span class="number">255.</span></div></pre></td></tr></table></figure>
<p>我们看下图片年龄大致分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.Class.value_counts(normalize=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<pre><code>MIDDLE    0.542751
YOUNG     0.336883
OLD       0.120366
Name: Class, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test[<span class="string">'Class'</span>] = <span class="string">'MIDDLE'</span></div><div class="line">test.to_csv(<span class="string">'sub01.csv'</span>, index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p>将目标变量处理虚拟列，能够使模型更容易接受识别它</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> keras</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line">lb=LabelEncoder()</div><div class="line">train_y=lb.fit_transform(train.Class)</div><div class="line">print(train_y)</div><div class="line">train_y=keras.utils.np_utils.to_categorical(train_y)</div><div class="line">print(train_y)</div><div class="line">print(train_y.shape)</div></pre></td></tr></table></figure>
<pre><code>[0 2 0 ..., 0 0 0]
[[ 1.  0.  0.]
 [ 0.  0.  1.]
 [ 1.  0.  0.]
 ..., 
 [ 1.  0.  0.]
 [ 1.  0.  0.]
 [ 1.  0.  0.]]
(19906, 3)
</code></pre><h1 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#构建神经网络</span></div><div class="line">input_num_units=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div><div class="line">hidden_num_units=<span class="number">500</span></div><div class="line">output_num_units=<span class="number">3</span></div><div class="line">epochs=<span class="number">5</span></div><div class="line">batch_size=<span class="number">128</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Flatten,InputLayer</div><div class="line">model=Sequential(&#123;</div><div class="line">    InputLayer(input_shape=input_num_units),</div><div class="line">    Flatten(),</div><div class="line">    Dense(units=hidden_num_units,activation=<span class="string">'relu'</span>),</div><div class="line">    Dense(input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>),units=output_num_units,activation=<span class="string">'softmax'</span>)</div><div class="line">&#125;)</div><div class="line">model.summary()</div></pre></td></tr></table></figure>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        (None, 32, 32, 3)         0         
_________________________________________________________________
flatten_23 (Flatten)         (None, 3072)              0         
_________________________________________________________________
dense_45 (Dense)             (None, 500)               1536500   
_________________________________________________________________
dense_46 (Dense)             (None, 3)                 1503      
=================================================================
Total params: 1,538,003
Trainable params: 1,538,003
Non-trainable params: 0
_________________________________________________________________
</code></pre><h1 id="编译模型"><a href="#编译模型" class="headerlink" title="编译模型"></a>编译模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])</span></div><div class="line">model.compile(optimizer=<span class="string">'sgd'</span>,loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,verbose=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
19906/19906 [==============================] - 4s - loss: 0.8878 - acc: 0.5809     
Epoch 2/5
19906/19906 [==============================] - 4s - loss: 0.8420 - acc: 0.6077     
Epoch 3/5
19906/19906 [==============================] - 4s - loss: 0.8210 - acc: 0.6214     
Epoch 4/5
19906/19906 [==============================] - 4s - loss: 0.8149 - acc: 0.6194     
Epoch 5/5
19906/19906 [==============================] - 4s - loss: 0.8042 - acc: 0.6305     





&lt;keras.callbacks.History at 0x1d3803e6278&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(train_x, train_y, batch_size=batch_size,epochs=epochs,verbose=<span class="number">1</span>, validation_split=<span class="number">0.2</span>)</div></pre></td></tr></table></figure>
<pre><code>Train on 15924 samples, validate on 3982 samples
Epoch 1/5
15924/15924 [==============================] - 3s - loss: 0.7970 - acc: 0.6375 - val_loss: 0.7854 - val_acc: 0.6396
Epoch 2/5
15924/15924 [==============================] - 3s - loss: 0.7919 - acc: 0.6378 - val_loss: 0.7767 - val_acc: 0.6519
Epoch 3/5
15924/15924 [==============================] - 3s - loss: 0.7870 - acc: 0.6404 - val_loss: 0.7754 - val_acc: 0.6534
Epoch 4/5
15924/15924 [==============================] - 3s - loss: 0.7806 - acc: 0.6439 - val_loss: 0.7715 - val_acc: 0.6524
Epoch 5/5
15924/15924 [==============================] - 3s - loss: 0.7755 - acc: 0.6519 - val_loss: 0.7970 - val_acc: 0.6346





&lt;keras.callbacks.History at 0x1d3800a4eb8&gt;
</code></pre><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>我们使用最基本的模型来处理这个年龄预测结果，并且最终的预测结果为0.6375。接下来，从以下角度尝试优化：</p>
<ol>
<li>使用更好的神经网络模型</li>
<li>增加训练次数</li>
<li>将图片进行灰度处理（因为对于本问题而言，图片颜色不是一个特别重要的特征。）</li>
</ol>
<h1 id="optimize1-使用卷积神经网络"><a href="#optimize1-使用卷积神经网络" class="headerlink" title="optimize1 使用卷积神经网络"></a>optimize1 使用卷积神经网络</h1><p><code>添加卷积层之后，预测准确率有所上涨，从6.3到6.7；最开始epochs轮数是5，训练轮数增加到10，此时准确率为6.87；然后将训练轮数增加到20，结果没有发生变化。</code></p>
<h2 id="Conv2D层"><a href="#Conv2D层" class="headerlink" title="Conv2D层"></a>Conv2D层</h2><p><code>keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding=&#39;valid&#39;, data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)</code></p>
<ul>
<li>filters:输出的维度</li>
<li>strides:卷积的步长</li>
</ul>
<p>更多关于Conv2D的介绍请看<a href="http://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/#conv2d" target="_blank" rel="external">Keras文档Conv2D层</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#参数初始化</span></div><div class="line">filters=<span class="number">10</span></div><div class="line">filtersize=(<span class="number">5</span>,<span class="number">5</span>)</div><div class="line"></div><div class="line">epochs =<span class="number">10</span></div><div class="line">batchsize=<span class="number">128</span></div><div class="line"></div><div class="line">input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line">model = Sequential()</div><div class="line"></div><div class="line">model.add(keras.layers.InputLayer(input_shape=input_shape))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line">model.add(keras.layers.Flatten())</div><div class="line"></div><div class="line">model.add(keras.layers.Dense(units=<span class="number">3</span>, input_dim=<span class="number">50</span>,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x, train_y, epochs=epochs, batch_size=batchsize,validation_split=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">model.summary()</div></pre></td></tr></table></figure>
<pre><code>Train on 13934 samples, validate on 5972 samples
Epoch 1/10
13934/13934 [==============================] - 9s - loss: 0.8986 - acc: 0.5884 - val_loss: 0.8352 - val_acc: 0.6271
Epoch 2/10
13934/13934 [==============================] - 9s - loss: 0.8141 - acc: 0.6281 - val_loss: 0.7886 - val_acc: 0.6474
Epoch 3/10
13934/13934 [==============================] - 9s - loss: 0.7788 - acc: 0.6504 - val_loss: 0.7706 - val_acc: 0.6551
Epoch 4/10
13934/13934 [==============================] - 9s - loss: 0.7638 - acc: 0.6577 - val_loss: 0.7559 - val_acc: 0.6626
Epoch 5/10
13934/13934 [==============================] - 9s - loss: 0.7484 - acc: 0.6679 - val_loss: 0.7457 - val_acc: 0.6710
Epoch 6/10
13934/13934 [==============================] - 9s - loss: 0.7346 - acc: 0.6723 - val_loss: 0.7490 - val_acc: 0.6780
Epoch 7/10
13934/13934 [==============================] - 9s - loss: 0.7217 - acc: 0.6804 - val_loss: 0.7298 - val_acc: 0.6795
Epoch 8/10
13934/13934 [==============================] - 9s - loss: 0.7162 - acc: 0.6826 - val_loss: 0.7248 - val_acc: 0.6792
Epoch 9/10
13934/13934 [==============================] - 9s - loss: 0.7082 - acc: 0.6892 - val_loss: 0.7202 - val_acc: 0.6890
Epoch 10/10
13934/13934 [==============================] - 9s - loss: 0.7001 - acc: 0.6940 - val_loss: 0.7226 - val_acc: 0.6885
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         (None, 32, 32, 3)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 28, 28, 10)        760       
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 14, 14, 10)        0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 1960)              0         
_________________________________________________________________
dense_6 (Dense)              (None, 3)                 5883      
=================================================================
Total params: 6,643
Trainable params: 6,643
Non-trainable params: 0
_________________________________________________________________
</code></pre><h1 id="optimize2-增加神经网络的层数"><a href="#optimize2-增加神经网络的层数" class="headerlink" title="optimize2 增加神经网络的层数"></a>optimize2 增加神经网络的层数</h1><p>我们在模型中多添加几层并且提高卷几层的输出维度，这次结果得到显著提升：0.750904</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#参数初始化</span></div><div class="line">filters1=<span class="number">50</span></div><div class="line">filters2=<span class="number">100</span></div><div class="line">filters3=<span class="number">100</span></div><div class="line"></div><div class="line">filtersize=(<span class="number">5</span>,<span class="number">5</span>)</div><div class="line"></div><div class="line">epochs =<span class="number">10</span></div><div class="line">batchsize=<span class="number">128</span></div><div class="line"></div><div class="line">input_shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line"></div><div class="line">model.add(keras.layers.InputLayer(input_shape=input_shape))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters1, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters2, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</div><div class="line"></div><div class="line">model.add(keras.layers.convolutional.Conv2D(filters3, filtersize, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="string">"channels_last"</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(keras.layers.Flatten())</div><div class="line"></div><div class="line">model.add(keras.layers.Dense(units=<span class="number">3</span>, input_dim=<span class="number">50</span>,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(train_x, train_y, epochs=epochs, batch_size=batchsize,validation_split=<span class="number">0.3</span>)</div><div class="line">model.summary()</div></pre></td></tr></table></figure>
<pre><code>Train on 13934 samples, validate on 5972 samples
Epoch 1/10
13934/13934 [==============================] - 44s - loss: 0.8613 - acc: 0.5985 - val_loss: 0.7778 - val_acc: 0.6586
Epoch 2/10
13934/13934 [==============================] - 44s - loss: 0.7493 - acc: 0.6697 - val_loss: 0.7545 - val_acc: 0.6808
Epoch 3/10
13934/13934 [==============================] - 43s - loss: 0.7079 - acc: 0.6877 - val_loss: 0.7150 - val_acc: 0.6947
Epoch 4/10
13934/13934 [==============================] - 43s - loss: 0.6694 - acc: 0.7061 - val_loss: 0.6496 - val_acc: 0.7261
Epoch 5/10
13934/13934 [==============================] - 43s - loss: 0.6274 - acc: 0.7295 - val_loss: 0.6683 - val_acc: 0.7125
Epoch 6/10
13934/13934 [==============================] - 43s - loss: 0.5950 - acc: 0.7462 - val_loss: 0.6194 - val_acc: 0.7400
Epoch 7/10
13934/13934 [==============================] - 43s - loss: 0.5562 - acc: 0.7655 - val_loss: 0.5981 - val_acc: 0.7465
Epoch 8/10
13934/13934 [==============================] - 43s - loss: 0.5165 - acc: 0.7852 - val_loss: 0.6458 - val_acc: 0.7354
Epoch 9/10
13934/13934 [==============================] - 46s - loss: 0.4826 - acc: 0.7986 - val_loss: 0.6206 - val_acc: 0.7467
Epoch 10/10
13934/13934 [==============================] - 45s - loss: 0.4530 - acc: 0.8130 - val_loss: 0.5984 - val_acc: 0.7569
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        (None, 32, 32, 3)         0         
_________________________________________________________________
conv2d_31 (Conv2D)           (None, 28, 28, 50)        3800      
_________________________________________________________________
max_pooling2d_23 (MaxPooling (None, 14, 14, 50)        0         
_________________________________________________________________
conv2d_32 (Conv2D)           (None, 10, 10, 100)       125100    
_________________________________________________________________
max_pooling2d_24 (MaxPooling (None, 5, 5, 100)         0         
_________________________________________________________________
conv2d_33 (Conv2D)           (None, 1, 1, 100)         250100    
_________________________________________________________________
flatten_15 (Flatten)         (None, 100)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 3)                 303       
=================================================================
Total params: 379,303
Trainable params: 379,303
Non-trainable params: 0
_________________________________________________________________
</code></pre><h1 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">pred=model.predict_classes(test_x)</div><div class="line">pred=lb.inverse_transform(pred)</div><div class="line">print(pred)</div><div class="line">test[<span class="string">'Class'</span>]=pred</div><div class="line">test.to_csv(<span class="string">'sub02.csv'</span>,index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<pre><code>6636/6636 [==============================] - 7s     
[&apos;MIDDLE&apos; &apos;YOUNG&apos; &apos;MIDDLE&apos; ..., &apos;MIDDLE&apos; &apos;MIDDLE&apos; &apos;YOUNG&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">i = random.choice(train.index)</div><div class="line">img_name = train.ID[i]</div><div class="line"></div><div class="line">img=Image.open(os.path.join(root_dir,<span class="string">'Train'</span>,img_name))</div><div class="line">img.show()</div><div class="line">pred = model.predict_classes(train_x)</div><div class="line">print(<span class="string">'Original:'</span>, train.Class[i], <span class="string">'Predicted:'</span>, lb.inverse_transform(pred[i]))</div></pre></td></tr></table></figure>
<pre><code>19872/19906 [============================&gt;.] - ETA: 0sOriginal: MIDDLE Predicted: MIDDLE
</code></pre><h1 id="继续探讨"><a href="#继续探讨" class="headerlink" title="继续探讨"></a>继续探讨</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;用Keras构建基本的前馈神经网络以及借助卷积层逐步优化预测结果，从海量图片中预测印度人们的年龄。目前结果为（0.750904）&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>我会自动改变</title>
    <link href="http://yoursite.com/2017/08/03/hello-world/"/>
    <id>http://yoursite.com/2017/08/03/hello-world/</id>
    <published>2017-08-03T09:53:05.101Z</published>
    <updated>2017-08-03T09:53:05.101Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>我会自动改变<br>Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.<br><a id="more"></a><br>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</excerpt></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;我会自动改变&lt;br&gt;Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://yoursite.com/2017/08/02/02%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>http://yoursite.com/2017/08/02/02动态规划/</id>
    <published>2017-08-02T16:00:25.000Z</published>
    <updated>2017-08-03T09:53:05.086Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>文中主要总结动态规划的01背包问题以及相关实例，然后通过c++解决问题<br><a id="more"></a></excerpt></p>
<h1 id="一、背包问题"><a href="#一、背包问题" class="headerlink" title="一、背包问题"></a>一、背包问题</h1><h2 id="01背包问题"><a href="#01背包问题" class="headerlink" title="01背包问题"></a>01背包问题</h2><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote>
<p>有N件物品和一个容积为<code>V</code>的背包。第<code>i</code>件物品的体积是<code>c[i]</code>，价值是<code>w[i]</code>。求解将哪些物品装入背包可使价值总和最大。</p>
</blockquote>
<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><blockquote>
<p>这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。<br>用子问题定义状态：即<code>f[i][v]</code>表示前<code>i</code>件物品恰放入一个容量为<code>v</code>的背包可以获得的最大价值。则其状态转移方程便是：</p>
<ul>
<li>二维方程：<code>f[i][v]=max(f[i-1][v],f[i-1][v-c[i]]+w[i])</code></li>
<li>一维方程：<code>f[v]=max(f[v],f[v-c[i]]+w[i])</code><br><strong>状态转移方程解释</strong>：“将前<code>i</code>件物品放入容量为v的背包中”这个子问题，若只考虑第<code>i</code>件物品放或不放，那么就可以转化为一个只牵扯前<code>i-1</code>件物品的问题。如果不放第i件物品，那么问题就转化为“前<code>i-1</code>件物品放入容量为v的背包中”，价值为<code>f[i-1][v]</code>；如果放第i件物品，那么问题就转化为“前i-1件物品放入剩下的容量为<code>v-c[i]</code>的背包中”，此时能获得的最大价值就是<code>f[i-1][v-c[i]]</code>再加上通过放入第i件物品获得的价值<code>w[i]</code>。</li>
</ul>
</blockquote>
<h2 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h2><p><img src="https://raw.githubusercontent.com/arkulo56/thought/master/images/algorithm/beibao.png" alt="图解"><br><a href="http://www.jianshu.com/p/48f2dd394608" target="_blank" rel="external">参考</a></p>
<blockquote>
<p>有了这张图和上面总结的公式，我们就可以很清晰的理解01背包算法了</p>
<ol>
<li>e2单元格：当只有一件物品e，包的容量是2时，装不进去，所以最大值为0</li>
<li>a8单元格：物品包括a、b、c、d、e，容量为8时，F[i-1,j]=F[b,8]=9，F[i-1,j-Wi]+Pi=F[b,6]+6=9+6=15，两种情况取最大值，因此这里的最大值是15</li>
</ol>
</blockquote>
<p>##实例1 采药(RQNOJ15)；</p>
<h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>辰辰是个天资聪颖的孩子，他的梦想是成为世界上最伟大的医师。为此，他想拜附近最有威望的医师为师。医师为了判断他的资质，给他出了一个难题。医师把他带到一个到处都是草药的山洞里对他说：“孩子，这个山洞里有一些不同的草药，采每一株都需要一些时间，每一株也有它自身的价值。我会给你一段时间，在这段时间里，你可以采到一些草药。如果你是一个聪明的孩子，你应该可以让采到的草药的总价值最大。”<br>　　如果你是辰辰，你能完成这个任务吗？</p>
<h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><p>　输入的第一行有两个整数T（1 &lt;= T &lt;= 1000）和M（1 &lt;= M &lt;= 100），用一个空格隔开，T代表总共能够用来采药的时间，M代表山洞里的草药的数目。接下来的M行每行包括两个在1到100之间（包括1和100）的整数，分别表示采摘某株草药的时间和这株草药的价值。</p>
<h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><p>输出包括一行，这一行只包含一个整数，表示在规定的时间内，可以采到的草药的最大总价值。</p>
<h3 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h3><p>70 3<br>71 100<br>69 1<br>1 2</p>
<h3 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h3><p>3</p>
<h3 id="参考程序1"><a href="#参考程序1" class="headerlink" title="参考程序1"></a>参考程序1</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> ti[<span class="number">101</span>],money[<span class="number">101</span>];</div><div class="line"><span class="keyword">int</span> f[<span class="number">1001</span>];</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> t,m,i,j;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;t&gt;&gt;m;</div><div class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=m;i++)&#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;ti[i]&gt;&gt;money[i];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=m;i++)&#123;</div><div class="line">        <span class="keyword">for</span>(j=t;j&gt;=ti[i];j--)&#123;</div><div class="line">            <span class="keyword">if</span>(f[j-ti[i]]+money[i]&gt;f[j])&#123;<span class="comment">//把最大值赋值给f[t]</span></div><div class="line">                f[j]=f[j-ti[i]]+money[i];</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">cout</span>&lt;&lt;f[t];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="参考程序2"><a href="#参考程序2" class="headerlink" title="参考程序2"></a>参考程序2</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">define</span>  V 1500</span></div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> f[<span class="number">10</span>][V];<span class="comment">//全局变量，自动初始化为0</span></div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> weight[<span class="number">10</span>];</div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> value[<span class="number">10</span>];</div><div class="line"><span class="meta">#<span class="meta-keyword">define</span>  max(x,y)   (x)&gt;(y)?(x):(y)</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">int</span> N,M;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;M;<span class="comment">//背包容量</span></div><div class="line">    <span class="built_in">cin</span>&gt;&gt;N;<span class="comment">//药品个数</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=N; i++)</div><div class="line">    &#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;weight[i]&gt;&gt;value[i];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">        &#123;</div><div class="line">            <span class="keyword">if</span> (weight[i]&lt;=j)</div><div class="line">            &#123;</div><div class="line">                f[i][j]=max(f[i<span class="number">-1</span>][j],f[i<span class="number">-1</span>][j-weight[i]]+value[i]);</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span></div><div class="line">                f[i][j]=f[i<span class="number">-1</span>][j];</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="built_in">cout</span>&lt;&lt;f[N][M]&lt;&lt;<span class="built_in">endl</span>;<span class="comment">//输出最优解</span></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;文中主要总结动态规划的01背包问题以及相关实例，然后通过c++解决问题&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>排序算法</title>
    <link href="http://yoursite.com/2017/08/01/01%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2017/08/01/01排序问题/</id>
    <published>2017-08-01T16:00:25.000Z</published>
    <updated>2017-08-03T09:53:05.086Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>这篇文章主要介绍了啊哈算法的排序问题，包括桶排序、快速排序、冒泡排序<br><a id="more"></a></excerpt></p>
<h1 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p><code>这个算法好比有11个桶，编号从0~10。每出现一个数，就在对应编号的桶里放一个小旗子。最后只要数数每个桶中有几个小旗子就可以了。例如2号桶中有2个旗子，表示数字2出现了2次。</code></p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p><code>班上有5个同学，输入5个同学的分数（满分是10分），按从大到小输出5个同学的分数</code></p>
<h2 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h2><p><code>5 3 5 2 8</code></p>
<h2 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h2><p><code>8 5 5 3 2</code></p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> a[<span class="number">11</span>]=&#123;<span class="number">0</span>&#125;,t;<span class="comment">//将数组元素初始化为0</span></div><div class="line">    <span class="comment">//int a[11]=&#123;1&#125;;第一个元素为1，其他元素为0</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;<span class="comment">//循环输入5个数</span></div><div class="line">        <span class="built_in">cin</span>&gt;&gt;t;</div><div class="line">        a[t]++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)&#123;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;a[i];j++)&#123;</div><div class="line">            <span class="built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="string">"  "</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    getchar();getchar();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><h2 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h2><blockquote>
<p>通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p>
</blockquote>
<h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> n,a[<span class="number">101</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">quicksort</span><span class="params">(<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> temp,i ,j ,t;<span class="comment">//temp存基数</span></div><div class="line">    <span class="keyword">if</span>(left&gt;right)&#123;</div><div class="line">        <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line">    temp=a[left];</div><div class="line">    i=left;</div><div class="line">    j=right;</div><div class="line">    <span class="keyword">while</span>(i!=j)&#123;</div><div class="line">        <span class="comment">//顺序很重要，要先从右往左查找</span></div><div class="line">        <span class="keyword">while</span>(a[j]&gt;=temp &amp;&amp; i&lt;j)&#123;</div><div class="line">            j--;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//然后从左往右找</span></div><div class="line">        <span class="keyword">while</span>(a[i]&lt;=temp&amp;&amp;i&lt;j)&#123;</div><div class="line">            i++;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//交换两个数的位置</span></div><div class="line">        <span class="keyword">if</span>(i&lt;j)&#123;<span class="comment">//当哨兵没有相遇时</span></div><div class="line">            t=a[i];</div><div class="line">            a[i]=a[j];</div><div class="line">            a[j]=t;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//最终基数归位</span></div><div class="line">    a[left]=a[i];</div><div class="line">    a[i]=temp;</div><div class="line">    quicksort(left,i<span class="number">-1</span>);</div><div class="line">    quicksort(i+<span class="number">1</span>,right);</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="built_in">cin</span>&gt;&gt;n;</div><div class="line">    <span class="comment">//输入数据</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">        <span class="built_in">cin</span>&gt;&gt;a[i];</div><div class="line">    &#125;</div><div class="line">    quicksort(<span class="number">0</span>,n<span class="number">-1</span>);<span class="comment">//快速排序</span></div><div class="line">    <span class="comment">//输出数据</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;a[i]&lt;&lt;<span class="string">" "</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;这篇文章主要介绍了啊哈算法的排序问题，包括桶排序、快速排序、冒泡排序&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>HDU1009-FatMouse&#39; Trade</title>
    <link href="http://yoursite.com/2017/07/29/HDU1009-FatMouse&#39;%20Trade/"/>
    <id>http://yoursite.com/2017/07/29/HDU1009-FatMouse&#39; Trade/</id>
    <published>2017-07-29T14:54:03.000Z</published>
    <updated>2017-08-03T09:53:05.086Z</updated>
    
    <content type="html"><![CDATA[<excerpt in="" index="" |="" 首页摘要=""> 

<p>HDU 1009 FatMouse’ Trade<br><a id="more"></a></p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>FatMouse prepared M pounds of cat food, ready to trade with the cats guarding the warehouse containing his favorite food, JavaBean.</p>
<p>The warehouse has N rooms. The i-th room contains J[i] pounds of JavaBeans and requires F[i] pounds of cat food. FatMouse does not have to trade for all the JavaBeans in the room, instead, he may get J[i]<em> a% pounds of JavaBeans if he pays F[i]</em> a% pounds of cat food. Here a is a real number. Now he is assigning this homework to you: tell him the maximum amount of JavaBeans he can obtain.</p>
<h1 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h1><p>The input consists of multiple test cases. Each test case begins with a line containing two non-negative integers M and N. Then N lines follow, each contains two non-negative integers J[i] and F[i] respectively. The last test case is followed by two -1’s. All integers are not greater than 1000.</p>
<h1 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h1><p>For each test case, print in a single line a real number accurate up to 3 decimal places, which is the maximum amount of JavaBeans that FatMouse can obtain.</p>
<h1 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">5 3</div><div class="line">7 2</div><div class="line">4 3</div><div class="line">5 2</div><div class="line">20 3</div><div class="line">25 18</div><div class="line">24 15</div><div class="line">15 10</div><div class="line">-1 -1</div></pre></td></tr></table></figure>
<h1 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">13.333</div><div class="line">31.500</div></pre></td></tr></table></figure>
<h1 id="解题报告"><a href="#解题报告" class="headerlink" title="解题报告"></a>解题报告</h1><p>大意：一只老鼠有M磅的猫粮，另外有一只猫控制了老鼠的N个房间，这些房间里面放了老鼠爱吃的绿豆，给出每个房间的绿豆数量，和这个房间的绿豆所需要的猫粮数，现在要求老鼠用这M磅的猫粮最多能换到多少它爱吃的绿豆？</p>
<p>贪心题，由于所有的绿豆都是一样的，所以如果老鼠想要换到最多的绿豆，便可以换猫控制的房间里面最便宜的绿豆，也就是说先换取单位数量的绿豆所需要最少的猫粮的房间里的绿豆，这样就可以保证换到的绿豆是最多的。具体实现可以用一个结构体，里面保存每个房间里面有的绿豆的数量和换取这个房间的绿豆时所需要的猫粮的数量和换取这个房间的 单位重量的绿豆所需要的猫粮数（以下简称单价），然后再按照单价升序给这些结构体排一次序，这时就可以从最便宜的绿豆开始换了。</p>
<h1 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h1><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iomanip&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">house</span>&#123;</span></div><div class="line">    <span class="keyword">int</span> bean_num;<span class="comment">//每个房间含有的豆子数量</span></div><div class="line">    <span class="keyword">int</span> cost;<span class="comment">//获取bean_num个豆子，所需要的猫粮数</span></div><div class="line">    <span class="keyword">double</span> rate;<span class="comment">//性价比</span></div><div class="line">&#125;h[<span class="number">1005</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(house a,house b)</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(a.rate!=b.rate)</div><div class="line">    <span class="keyword">return</span> a.rate&gt;b.rate;</div><div class="line">    <span class="keyword">else</span></div><div class="line">    <span class="keyword">return</span> a.bean_num&lt;b.bean_num;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> m,n,i;</div><div class="line">    <span class="keyword">double</span> gains;</div><div class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;m&gt;&gt;n&amp;&amp;m!=<span class="number">-1</span>&amp;&amp;n!=<span class="number">-1</span>)&#123;</div><div class="line">        gains=<span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="built_in">cin</span>&gt;&gt;h[i].bean_num&gt;&gt;h[i].cost;</div><div class="line">            h[i].rate=h[i].bean_num*<span class="number">1.0</span>/h[i].cost;</div><div class="line">        &#125;</div><div class="line">        sort(h,h+n,cmp);</div><div class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</div><div class="line">            <span class="keyword">if</span>(m&gt;h[i].cost)&#123;</div><div class="line">                m-=h[i].cost;</div><div class="line">                gains+=h[i].bean_num;</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                gains+=h[i].rate*m;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;setiosflags(ios::fixed)&lt;&lt;setprecision(<span class="number">3</span>)&lt;&lt;gains&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></excerpt>]]></content>
    
    <summary type="html">
    
      &lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt; 

&lt;p&gt;HDU 1009 FatMouse’ Trade&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习算法的基本知识（使用Python和R代码）</title>
    <link href="http://yoursite.com/2017/07/26/Python28-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BD%BF%E7%94%A8Python%E5%92%8CR%E4%BB%A3%E7%A0%81%EF%BC%89/"/>
    <id>http://yoursite.com/2017/07/26/Python28-机器学习算法的基本知识（使用Python和R代码）/</id>
    <published>2017-07-26T15:51:25.000Z</published>
    <updated>2017-08-03T09:53:05.101Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是<a href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/" target="_blank" rel="external">原文</a>的译文，然后自己对其中做了一些修改和添加内容（随机森林和降维算法）。文章简洁地介绍了机器学习的主要算法和一些伪代码，对于初学者有很大帮助，是一篇不错的总结文章，后期可以通过文中提到的算法展开去做一些实际问题。<br><a id="more"></a></excerpt></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><hr>
<p><code>Google的自驾车和机器人得到了很多新闻，但公司的真正未来是机器学习，这种技术使计算机变得更智能，更个性化。</code><em>-Eric Schmidt (Google Chairman)</em></p>
<hr>
<p>我们可能生活在人类历史上最具影响力的时期——计算从大型主机到PC移动到云计算的时期。 但是使这段时期有意义的不是发生了什么，而是在未来几年里我们的方式。</p>
<p>这个时期令像我这样的一个人兴奋的就是，随着计算机的推动，工具和技术的民主化。 今天，作为数据科学家，我可以每小时为几个玩偶构建具有复杂算法的数据处理机。 但到达这里并不容易，我已经度过了许多黑暗的日日夜夜。</p>
<h1 id="谁可以从本指南中获益最多"><a href="#谁可以从本指南中获益最多" class="headerlink" title="谁可以从本指南中获益最多"></a>谁可以从本指南中获益最多</h1><p><strong>我今天发布的可能是我创造的最有价值的指南。</strong></p>
<p>创建本指南背后的理念是简化全球有抱负的数据科学家和机器学习爱好者的旅程。 本指南能够使你在研究机器学习问题的过程中获取经验。 我提供了关于各种机器学习算法以及R＆Python代码的高级理解以及运行它们，这些应该足以使你得心顺手。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Newl-Machine-Learning-Algorithms.jpg" alt="machine learning"><br>我故意跳过了这些技术背后的统计数据，因为你不需要在开始时就了解它们。 所以，如果你正在寻找对这些算法的统计学理解，你应该看看别的文章。 但是，如果你正在寻找并开始构建机器学习项目，那么这篇文章给你带来极大好处。</p>
<h1 id="3类机器学习算法（广义上）"><a href="#3类机器学习算法（广义上）" class="headerlink" title="3类机器学习算法（广义上）"></a>3类机器学习算法（广义上）</h1><ol>
<li><p>监督学习<br>工作原理：该算法由一组目标/结果变量（或因变量）组成，该变量将根据给定的一组预测变量（独立变量）进行预测。 使用这些变量集，我们生成一个将输入映射到所需输出的函数。 训练过程继续进行执行，直到模型达到培训数据所需的准确度水平。 监督学习的例子：回归，决策树，随机森林，KNN，逻辑回归等</p>
</li>
<li><p>无监督学习<br>如何工作：在这个算法中，我们没有任何目标或结果变量来预测/估计。 用于不同群体的群体聚类和用于不同群体的客户进行特定干预。 无监督学习的例子：Apriori算法，K-means。</p>
</li>
<li><p>加强学习：<br>工作原理：使用这种算法，机器受到学习和训练，作出具体决定。 它以这种方式工作：机器暴露在一个环境中，它连续不断地使用试错。 该机器从过去的经验中学习，并尝试捕获最好的知识，以做出准确的业务决策。 加强学习示例：马尔可夫决策过程</p>
</li>
</ol>
<h1 id="常见机器学习算法"><a href="#常见机器学习算法" class="headerlink" title="常见机器学习算法"></a>常见机器学习算法</h1><p>以下是常用机器学习算法的列表。 这些算法几乎可以应用于任何数据问题：</p>
<ul>
<li>线性回归</li>
<li>逻辑回归</li>
<li>决策树</li>
<li>SVM</li>
<li>朴素贝叶斯</li>
<li>KNN</li>
<li>K-Means</li>
<li>随机森林</li>
<li>降维算法</li>
<li>Gradient Boost＆Adaboost</li>
</ul>
<h1 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h1><p>它用于基于连续变量来估计实际价值（房屋成本，电话数量，总销售额等）。在这里，我们通过拟合最佳线来建立独立变量和因变量之间的关系。这个最佳拟合线被称为回归线，由线性方程<code>Y = a * X + b</code>表示。</p>
<p>理解线性回归的最好方法是回想童年的经历。比如，你要求五年级的孩子通过体重来从小到大排序班里的学生，而事先不告诉学生们的体重！你认为孩子会做什么？他/她很可能在身高和体格上分析人物的体重，并使用这些可视参数的组合进行排列。这是现实生活中的线性回归！孩子实际上已经弄清楚，身高和体格将有一个关系与体重相关联，看起来就像上面的等式。</p>
<p>在这个方程式中：</p>
<p><code>Y-因变量</code><br><code>a - 斜率</code><br><code>X - 自变量</code><br><code>b - 截距</code><br>这些系数a和b是基于最小化数据点和回归线之间的距离的平方差之和导出的。</p>
<p>看下面的例子。这里我们确定了线性方程<code>y = 0.2811x + 13.9</code>的最佳拟合线。现在使用这个方程，我们可以找到一个人（身高已知）的体重。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Linear_Regression.png" alt="线性回归"><br>线性回归主要有两种类型：简单线性回归和多元线性回归。 简单线性回归的特征在于一个自变量。 而且，多元线性回归（顾名思义）的特征是多个（多于1个）自变量。 在找到最佳拟合线的同时，可以拟合多项式或曲线回归线，这些被称为多项式或曲线回归。</p>
<h2 id="Python-Code"><a href="#Python-Code" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line"><span class="comment"># x_train=input_variables_values_training_datasets</span></div><div class="line">x_train=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(x_train)</div><div class="line"><span class="comment"># y_train=target_variables_values_training_datasets</span></div><div class="line">y_train=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(y_train)</div><div class="line"></div><div class="line"><span class="comment"># x_test=input_variables_values_test_datasets</span></div><div class="line">x_test=np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</div><div class="line">print(x_test)</div><div class="line"></div><div class="line"><span class="comment"># Create linear regression object</span></div><div class="line">linear = linear_model.LinearRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear.fit(x_train, y_train)</div><div class="line">linear.score(x_train, y_train)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, linear.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, linear.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= linear.predict(x_test)</div><div class="line">print(<span class="string">'predicted:\n'</span>,predicted)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.98267731  0.23364069  0.35133775  0.92826309]
 [ 0.80538991  0.05637806  0.87662175  0.3960776 ]
 [ 0.54686738  0.6816495   0.99747716  0.32531085]
 [ 0.19189509  0.87105462  0.88158122  0.25056621]]
[[ 0.55541608  0.56859636  0.40616234  0.14683524]
 [ 0.09937835  0.63874553  0.92062536  0.32798326]
 [ 0.87174236  0.779044    0.79119392  0.06912842]
 [ 0.87907434  0.53175367  0.01371655  0.11414196]]
[[ 0.37568516  0.17267374  0.51647046  0.04774661]
 [ 0.38573914  0.85335136  0.11647555  0.0758696 ]
 [ 0.67559384  0.57535368  0.88579261  0.26278658]
 [ 0.13829782  0.28328756  0.51170484  0.04260013]]
Coefficient: 
 [[ 0.55158868  1.45901817  0.31224322  0.49538173]
 [ 0.6995448   0.40804135  0.59938423  0.09084578]
 [ 1.79010371  0.21674532  1.60972012 -0.046387  ]
 [-0.31562917 -0.53767439 -0.16141312 -0.2154683 ]]
Intercept: 
 [-0.89705102 -0.50908061 -1.9260686   0.83934127]
predicted:
 [[-0.25297601  0.13808785 -0.38696891  0.53426883]
 [ 0.63472658  0.18566989 -0.86662193  0.22361739]
 [ 0.72181277  0.75309881  0.82170796  0.11715048]
 [-0.22656611  0.01383581 -0.79537442  0.55159912]]
</code></pre><h2 id="R-Code"><a href="#R-Code" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train &lt;- input_variables_values_training_datasets</div><div class="line">y_train &lt;- target_variables_values_training_datasets</div><div class="line">x_test &lt;- input_variables_values_test_datasets</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear &lt;- lm(y_train ~ ., data = x)</div><div class="line">summary(linear)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(linear,x_test)</div></pre></td></tr></table></figure>
<h1 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2.逻辑回归"></a>2.逻辑回归</h1><p>不要因为它的名字而感到困惑，逻辑回归是一个分类算法而不是回归算法。它用于基于给定的一组自变量来估计离散值（二进制值，如0/1，是/否，真/假）。简单来说，它通过将数据拟合到logit函数来预测事件发生的概率。因此，它也被称为logit回归。由于它预测概率，其输出值在0和1之间（如预期的那样）。</p>
<p>再次，让我们通过一个简单的例子来尝试理解这一点。</p>
<p>假设你的朋友给你一个难题解决。只有2个结果场景 - 你能解决和不能解决。现在想象，你正在被许多猜谜或者简单测验，来试图理解你擅长的科目。这项研究的结果将是这样的结果 - 如果给你一个10级的三角形问题，那么你有70％可能会解决这个问题。另外一个例子，如果是五级的历史问题，得到答案的概率只有30％。这就是逻辑回归为你提供的结果。</p>
<p>对数学而言，结果的对数几率被建模为预测变量的线性组合。</p>
<p><code>odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk</code></p>
<p>以上，p是感兴趣特征的概率。 它选择最大化观察样本值的可能性的参数，而不是最小化平方误差的总和（如在普通回归中）。</p>
<p>现在，你可能会问，为什么要采用log？ 为了简单起见，让我们来说，这是复制阶梯函数的最好的数学方法之一。 我可以进一步详细介绍，但这将会打破这篇文章的目的。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Logistic_Regression.png" alt="逻辑回归"></p>
<h2 id="Python-Code-1"><a href="#Python-Code-1" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, model.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, model.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-1"><a href="#R-Code-1" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</div><div class="line">summary(logistic)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(logistic,x_test)</div></pre></td></tr></table></figure>
<h1 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3.决策树"></a>3.决策树</h1><p>这是我最喜欢的算法之一，我经常使用它。 它是一种主要用于分类问题的监督学习算法，令人惊讶的是，它可以适用于分类和连·续因变量。 在该算法中，我们将群体分为两个或多个均匀集合。 这是基于最重要的属性/自变量来做出的并将它们分为不同的组。关于决策树的更多细节，你可以阅读<a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="external">决策树简介</a></p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png" alt="决策树"><br>在上图中，您可以看到根据多个属性将群体分为四个不同的群组，以确定用户“是否可以玩”。为了 将人口分为不同的特征群体，它使用了诸如Gini，信息增益，卡方，熵等各种技术。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg" alt="JezzBall"><br>了解决策树如何运作的最佳方法是播放Jezzball - 微软的经典游戏（下图）。 大体上就是，来一起在屏幕上滑动手指，筑起墙壁，掩住移动的球吧。</p>
<h2 id="Python-Code-2"><a href="#Python-Code-2" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create tree object </span></div><div class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>) </div><div class="line"><span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></div><div class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-2"><a href="#R-Code-2" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(rpart)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># grow tree </span></div><div class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>
<h1 id="4-SVM-支持向量机"><a href="#4-SVM-支持向量机" class="headerlink" title="4.SVM(支持向量机)"></a>4.SVM(支持向量机)</h1><p>这是一种分类方法。 在这个算法中，我们将每个数据项目绘制为n维空间中的一个点（其中n是拥有的特征数），每个特征的值是特定坐标的值。</p>
<p>例如，如果我们有一个人的“高度”和“头发长度”这两个特征，我们首先将这两个变量绘制在二维空间中，其中每个点都有两个坐标（这些坐标称为支持向量）<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM1-850x575.png" alt="支持向量机"><br>现在，我们将找到一些可以将数据分割成两类的线。 而我们想要的线，就是使得两组数据中最近点到分割线的距离最长的线。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2-850x578.png" alt="最佳分割直线"><br>在上述示例中，将数据分成两个不同分类的组的线是黑线，因为两个最接近的点距离线最远（红线也可以，但不是一最远）。 这条线是我们的分类器， 然后根据测试数据位于线路两边的位置，我们可以将新数据分类为什么类别。</p>
<h2 id="Python-Code-3"><a href="#Python-Code-3" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object </span></div><div class="line">model = svm.svc() <span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-3"><a href="#R-Code-3" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-svm(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>
<h1 id="5-朴素贝叶斯"><a href="#5-朴素贝叶斯" class="headerlink" title="5. 朴素贝叶斯"></a>5. 朴素贝叶斯</h1><p>它是基于贝叶斯定理的分类技术，假设预测因子之间是独立的。 简单来说，朴素贝叶斯分类器假设类中特定特征的存在与任何其他特征的存在无关。 例如，如果果实是红色，圆形，直径约3英寸，则果实可能被认为是苹果。 即使这些特征依赖于彼此或其他特征的存在，一个朴素的贝叶斯分类器将考虑的是所有属性来单独地贡献这个果实是苹果的概率。</p>
<p>朴素贝叶斯模型易于构建，对于非常大的数据集尤其有用。 除了简单之外，朴素贝叶斯也被称为超高级分类方法。</p>
<p>贝叶斯定理提供了一种由P（c），P（x）和P（x | c）计算概率P（c | x）的方法。 看下面的等式：<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule.png" alt="朴素贝叶斯"></p>
<ul>
<li>其中：<ul>
<li>P（c | x）是在x条件下c发生的概率。</li>
<li>P（c）是c发生的概率。</li>
<li>P（x | c）在c条件下x发生的概率。</li>
<li>P（x）是x发生的概率。</li>
</ul>
</li>
</ul>
<h2 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h2><p>让我们用一个例子来理解它。 下面我有一个天气和相应的目标变量“玩游戏”的训练数据集。 现在，我们需要根据天气条件对玩家是否玩游戏进行分类。 我们按照以下步骤执行。</p>
<p>步骤1：将数据集转换为频率表</p>
<p>步骤2：通过发现像“Overcast”概率= 0.29和播放概率为0.64的概率来创建似然表。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41-850x310.png" alt="例子"><br>步骤3：现在，使用朴素贝叶斯方程来计算每个类的概率。 其中概率最高的情况就是是预测的结果。</p>
<h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>如果天气晴朗，玩家会玩游戏，这个说法是正确的吗？</p>
<p>我们可以使用上述方法解决，所以P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p>
<p>这里，P（Sunny | Yes）= 3/9 = 0.33，P（Sunny）= 5/14 = 0.36，P（Yes）= 9/14 = 0.64</p>
<p>现在，P（Yes | Sunny）= 0.33 * 0.64 / 0.36 = 0.60，该事件发生的概率还是比较高的。</p>
<p>朴素贝叶斯使用类似的方法根据各种属性预测不同分类的概率，该算法主要用于文本分类和具有多个类的问题。</p>
<h2 id="Python-Code-4"><a href="#Python-Code-4" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object model = GaussianNB() </span></div><div class="line"><span class="comment"># there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-4"><a href="#R-Code-4" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>
<h1 id="6-KNN-K-近邻算法"><a href="#6-KNN-K-近邻算法" class="headerlink" title="6. KNN (K-近邻算法)"></a>6. KNN (K-近邻算法)</h1><p>它可以用于分类和回归问题, 然而，它在行业中被广泛地应用于分类问题。 K-近邻算法用于存储所有训练样本集（所有已知的案列），并通过其k个邻近数据多数投票对新的数据（或者案列）进行分类。通常，选择k个最近邻数据中出现次数最多的分类作为新数据的分类。</p>
<p>这些计算机的距离函数可以是欧几里德，曼哈顿，闵可夫斯基和汉明距离。 前三个函数用于连续函数，第四个函数用于分类变量。 如果K = 1，则简单地将该情况分配给其最近邻的类。 有时，选择K在执行KNN建模时是一个难点。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png" alt="K-近邻算法"><br>KNN可以轻松映射到我们的现实生活中。 如果你想了解一个人，你没有任何信息，你可能想知道先去了解他的亲密的朋友和他活动的圈子，从而获得他/她的信息！</p>
<p>选择KNN之前要考虑的事项：</p>
<ul>
<li>KNN在计算上是昂贵的</li>
<li>变量应该被归一化，否则更高的范围变量可以偏移它</li>
<li>在进行KNN之前，预处理阶段的工作更像去除离群值、噪声值</li>
</ul>
<h2 id="Python-Code-5"><a href="#Python-Code-5" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">KNeighborsClassifier(n_neighbors=<span class="number">6</span>) <span class="comment"># default value for n_neighbors is 5</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-5"><a href="#R-Code-5" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">library(knn)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-knn(y_train ~ ., data = x,k=<span class="number">5</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>
<h1 id="7-K-Means"><a href="#7-K-Means" class="headerlink" title="7. K-Means"></a>7. K-Means</h1><p>它是解决聚类问题的一种无监督算法。 其过程遵循一种简单而简单的方式，通过一定数量的聚类（假设k个聚类）对给定的数据集进行分类。 集群内的数据点与对等组是同构的和异构的。</p>
<p>尝试从油墨印迹中找出形状？（见下图） k means 与这个活动相似， 你通过墨水渍形状来判断有多少群体存在！<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph-284x300.jpg" alt="K-Means"><br>下面两点感觉原文解释的不是很清楚，自己然后查了下国内的解释方法</p>
<h2 id="K-means如何形成集群"><a href="#K-means如何形成集群" class="headerlink" title="K-means如何形成集群"></a>K-means如何形成集群</h2><ul>
<li>（1） 从 n个数据对象任意选择 k 个对象作为初始聚类中心；</li>
<li>（2） 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</li>
<li>（3） 重新计算每个（有变化）聚类的均值（中心对象）</li>
<li>（4） 循环（2）到（3）直到每个聚类不再发生变化为止<a href="https://baike.baidu.com/item/K-means/4934806?fr=aladdin" target="_blank" rel="external">参考</a></li>
</ul>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="http://cms.csdnimg.cn/articlev1/uploads/allimg/120703/091301K62-1.jpg" alt="K-Means例子"><br>从上图中，我们可以看到，<code>A，B，C，D，E</code>是五个在图中点。而灰色的点是我们的种子点，也就是我们用来找点群的点。有两个种子点，所以<code>K=2</code>。</p>
<p>然后，<code>K-Means</code>的算法如下：</p>
<ol>
<li>随机在图中取K（这里K=2）个种子点。</li>
<li>然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群。（上图中，我们可以看到A，B属于上面的种子点，C，D，E属于下面中部的种子点）</li>
<li>接下来，我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）</li>
<li>然后重复第2）和第3）步，直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A，B，C，下面的种子点聚合了D，E）。<a href="http://www.csdn.net/article/2012-07-03/2807073-k-means" target="_blank" rel="external">参考</a></li>
</ol>
<h2 id="K值如何确定"><a href="#K值如何确定" class="headerlink" title="K值如何确定"></a>K值如何确定</h2><p>在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。<a href="http://www.cnblogs.com/dudumiaomiao/p/5839905.html" target="_blank" rel="external">参考</a></p>
<p>我们知道随着群集数量的增加，该值不断减少，但是如果绘制结果，则可能会发现平方距离的总和急剧下降到k的某个值，然后再慢一些。 在这里，我们可以找到最佳聚类数。<br><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Kmenas-850x429.png" alt="k值"></p>
<h2 id="Python-Code-6"><a href="#Python-Code-6" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-6"><a href="#R-Code-6" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">library(cluster)</div><div class="line">fit &lt;- kmeans(X, <span class="number">3</span>) <span class="comment"># 5 cluster solution</span></div></pre></td></tr></table></figure>
<h1 id="8-Random-Forest（随机树林）"><a href="#8-Random-Forest（随机树林）" class="headerlink" title="8. Random Forest（随机树林）"></a>8. Random Forest（随机树林）</h1><p>随机森林(Random Forest)是一个包含多个决策树的分类器， 其输出的类别由个别树输出类别的众数而定。（相当于许多不同领域的专家对数据进行分类判断，然后投票）<br><img src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=e0a6ac59104c510fbac9ea4801304e48/960a304e251f95cab62ae027c3177f3e66095247.jpg" alt="随机树林"><br>感觉原文没有将什么实质内容，给大家推进这一篇<a href="https://www.zybuluo.com/hshustc/note/179319" target="_blank" rel="external">Random Forest入门</a></p>
<h1 id="9-降维算法"><a href="#9-降维算法" class="headerlink" title="9. 降维算法"></a>9. 降维算法</h1><p>在过去的4-5年中，数据挖掘在每个可能的阶段都呈指数级增长。 公司/政府机构/研究机构不仅有新的来源，而且他们正在非常详细地挖掘数据。</p>
<p>例如：电子商务公司正在捕获更多关于客户的细节，例如人口统计，网络爬网历史，他们喜欢或不喜欢的内容，购买历史记录，反馈信息等等，给予他们个性化的关注，而不是离你最近的杂货店主。</p>
<p>作为数据科学家，我们提供的数据还包括许多功能，这对建立良好的稳健模型是非常有用的，但是有一个挑战。 你如何识别出1000或2000年高度重要的变量？ 在这种情况下，维数降低算法可以帮助我们与决策树，随机森林，PCA，因子分析，基于相关矩阵，缺失值比等的其他算法一起使用。<br>要了解更多有关此算法的信息，您可以阅读<a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/" target="_blank" rel="external"> “Beginners Guide To Learn Dimension Reduction Techniques“.</a></p>
<h2 id="Python-Code-7"><a href="#Python-Code-7" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</div><div class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></div><div class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></div><div class="line"><span class="comment"># For Factor analysis</span></div><div class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></div><div class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></div><div class="line">train_reduced = pca.fit_transform(train)</div><div class="line"><span class="comment">#Reduced the dimension of test dataset</span></div><div class="line">test_reduced = pca.transform(test)</div></pre></td></tr></table></figure>
<p>For more detail on this, please refer  <a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions" target="_blank" rel="external">this link</a>.</p>
<h2 id="R-Code-7"><a href="#R-Code-7" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">library(stats)</div><div class="line">pca &lt;- princomp(train, cor = TRUE)</div><div class="line">train_reduced  &lt;- predict(pca,train)</div><div class="line">test_reduced  &lt;- predict(pca,test)</div></pre></td></tr></table></figure>
<h1 id="10-Gradient-Boosting-amp-AdaBoost"><a href="#10-Gradient-Boosting-amp-AdaBoost" class="headerlink" title="10. Gradient Boosting &amp; AdaBoost"></a>10. Gradient Boosting &amp; AdaBoost</h1><p>当我们处理大量数据以预测高预测能力时，GBM＆AdaBoost是更加强大的算法。 Boosting是一种综合学习算法，它结合了几个基本估计器的预测，以提高单个估计器的鲁棒性。 它将多个弱或平均预测值组合到一个强大的预测变量上。 这些提升算法在数据科学比赛中总是能够很好地运行，如Kaggle，AV Hackathon，CrowdAnalytix。<br>More: <a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="external">Know about Gradient and AdaBoost in detail</a></p>
<h2 id="Python-Code-8"><a href="#Python-Code-8" class="headerlink" title="Python Code"></a>Python Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></div><div class="line">model= GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<h2 id="R-Code-8"><a href="#R-Code-8" class="headerlink" title="R Code"></a>R Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">library(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = <span class="number">4</span>, repeats = <span class="number">4</span>)</div><div class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = FALSE)</div><div class="line">predicted= predict(fit,x_test,type= <span class="string">"prob"</span>)[,<span class="number">2</span>]</div></pre></td></tr></table></figure>
<h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>现在我相信，你会有一个常用的机器学习算法的想法。 我在写这篇文章和提供R和Python中的代码的唯一意图就是让你马上开始。 如果您想要掌握机器学习，请将算法运用实际问题，体会其中的乐趣</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文&lt;/a&gt;的译文，然后自己对其中做了一些修改和添加内容（随机森林和降维算法）。文章简洁地介绍了机器学习的主要算法和一些伪代码，对于初学者有很大帮助，是一篇不错的总结文章，后期可以通过文中提到的算法展开去做一些实际问题。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Kera实例：预测白酒和红酒的质量</title>
    <link href="http://yoursite.com/2017/07/26/Keras04-%E7%99%BD%E9%85%92%E5%92%8C%E7%BA%A2%E9%85%92%E5%AE%9E%E4%BE%8B/"/>
    <id>http://yoursite.com/2017/07/26/Keras04-白酒和红酒实例/</id>
    <published>2017-07-26T15:51:25.000Z</published>
    <updated>2017-08-03T09:53:05.101Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是益智的教程，参考之后动手进行实践了一遍。编译环境windows10+python3.5<br><a id="more"></a></excerpt></p>
<p><a href="https://jizhi.im/course/dl_keras/2" target="_blank" rel="external">参考</a></p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment">#读取数据</span></div><div class="line">red=pd.read_csv(<span class="string">'winequality-red.csv'</span>,sep=<span class="string">';'</span>)</div><div class="line">white=pd.read_csv(<span class="string">'winequality-white.csv'</span>,sep=<span class="string">';'</span>)</div><div class="line"><span class="comment">#输出数据</span></div><div class="line">print(red.info)</div><div class="line">print(white.info)</div></pre></td></tr></table></figure>
<pre><code>&lt;bound method DataFrame.info of       fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \
0               7.4             0.700         0.00             1.9      0.076   
1               7.8             0.880         0.00             2.6      0.098   
2               7.8             0.760         0.04             2.3      0.092   
3              11.2             0.280         0.56             1.9      0.075   
4               7.4             0.700         0.00             1.9      0.076   
5               7.4             0.660         0.00             1.8      0.075   
6               7.9             0.600         0.06             1.6      0.069   
7               7.3             0.650         0.00             1.2      0.065   
8               7.8             0.580         0.02             2.0      0.073   
9               7.5             0.500         0.36             6.1      0.071   
10              6.7             0.580         0.08             1.8      0.097   
11              7.5             0.500         0.36             6.1      0.071   
12              5.6             0.615         0.00             1.6      0.089   
13              7.8             0.610         0.29             1.6      0.114   
14              8.9             0.620         0.18             3.8      0.176   
15              8.9             0.620         0.19             3.9      0.170   
16              8.5             0.280         0.56             1.8      0.092   
17              8.1             0.560         0.28             1.7      0.368   
18              7.4             0.590         0.08             4.4      0.086   
19              7.9             0.320         0.51             1.8      0.341   
20              8.9             0.220         0.48             1.8      0.077   
21              7.6             0.390         0.31             2.3      0.082   
22              7.9             0.430         0.21             1.6      0.106   
23              8.5             0.490         0.11             2.3      0.084   
24              6.9             0.400         0.14             2.4      0.085   
25              6.3             0.390         0.16             1.4      0.080   
26              7.6             0.410         0.24             1.8      0.080   
27              7.9             0.430         0.21             1.6      0.106   
28              7.1             0.710         0.00             1.9      0.080   
29              7.8             0.645         0.00             2.0      0.082   
...             ...               ...          ...             ...        ...   
1569            6.2             0.510         0.14             1.9      0.056   
1570            6.4             0.360         0.53             2.2      0.230   
1571            6.4             0.380         0.14             2.2      0.038   
1572            7.3             0.690         0.32             2.2      0.069   
1573            6.0             0.580         0.20             2.4      0.075   
1574            5.6             0.310         0.78            13.9      0.074   
1575            7.5             0.520         0.40             2.2      0.060   
1576            8.0             0.300         0.63             1.6      0.081   
1577            6.2             0.700         0.15             5.1      0.076   
1578            6.8             0.670         0.15             1.8      0.118   
1579            6.2             0.560         0.09             1.7      0.053   
1580            7.4             0.350         0.33             2.4      0.068   
1581            6.2             0.560         0.09             1.7      0.053   
1582            6.1             0.715         0.10             2.6      0.053   
1583            6.2             0.460         0.29             2.1      0.074   
1584            6.7             0.320         0.44             2.4      0.061   
1585            7.2             0.390         0.44             2.6      0.066   
1586            7.5             0.310         0.41             2.4      0.065   
1587            5.8             0.610         0.11             1.8      0.066   
1588            7.2             0.660         0.33             2.5      0.068   
1589            6.6             0.725         0.20             7.8      0.073   
1590            6.3             0.550         0.15             1.8      0.077   
1591            5.4             0.740         0.09             1.7      0.089   
1592            6.3             0.510         0.13             2.3      0.076   
1593            6.8             0.620         0.08             1.9      0.068   
1594            6.2             0.600         0.08             2.0      0.090   
1595            5.9             0.550         0.10             2.2      0.062   
1596            6.3             0.510         0.13             2.3      0.076   
1597            5.9             0.645         0.12             2.0      0.075   
1598            6.0             0.310         0.47             3.6      0.067   

      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \
0                    11.0                  34.0  0.99780  3.51       0.56   
1                    25.0                  67.0  0.99680  3.20       0.68   
2                    15.0                  54.0  0.99700  3.26       0.65   
3                    17.0                  60.0  0.99800  3.16       0.58   
4                    11.0                  34.0  0.99780  3.51       0.56   
5                    13.0                  40.0  0.99780  3.51       0.56   
6                    15.0                  59.0  0.99640  3.30       0.46   
7                    15.0                  21.0  0.99460  3.39       0.47   
8                     9.0                  18.0  0.99680  3.36       0.57   
9                    17.0                 102.0  0.99780  3.35       0.80   
10                   15.0                  65.0  0.99590  3.28       0.54   
11                   17.0                 102.0  0.99780  3.35       0.80   
12                   16.0                  59.0  0.99430  3.58       0.52   
13                    9.0                  29.0  0.99740  3.26       1.56   
14                   52.0                 145.0  0.99860  3.16       0.88   
15                   51.0                 148.0  0.99860  3.17       0.93   
16                   35.0                 103.0  0.99690  3.30       0.75   
17                   16.0                  56.0  0.99680  3.11       1.28   
18                    6.0                  29.0  0.99740  3.38       0.50   
19                   17.0                  56.0  0.99690  3.04       1.08   
20                   29.0                  60.0  0.99680  3.39       0.53   
21                   23.0                  71.0  0.99820  3.52       0.65   
22                   10.0                  37.0  0.99660  3.17       0.91   
23                    9.0                  67.0  0.99680  3.17       0.53   
24                   21.0                  40.0  0.99680  3.43       0.63   
25                   11.0                  23.0  0.99550  3.34       0.56   
26                    4.0                  11.0  0.99620  3.28       0.59   
27                   10.0                  37.0  0.99660  3.17       0.91   
28                   14.0                  35.0  0.99720  3.47       0.55   
29                    8.0                  16.0  0.99640  3.38       0.59   
...                   ...                   ...      ...   ...        ...   
1569                 15.0                  34.0  0.99396  3.48       0.57   
1570                 19.0                  35.0  0.99340  3.37       0.93   
1571                 15.0                  25.0  0.99514  3.44       0.65   
1572                 35.0                 104.0  0.99632  3.33       0.51   
1573                 15.0                  50.0  0.99467  3.58       0.67   
1574                 23.0                  92.0  0.99677  3.39       0.48   
1575                 12.0                  20.0  0.99474  3.26       0.64   
1576                 16.0                  29.0  0.99588  3.30       0.78   
1577                 13.0                  27.0  0.99622  3.54       0.60   
1578                 13.0                  20.0  0.99540  3.42       0.67   
1579                 24.0                  32.0  0.99402  3.54       0.60   
1580                  9.0                  26.0  0.99470  3.36       0.60   
1581                 24.0                  32.0  0.99402  3.54       0.60   
1582                 13.0                  27.0  0.99362  3.57       0.50   
1583                 32.0                  98.0  0.99578  3.33       0.62   
1584                 24.0                  34.0  0.99484  3.29       0.80   
1585                 22.0                  48.0  0.99494  3.30       0.84   
1586                 34.0                  60.0  0.99492  3.34       0.85   
1587                 18.0                  28.0  0.99483  3.55       0.66   
1588                 34.0                 102.0  0.99414  3.27       0.78   
1589                 29.0                  79.0  0.99770  3.29       0.54   
1590                 26.0                  35.0  0.99314  3.32       0.82   
1591                 16.0                  26.0  0.99402  3.67       0.56   
1592                 29.0                  40.0  0.99574  3.42       0.75   
1593                 28.0                  38.0  0.99651  3.42       0.82   
1594                 32.0                  44.0  0.99490  3.45       0.58   
1595                 39.0                  51.0  0.99512  3.52       0.76   
1596                 29.0                  40.0  0.99574  3.42       0.75   
1597                 32.0                  44.0  0.99547  3.57       0.71   
1598                 18.0                  42.0  0.99549  3.39       0.66   

      alcohol  quality  
0         9.4        5  
1         9.8        5  
2         9.8        5  
3         9.8        6  
4         9.4        5  
5         9.4        5  
6         9.4        5  
7        10.0        7  
8         9.5        7  
9        10.5        5  
10        9.2        5  
11       10.5        5  
12        9.9        5  
13        9.1        5  
14        9.2        5  
15        9.2        5  
16       10.5        7  
17        9.3        5  
18        9.0        4  
19        9.2        6  
20        9.4        6  
21        9.7        5  
22        9.5        5  
23        9.4        5  
24        9.7        6  
25        9.3        5  
26        9.5        5  
27        9.5        5  
28        9.4        5  
29        9.8        6  
...       ...      ...  
1569     11.5        6  
1570     12.4        6  
1571     11.1        6  
1572      9.5        5  
1573     12.5        6  
1574     10.5        6  
1575     11.8        6  
1576     10.8        6  
1577     11.9        6  
1578     11.3        6  
1579     11.3        5  
1580     11.9        6  
1581     11.3        5  
1582     11.9        5  
1583      9.8        5  
1584     11.6        7  
1585     11.5        6  
1586     11.4        6  
1587     10.9        6  
1588     12.8        6  
1589      9.2        5  
1590     11.6        6  
1591     11.6        6  
1592     11.0        6  
1593      9.5        6  
1594     10.5        5  
1595     11.2        6  
1596     11.0        6  
1597     10.2        5  
1598     11.0        6  

[1599 rows x 12 columns]&gt;
&lt;bound method DataFrame.info of       fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \
0               7.0             0.270         0.36           20.70      0.045   
1               6.3             0.300         0.34            1.60      0.049   
2               8.1             0.280         0.40            6.90      0.050   
3               7.2             0.230         0.32            8.50      0.058   
4               7.2             0.230         0.32            8.50      0.058   
5               8.1             0.280         0.40            6.90      0.050   
6               6.2             0.320         0.16            7.00      0.045   
7               7.0             0.270         0.36           20.70      0.045   
8               6.3             0.300         0.34            1.60      0.049   
9               8.1             0.220         0.43            1.50      0.044   
10              8.1             0.270         0.41            1.45      0.033   
11              8.6             0.230         0.40            4.20      0.035   
12              7.9             0.180         0.37            1.20      0.040   
13              6.6             0.160         0.40            1.50      0.044   
14              8.3             0.420         0.62           19.25      0.040   
15              6.6             0.170         0.38            1.50      0.032   
16              6.3             0.480         0.04            1.10      0.046   
17              6.2             0.660         0.48            1.20      0.029   
18              7.4             0.340         0.42            1.10      0.033   
19              6.5             0.310         0.14            7.50      0.044   
20              6.2             0.660         0.48            1.20      0.029   
21              6.4             0.310         0.38            2.90      0.038   
22              6.8             0.260         0.42            1.70      0.049   
23              7.6             0.670         0.14            1.50      0.074   
24              6.6             0.270         0.41            1.30      0.052   
25              7.0             0.250         0.32            9.00      0.046   
26              6.9             0.240         0.35            1.00      0.052   
27              7.0             0.280         0.39            8.70      0.051   
28              7.4             0.270         0.48            1.10      0.047   
29              7.2             0.320         0.36            2.00      0.033   
...             ...               ...          ...             ...        ...   
4868            5.8             0.230         0.31            4.50      0.046   
4869            6.6             0.240         0.33           10.10      0.032   
4870            6.1             0.320         0.28            6.60      0.021   
4871            5.0             0.200         0.40            1.90      0.015   
4872            6.0             0.420         0.41           12.40      0.032   
4873            5.7             0.210         0.32            1.60      0.030   
4874            5.6             0.200         0.36            2.50      0.048   
4875            7.4             0.220         0.26            1.20      0.035   
4876            6.2             0.380         0.42            2.50      0.038   
4877            5.9             0.540         0.00            0.80      0.032   
4878            6.2             0.530         0.02            0.90      0.035   
4879            6.6             0.340         0.40            8.10      0.046   
4880            6.6             0.340         0.40            8.10      0.046   
4881            5.0             0.235         0.27           11.75      0.030   
4882            5.5             0.320         0.13            1.30      0.037   
4883            4.9             0.470         0.17            1.90      0.035   
4884            6.5             0.330         0.38            8.30      0.048   
4885            6.6             0.340         0.40            8.10      0.046   
4886            6.2             0.210         0.28            5.70      0.028   
4887            6.2             0.410         0.22            1.90      0.023   
4888            6.8             0.220         0.36            1.20      0.052   
4889            4.9             0.235         0.27           11.75      0.030   
4890            6.1             0.340         0.29            2.20      0.036   
4891            5.7             0.210         0.32            0.90      0.038   
4892            6.5             0.230         0.38            1.30      0.032   
4893            6.2             0.210         0.29            1.60      0.039   
4894            6.6             0.320         0.36            8.00      0.047   
4895            6.5             0.240         0.19            1.20      0.041   
4896            5.5             0.290         0.30            1.10      0.022   
4897            6.0             0.210         0.38            0.80      0.020   

      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \
0                    45.0                 170.0  1.00100  3.00       0.45   
1                    14.0                 132.0  0.99400  3.30       0.49   
2                    30.0                  97.0  0.99510  3.26       0.44   
3                    47.0                 186.0  0.99560  3.19       0.40   
4                    47.0                 186.0  0.99560  3.19       0.40   
5                    30.0                  97.0  0.99510  3.26       0.44   
6                    30.0                 136.0  0.99490  3.18       0.47   
7                    45.0                 170.0  1.00100  3.00       0.45   
8                    14.0                 132.0  0.99400  3.30       0.49   
9                    28.0                 129.0  0.99380  3.22       0.45   
10                   11.0                  63.0  0.99080  2.99       0.56   
11                   17.0                 109.0  0.99470  3.14       0.53   
12                   16.0                  75.0  0.99200  3.18       0.63   
13                   48.0                 143.0  0.99120  3.54       0.52   
14                   41.0                 172.0  1.00020  2.98       0.67   
15                   28.0                 112.0  0.99140  3.25       0.55   
16                   30.0                  99.0  0.99280  3.24       0.36   
17                   29.0                  75.0  0.98920  3.33       0.39   
18                   17.0                 171.0  0.99170  3.12       0.53   
19                   34.0                 133.0  0.99550  3.22       0.50   
20                   29.0                  75.0  0.98920  3.33       0.39   
21                   19.0                 102.0  0.99120  3.17       0.35   
22                   41.0                 122.0  0.99300  3.47       0.48   
23                   25.0                 168.0  0.99370  3.05       0.51   
24                   16.0                 142.0  0.99510  3.42       0.47   
25                   56.0                 245.0  0.99550  3.25       0.50   
26                   35.0                 146.0  0.99300  3.45       0.44   
27                   32.0                 141.0  0.99610  3.38       0.53   
28                   17.0                 132.0  0.99140  3.19       0.49   
29                   37.0                 114.0  0.99060  3.10       0.71   
...                   ...                   ...      ...   ...        ...   
4868                 42.0                 124.0  0.99324  3.31       0.64   
4869                  8.0                  81.0  0.99626  3.19       0.51   
4870                 29.0                 132.0  0.99188  3.15       0.36   
4871                 20.0                  98.0  0.98970  3.37       0.55   
4872                 50.0                 179.0  0.99622  3.14       0.60   
4873                 33.0                 122.0  0.99044  3.33       0.52   
4874                 16.0                 125.0  0.99282  3.49       0.49   
4875                 18.0                  97.0  0.99245  3.12       0.41   
4876                 34.0                 117.0  0.99132  3.36       0.59   
4877                 12.0                  82.0  0.99286  3.25       0.36   
4878                  6.0                  81.0  0.99234  3.24       0.35   
4879                 68.0                 170.0  0.99494  3.15       0.50   
4880                 68.0                 170.0  0.99494  3.15       0.50   
4881                 34.0                 118.0  0.99540  3.07       0.50   
4882                 45.0                 156.0  0.99184  3.26       0.38   
4883                 60.0                 148.0  0.98964  3.27       0.35   
4884                 68.0                 174.0  0.99492  3.14       0.50   
4885                 68.0                 170.0  0.99494  3.15       0.50   
4886                 45.0                 121.0  0.99168  3.21       1.08   
4887                  5.0                  56.0  0.98928  3.04       0.79   
4888                 38.0                 127.0  0.99330  3.04       0.54   
4889                 34.0                 118.0  0.99540  3.07       0.50   
4890                 25.0                 100.0  0.98938  3.06       0.44   
4891                 38.0                 121.0  0.99074  3.24       0.46   
4892                 29.0                 112.0  0.99298  3.29       0.54   
4893                 24.0                  92.0  0.99114  3.27       0.50   
4894                 57.0                 168.0  0.99490  3.15       0.46   
4895                 30.0                 111.0  0.99254  2.99       0.46   
4896                 20.0                 110.0  0.98869  3.34       0.38   
4897                 22.0                  98.0  0.98941  3.26       0.32   

        alcohol  quality  
0      8.800000        6  
1      9.500000        6  
2     10.100000        6  
3      9.900000        6  
4      9.900000        6  
5     10.100000        6  
6      9.600000        6  
7      8.800000        6  
8      9.500000        6  
9     11.000000        6  
10    12.000000        5  
11     9.700000        5  
12    10.800000        5  
13    12.400000        7  
14     9.700000        5  
15    11.400000        7  
16     9.600000        6  
17    12.800000        8  
18    11.300000        6  
19     9.500000        5  
20    12.800000        8  
21    11.000000        7  
22    10.500000        8  
23     9.300000        5  
24    10.000000        6  
25    10.400000        6  
26    10.000000        6  
27    10.500000        6  
28    11.600000        6  
29    12.300000        7  
...         ...      ...  
4868  10.800000        6  
4869   9.800000        6  
4870  11.450000        7  
4871  12.050000        6  
4872   9.700000        5  
4873  11.900000        6  
4874  10.000000        6  
4875   9.700000        6  
4876  11.600000        7  
4877   8.800000        5  
4878   9.500000        4  
4879   9.533333        6  
4880   9.533333        6  
4881   9.400000        6  
4882  10.700000        5  
4883  11.500000        6  
4884   9.600000        5  
4885   9.550000        6  
4886  12.150000        7  
4887  13.000000        7  
4888   9.200000        5  
4889   9.400000        6  
4890  11.800000        6  
4891  10.600000        6  
4892   9.700000        5  
4893  11.200000        6  
4894   9.600000        5  
4895   9.400000        6  
4896  12.800000        7  
4897  11.800000        6  

[4898 rows x 12 columns]&gt;
</code></pre><p>在读取数据集时，红酒和白酒是分别存在于两个DataFrame变量中的，为了方便分类任务，需要将两个变量进行合并。下面对数据作预处理，然后就可以开始搭建自己的神经网络了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将红酒数据集添加一列‘type=1’</span></div><div class="line">red[<span class="string">'type'</span>]=<span class="number">1</span></div><div class="line"><span class="comment">#将白酒数据集添加“type=0”</span></div><div class="line">white[<span class="string">'type'</span>]=<span class="number">0</span></div><div class="line"><span class="comment">#将“white”、增补到“red”之后</span></div><div class="line">wines=red.append(white,ignore_index=<span class="keyword">True</span>)</div><div class="line">wines</div></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2017/07/27/59798df875dd8.png" alt="wines.png"></p>
<h1 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h1><p>现在我们已经有了完整数据集，可以再做一些更深入的数据挖掘。协方差矩阵图像就是一种很好的方法，可以直观地展示变量之间的相关性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">corr=wines.corr()</div><div class="line">sns.heatmap(corr,</div><div class="line">           xticklabels=corr.columns.values,</div><div class="line">           yticklabels=corr.columns.values)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2017/07/27/59798d8ad45f3.png" alt="output_7_0.png"></p>
<h1 id="训练集与测试集"><a href="#训练集与测试集" class="headerlink" title="训练集与测试集"></a>训练集与测试集</h1><p>大多数分类数据，都不是每个类别的样本恰好一样多，这种不平衡就会导致一些分类上的问题。（比如一个数据集里，两个类别的数量比例是<code>7:3</code>，那只要算法全部猜测为多的那一类，也能得到<code>70%</code>的正确率。）这样我们就需要让两个类别的酒都在训练集里出现，而且数量要基本一致，这样才不会产生偏差。</p>
<p>酒质量的这个数据及就是不平衡的，但我们先不做额外处理，之后可以再衡量分类性能是否有所下降，借助下采样或上采样等方式。现在，先导入<code>sklearn.model_selection</code>里的<code>train_test_split</code>方法，来把数据和标签分配到变量<code>X</code>和<code>y</code>当中。我们还需要调用<code>ravel()</code>函数把数据“展平”，以适应之后的函数输入格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#从Scikit-learn中导入train_test_split模块</span></div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment">#指定特征变量列</span></div><div class="line">X=wines.iloc[:,<span class="number">0</span>:<span class="number">11</span>]</div><div class="line"><span class="comment">#指定标签列，展平多维数组</span></div><div class="line">y=np.ravel(wines.type)</div><div class="line"><span class="comment">#将数据分割为训练集和测试集</span></div><div class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=<span class="number">0.33</span>,random_state=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>至此我们已经准备好构建第一个神经网络了，但是还有一件事值得留意，那就是数据的标准化。</p>
<h1 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h1><p>当有些数据值相隔甚远的时候，就需要进行标准化处理。<code>Scikit-Learn</code>提供了很强力且快捷的方式：从<code>sklearn.preprocessing</code>模块导入<code>StandardScaler</code>工具：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line">scaler=StandardScaler().fit(X_train)</div><div class="line">X_train=scaler.transform(X_train)</div><div class="line">X_test=scaler.transform(X_test)</div></pre></td></tr></table></figure>
<h1 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h1><p>在真正开始建模之前，回顾我们一开始的问题：能否根据化学性质，如挥发性酸度或硫酸盐，预测酒是红酒还是白酒？因为这里有两个分类：红or白，所以是个二分类<code>(binary classification)</code>问题，本质上相当于0/1, yes/no。因为神经网络只能处理数值信息，所以之前已经将红/白编码成了0/1。</p>
<p>多层感知器是一种擅长二分类的神经网络，在本教程开头已经介绍过，多层感知器通常是全连接的，也就是简单地把若干全连接层堆砌起来。在激活函数的选择上，基于熟悉Keras和神经网络的目的，可以使用最最普遍的ReLU函数。</p>
<p>那么如何开始着手构建呢？一个快捷的方法是使用Keras的序贯模型(Sequential model)：层的线性堆叠。我们可以轻松地创建模型，再把层实例传递给模型，具体的命令是:<code>model=Sequential()</code>。</p>
<p>现在来想一想多层感知器的结构：输入层，若干隐藏层和输出层。当你构建自己的模型时，必须清楚定义输入形状，模型需要知道输入形状，所以你会发现<code>input_shape, input_dim, input_length</code>或<code>batch_size</code>等。</p>
<p>全连接层在<code>Keras</code>里称为<code>Dense</code>层，执行了以下操作<code>output = activation(dot(inputs, units) + bias)</code>。注意如果没有激活函数的话，Dense层就只包含两个线性操作：点乘、求和。</p>
<p>在第一层当中，<code>activation</code>参数取值<code>relu</code>，之后定义了<code>input_shape=(11, )</code>，因为有11个特征。第一个隐藏层含有16个神经元，所以Dense()的units参数等于16，也就是说模型的输出形状为(*, 16)。units代表的就是权重矩阵，内有对应每个输入节点的权重值。因为没有将use_bias设为TRUE，所以暂时没有偏置项，这也是可行的。</p>
<p>第二个隐藏层同样使用relu激活函数，这层的输出数组形状为<code>(*, 8)</code>。最后的输出Dense层尺寸为1，用sigmoid激活函数，所以最终的输出结果是一个0-1之间的概率，对应的是样本属于标签1，即红酒的概率。</p>
<p>请在下方的代码区域搭建神经网络，要求：</p>
<ul>
<li>使用Sequential()模型</li>
<li>共有3层，且第一层的输入参数为(11,)</li>
<li>输出层使用sigmoid激活函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#导入Sequential模型和Dense层</span></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line">model=Sequential()</div><div class="line"></div><div class="line"><span class="comment">#隐藏层1</span></div><div class="line">model.add(Dense(<span class="number">16</span>,activation=<span class="string">'relu'</span>,input_shape=(<span class="number">11</span>,)))</div><div class="line"><span class="comment">#隐藏层2</span></div><div class="line">model.add(Dense(<span class="number">8</span>,activation=<span class="string">'relu'</span>))</div><div class="line"><span class="comment">#输出层</span></div><div class="line">model.add(Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>))</div></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><p>总的来讲，关于神经网络的架构，有两个关键的决策：</p>
<ol>
<li>多少层？</li>
<li>每层多少个单元？<br>在这个例子中，我们第一层有16个单元，也就是在学习数据表征时的自由度，更多的隐藏单元可以学习更复杂的表征，但是计算消耗也更大，而且容易过拟合<code>(overfitting)</code>。当模型过于复杂的时候，就会出现过拟合：把一些随机的误差或噪音也当作特征，换言之就是训练数据被拟合的“太好了”。所以当我们并没有足够多数据的时候，最好还是用相对小的神经网络，层数也不要太多。</li>
</ol>
<p>如果想要获取所建模型的信息，可以使用<code>output_shape</code>或<code>summary()</code>函数，喜爱main列举了几种常用方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#输出形状</span></div><div class="line">model.output_shape</div><div class="line"><span class="comment">#模型总览</span></div><div class="line">model.summary()</div><div class="line"><span class="comment">#详细参数</span></div><div class="line">model.get_config()</div><div class="line"><span class="comment">#权重矩阵</span></div><div class="line">model.get_weights()</div></pre></td></tr></table></figure>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 16)                192       
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 337
Trainable params: 337
Non-trainable params: 0
_________________________________________________________________





[array([[ -4.98264432e-02,  -8.99875760e-02,   1.66897923e-01,
           3.89293462e-01,   2.48389035e-01,   1.94905251e-01,
           3.81554663e-02,  -1.70459509e-01,  -4.62478936e-01,
           9.45781171e-02,  -9.45084095e-02,   4.50080931e-02,
          -2.01654226e-01,  -2.18820870e-02,  -3.53524268e-01,
          -3.39704037e-01],
        [  4.67661113e-01,   6.37504160e-02,  -2.29388103e-01,
          -5.40849864e-02,   2.22171873e-01,   2.39076287e-01,
          -3.60502452e-01,  -3.84893119e-01,   1.26932710e-01,
           3.79719436e-02,   3.56621891e-01,   1.69539779e-01,
           4.34244841e-01,   4.50510353e-01,   2.42370367e-02,
          -2.50114679e-01],
        [ -3.73600125e-01,  -2.06571698e-01,  -1.06325597e-01,
           1.82575583e-02,   9.36785340e-03,  -7.66809583e-02,
           3.23935062e-01,   3.03234130e-01,   1.04181617e-01,
          -3.18242192e-01,   2.15769619e-01,  -2.10983753e-02,
           1.22898072e-01,   3.79836261e-02,  -2.06408739e-01,
           1.86543435e-01],
        [ -1.59280300e-02,   2.84385353e-01,  -1.80770189e-01,
          -6.91838861e-02,  -4.28074747e-01,  -3.27124178e-01,
           1.92455947e-02,   4.65576321e-01,   2.14139491e-01,
           2.47457176e-01,   9.40738022e-02,  -2.64835954e-01,
          -3.01520914e-01,  -2.66410232e-01,   2.50897020e-01,
          -2.39203826e-01],
        [  9.09360349e-02,  -2.52071738e-01,   1.81674153e-01,
           4.17934448e-01,  -4.57543045e-01,   4.53864366e-01,
           1.57245368e-01,  -3.64349395e-01,   3.86538893e-01,
          -1.76164597e-01,  -5.79869747e-02,  -2.85525113e-01,
          -1.39552027e-01,   5.49268723e-03,  -3.44688624e-01,
          -2.01445311e-01],
        [ -3.61947805e-01,  -4.36158180e-02,   2.21010417e-01,
          -4.11448449e-01,   1.11243278e-01,  -1.96210444e-01,
          -3.63108486e-01,   3.47647637e-01,   7.67233074e-02,
          -4.12058502e-01,  -2.14669198e-01,  -3.62275094e-01,
          -1.37348175e-02,   1.43671960e-01,  -1.09374881e-01,
          -1.29260212e-01],
        [  1.84318751e-01,   1.69243068e-01,   2.64439911e-01,
          -3.27584505e-01,  -3.12709033e-01,   2.97704428e-01,
           1.93249792e-01,   2.26672620e-01,  -2.32822448e-01,
          -3.53965074e-01,   3.30718786e-01,   8.20287764e-02,
           1.41222507e-01,  -4.48238492e-01,  -1.47753030e-01,
          -4.31054354e-01],
        [ -3.64983499e-01,   2.66292900e-01,   8.03867280e-02,
          -3.78615826e-01,  -3.46475422e-01,   1.89222127e-01,
           2.69394010e-01,   2.37171561e-01,  -3.25533509e-01,
           3.10469061e-01,   1.54059440e-01,   4.10036236e-01,
           3.57707292e-01,  -4.47573662e-02,  -3.61494094e-01,
           2.87418455e-01],
        [ -3.18877876e-01,   2.47041434e-01,  -2.29884654e-01,
           8.18514526e-02,   2.36380666e-01,  -3.12529325e-01,
           2.58298367e-01,  -3.12896848e-01,   4.36720461e-01,
           8.30825865e-02,  -1.53442502e-01,   2.92674035e-01,
           2.43945867e-01,  -3.45032215e-01,   9.18445289e-02,
          -2.73343891e-01],
        [  1.14024431e-01,  -1.97158337e-01,   2.65030652e-01,
          -3.90317142e-01,  -5.33969104e-02,  -1.00827187e-01,
           1.35453552e-01,  -2.08345950e-02,  -3.05458009e-01,
           3.28467578e-01,   3.91551107e-01,   3.88602704e-01,
           4.19867784e-01,   1.98601454e-01,  -2.90410578e-01,
           2.18321770e-01],
        [  3.94538552e-01,  -3.01331282e-04,  -4.59927320e-01,
           3.52448225e-03,  -2.55332798e-01,  -7.66898394e-02,
           1.71944499e-02,   2.51493305e-01,  -6.00979328e-02,
           4.07272190e-01,  -1.14112884e-01,  -4.47229087e-01,
          -1.85045898e-02,   2.91900188e-01,   4.34516460e-01,
          -3.59144658e-01]], dtype=float32),
 array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.], dtype=float32),
 array([[ 0.27758062,  0.32696116, -0.12059772, -0.2686069 ,  0.08139598,
          0.38036656,  0.32520974,  0.19151318],
        [ 0.43766093,  0.29809725, -0.4557929 , -0.18581784,  0.08751357,
         -0.39931965, -0.09964991,  0.17332137],
        [-0.2620455 , -0.24762535,  0.35845268, -0.13336289,  0.04007018,
         -0.39839149,  0.01755929,  0.11646259],
        [-0.28185141, -0.41674638,  0.07205951,  0.46127093,  0.42340422,
         -0.12234998, -0.32808745,  0.49965596],
        [-0.26166177, -0.4406935 ,  0.3176899 ,  0.32351041, -0.06424642,
          0.41437888,  0.36301064,  0.2036624 ],
        [-0.27416241, -0.35417187,  0.26924002,  0.32288253, -0.16948187,
         -0.35796487, -0.04283953, -0.44096291],
        [-0.01216853, -0.30725086, -0.38324308,  0.19532835, -0.30979538,
          0.18932819,  0.26240873, -0.4475528 ],
        [-0.1612885 ,  0.19788098, -0.19374907, -0.06785023,  0.21359551,
          0.3040458 ,  0.39540446,  0.23423409],
        [ 0.01686943,  0.07593989,  0.00735629,  0.25039053,  0.25843036,
         -0.23249888,  0.02778065, -0.30911994],
        [-0.1596216 , -0.25759542, -0.19575047, -0.02004528,  0.22266507,
         -0.1529597 , -0.2789892 ,  0.12094378],
        [ 0.19889224,  0.44975781,  0.11675143, -0.16397417,  0.25484574,
          0.36306274,  0.48795998,  0.47419429],
        [-0.45383811,  0.13647282, -0.2559135 ,  0.05184174,  0.02903581,
          0.17449057, -0.27694225,  0.13545072],
        [ 0.29954553,  0.2175715 , -0.04698312,  0.05174255,  0.25326657,
          0.12707448, -0.45172453,  0.41674447],
        [-0.34929419,  0.17539358,  0.35529578,  0.26315773, -0.06466413,
         -0.19027662, -0.204934  , -0.33771062],
        [-0.30182111, -0.01916206, -0.07562017, -0.34805727, -0.27742755,
          0.18699825, -0.30500996, -0.43830144],
        [-0.45377958, -0.09787893,  0.16146803, -0.07033706, -0.08875155,
          0.04072464, -0.32710898, -0.18625259]], dtype=float32),
 array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),
 array([[-0.47236764],
        [-0.39696497],
        [ 0.32774436],
        [ 0.39144981],
        [-0.42509505],
        [-0.5582419 ],
        [ 0.58168077],
        [ 0.20806301]], dtype=float32),
 array([ 0.], dtype=float32)]
</code></pre><h1 id="编译和拟合"><a href="#编译和拟合" class="headerlink" title="编译和拟合"></a>编译和拟合</h1><p>现在是时候编译我们的模型并针对数据进行拟合了，相应的函数是compile()和fit()：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(X_train,y_train,epochs=<span class="number">5</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">2</span>)</div></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
3s - loss: 0.0807 - acc: 0.9809
Epoch 2/5
3s - loss: 0.0291 - acc: 0.9949
Epoch 3/5
3s - loss: 0.0239 - acc: 0.9959
Epoch 4/5
3s - loss: 0.0211 - acc: 0.9961
Epoch 5/5
3s - loss: 0.0187 - acc: 0.9966





&lt;keras.callbacks.History at 0x2aebe9a4d68&gt;
</code></pre><p>在编译<code>(compile)</code>过程中，我们为模型指定了adam优化器和<code>binary_crossentropy</code>损失函数。将<code>[&#39;accuracy&#39;]</code>传给参数metrics还可以监测训练过程中的准确度。optimizer和loss是编译模型需要的另外两个参数，最流行的几种优化算法有：随即梯度下降<code>(Stochastic Gradient Descent, SGD)</code>，<code>ADAM</code>和<code>RMSprop</code>。根据所选算法不同，调整的参数也会有不同，不如学习率或者动量(momentum)。损失函数的选择取决于面对的任务：比如回归问题一般用均方误差(<code>Mean Squared Error, MSE)</code>。而在这个二分类的例子中，我们用<code>binary_crossentropy</code>；对于多分类任务，可以使用<code>categorical_crossentropy</code>。</p>
<p>之后我们对所有<code>X_train</code>和<code>y_trai</code>n的样本迭代训练了5个来回，批次规模为1个样本。<code>verbose</code>则是为了设置输出内容。我们用特定的迭代回数训练模型，一次迭代(epoch)就是把所有训练集筛过一遍，然后对照测试集。批规模<code>(batch size)</code>则定义了每次在网络里传播的样本数量，这样做也是为了在内存有限的情况下优化效率。</p>
<h2 id="预测值"><a href="#预测值" class="headerlink" title="预测值"></a>预测值</h2><p>下面把训练的模型投入实战，你可以对测试集数据，预测每个样本的标签，只需调用predict()，把结果赋值给变量y_pred:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = model.predict(X_test)</div></pre></td></tr></table></figure>
<h2 id="评价模型"><a href="#评价模型" class="headerlink" title="评价模型"></a>评价模型</h2><p>现在我们已经建立了模型，并且用于对此前未见的数据做预测，之后肯定还要衡量平价一下整个模型的表现。可以直接拿y_pred和y_test去比较看看中了几个，或者使用其他更高级的度量衡。对这个实例，我们调用evaluate()函数，传递测试数据+测试标签即可得到全局得分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">score=model.evaluate(X_test,y_test,verbose=<span class="number">2</span>)</div><div class="line">print(score)</div></pre></td></tr></table></figure>
<pre><code>[0.022412170111003608, 0.9944055944055944]
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是益智的教程，参考之后动手进行实践了一遍。编译环境windows10+python3.5&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Keras实现简单的手写数字识别</title>
    <link href="http://yoursite.com/2017/07/24/Keras02-MNIST%E6%89%8B%E5%86%99%E5%AE%9E%E4%BE%8B/"/>
    <id>http://yoursite.com/2017/07/24/Keras02-MNIST手写实例/</id>
    <published>2017-07-24T14:54:03.000Z</published>
    <updated>2017-08-03T09:53:05.086Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>Keras实现简单的手写数字识别：构建模型、编译模型、训练数据、输出<br><a id="more"></a></excerpt></p>
<p><a href="http://www.cnblogs.com/yqtm/p/6924939.html" target="_blank" rel="external">参考</a><br>文中代码有点小bug,加以改正。顺带才了下数据集的坑</p>
<h2 id="导入需要的函数和包"><a href="#导入需要的函数和包" class="headerlink" title="导入需要的函数和包"></a>导入需要的函数和包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Activation,Dropout</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#从s3.amazonaws.com/img-datasets/mnist.npz下载数据太慢了。挂了代理，结果程序运行崩溃，只好写一个加载本地的文件函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path=<span class="string">'mnist.npz'</span>)</span>:</span></div><div class="line">    f=np.load(path)</div><div class="line">    x_train,y_train=f[<span class="string">'x_train'</span>],f[<span class="string">'y_train'</span>]</div><div class="line">    x_test,y_test=f[<span class="string">'x_test'</span>],f[<span class="string">'y_test'</span>]</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">return</span> (x_train,y_train),(x_test,y_test)</div></pre></td></tr></table></figure>
<p>Sequential是序贯模型，Dense是用于添加模型的层数，SGD是用于模型变异的时候优化器参数,<br>mnist是用于加载手写识别的数据集，需要在网上下载,下面是mnist.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">from ..utils.data_utils import get_file</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"></div><div class="line">def load_data(path=&apos;mnist.npz&apos;):</div><div class="line">    &quot;&quot;&quot;Loads the MNIST dataset.</div><div class="line"></div><div class="line">    # Arguments</div><div class="line">        path: path where to cache the dataset locally</div><div class="line">            (relative to ~/.keras/datasets).</div><div class="line"></div><div class="line">    # Returns</div><div class="line">        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    path = get_file(path, origin=&apos;https://s3.amazonaws.com/img-datasets/mnist.npz&apos;)</div><div class="line">    f = np.load(path)</div><div class="line">    x_train, y_train = f[&apos;x_train&apos;], f[&apos;y_train&apos;]</div><div class="line">    x_test, y_test = f[&apos;x_test&apos;], f[&apos;y_test&apos;]</div><div class="line">    f.close()</div><div class="line">    return (x_train, y_train), (x_test, y_test)</div></pre></td></tr></table></figure>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">model=Sequential()</div><div class="line">model.add(Dense(<span class="number">500</span>,input_shape=(<span class="number">784</span>,)))<span class="comment">#输入层</span></div><div class="line">model.add(Activation(<span class="string">'tanh'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"></div><div class="line">model.add(Dense(<span class="number">500</span>))<span class="comment">#隐藏层</span></div><div class="line">model.add(Activation(<span class="string">'tanh'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"></div><div class="line">model.add(Dense(<span class="number">10</span>))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure>
<ol>
<li>Dense()设定该层的结构，第一个参数表示输出的个数，第二个参数是接受的输入数据的格式。第一层中需要指定输入的格式，在之后的增加的层中输入层节点数默认是上一层的输出个数</li>
<li>Activation()指定预定义激活函数：softmax，elu、softplus、softsign、relu、、sigmoid、hard_sigmoid、linear<br></li>
<li>Dropout()用于指定每层丢掉的信息百分比。</li>
</ol>
<h2 id="编译模型"><a href="#编译模型" class="headerlink" title="编译模型"></a>编译模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sgd=SGD(lr=<span class="number">0.01</span>,decay=<span class="number">1e-6</span>,momentum=<span class="number">0.9</span>,nesterov=<span class="keyword">True</span>)<span class="comment">#设定学习效率等参数</span></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=sgd)</div><div class="line"><span class="comment">#model.compile(loss = 'categorical_crossentropy', optimizer=sgd, class_mode='categorical') #使用交叉熵作为loss</span></div></pre></td></tr></table></figure>
<p>调用model.compile()之前初始化一个优化器对象，然后传入该函数,使用优化器sgd来编译模型，用来指定学习效率等参数。编译时指定loss函数，这里使用交叉熵函数作为loss函数。</p>
<p><em>SGD</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)</div></pre></td></tr></table></figure>
<p>随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesterov动量</p>
<p>参数</p>
<ul>
<li><code>lr</code>：大于0的浮点数，学习率</li>
<li><code>momentum</code>：大于0的浮点数，动量参数</li>
<li><code>decay</code>：大于0的浮点数，每次更新后的学习率衰减值</li>
<li><code>nesterov</code>：布尔值，确定是否使用Nesterov动量</li>
</ul>
<h2 id="读取训练集和测试集"><a href="#读取训练集和测试集" class="headerlink" title="读取训练集和测试集"></a>读取训练集和测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">(x_train,y_train),(x_test,y_test)=load_data()<span class="comment">#直接加载本地文件</span></div><div class="line"><span class="comment">#(x_train,y_train),(x_test,y_test)=mnist.load_data()#不使用mnist提供的load_data函数，</span></div><div class="line">X_train=x_train.reshape(x_train.shape[<span class="number">0</span>],x_train.shape[<span class="number">1</span>]*x_train.shape[<span class="number">2</span>])</div><div class="line">X_test=x_test.reshape(x_test.shape[<span class="number">0</span>],x_test.shape[<span class="number">1</span>]*x_test.shape[<span class="number">2</span>])</div><div class="line">Y_train=(np.arange(<span class="number">10</span>)==y_train[:,<span class="keyword">None</span>]).astype(int)<span class="comment">#将index转换成一个one_hot矩阵</span></div><div class="line">Y_test=(np.arange(<span class="number">10</span>)==y_test[:,<span class="keyword">None</span>]).astype(int)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">print(x_train.shape)</div><div class="line">print(x_train)</div><div class="line">print(x_test.shape)</div><div class="line">print(<span class="string">"y_train:"</span>,y_train,len(y_train))</div><div class="line">print(y_train[:<span class="keyword">None</span>])</div><div class="line">print(y_train[:,<span class="keyword">None</span>]==np.arange(<span class="number">10</span>))</div><div class="line">print(np.arange(<span class="number">10</span>))</div></pre></td></tr></table></figure>
<pre><code>(60000, 28, 28)
[[[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]

 [[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]

 [[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]

 ..., 
 [[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]

 [[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]

 [[0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  ..., 
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]
  [0 0 0 ..., 0 0 0]]]
(10000, 28, 28)
y_train: [5 0 4 ..., 5 6 8] 60000
[5 0 4 ..., 5 6 8]
[[False False False ..., False False False]
 [ True False False ..., False False False]
 [False False False ..., False False False]
 ..., 
 [False False False ..., False False False]
 [False False False ..., False False False]
 [False False False ..., False  True False]]
[0 1 2 3 4 5 6 7 8 9]
</code></pre><ol>
<li>读取minst数据集，通过reshape()函数转换数据的格式。</li>
<li>如果我们打印x_train.shape会发现它是(60000,28,28)，即一共60000个数据，每个数据是28*28的图片。通过reshape转换为(60000,784)的线性张量。</li>
<li>如果我们打印y_train会发现它是一组表示每张图片的表示数字的数组，通过numpy的arange()和astype()函数将每个数字转换为一组长度为10的张量，代表的数字的位置是1，其它位置为0.</li>
</ol>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train,Y_train,batch_size=<span class="number">200</span>,epochs=<span class="number">100</span>,shuffle=<span class="keyword">True</span>,verbose=<span class="number">1</span>,validation_split=<span class="number">0.3</span>)</div></pre></td></tr></table></figure>
<pre><code>Train on 42000 samples, validate on 18000 samples
Epoch 1/100
42000/42000 [==============================] - 5s - loss: 1.2457 - val_loss: 0.5666
Epoch 2/100
42000/42000 [==============================] - 4s - loss: 0.9481 - val_loss: 0.4958
Epoch 3/100
42000/42000 [==============================] - 4s - loss: 0.8623 - val_loss: 0.4659
Epoch 4/100
42000/42000 [==============================] - 4s - loss: 0.8145 - val_loss: 0.4691
Epoch 5/100
42000/42000 [==============================] - 4s - loss: 0.7788 - val_loss: 0.4342
Epoch 6/100
42000/42000 [==============================] - 4s - loss: 0.7225 - val_loss: 0.4105
Epoch 7/100
42000/42000 [==============================] - 4s - loss: 0.7338 - val_loss: 0.3970
Epoch 8/100
42000/42000 [==============================] - 4s - loss: 0.6848 - val_loss: 0.3961
Epoch 9/100
42000/42000 [==============================] - 4s - loss: 0.6693 - val_loss: 0.3875
Epoch 10/100
42000/42000 [==============================] - 4s - loss: 0.6544 - val_loss: 0.3751
Epoch 11/100
42000/42000 [==============================] - 4s - loss: 0.6276 - val_loss: 0.3681
Epoch 12/100
42000/42000 [==============================] - 4s - loss: 0.6605 - val_loss: 0.3660
Epoch 13/100
42000/42000 [==============================] - 4s - loss: 0.6487 - val_loss: 0.3515
Epoch 14/100
42000/42000 [==============================] - 4s - loss: 0.6426 - val_loss: 0.3646
Epoch 15/100
42000/42000 [==============================] - 4s - loss: 0.6292 - val_loss: 0.3424
Epoch 16/100
42000/42000 [==============================] - 4s - loss: 0.6074 - val_loss: 0.3378
Epoch 17/100
42000/42000 [==============================] - 4s - loss: 0.5844 - val_loss: 0.3320
Epoch 18/100
42000/42000 [==============================] - 4s - loss: 0.5753 - val_loss: 0.3363
Epoch 19/100
42000/42000 [==============================] - 4s - loss: 0.5570 - val_loss: 0.3199
Epoch 20/100
42000/42000 [==============================] - 4s - loss: 0.5452 - val_loss: 0.3108
Epoch 21/100
42000/42000 [==============================] - 4s - loss: 0.5320 - val_loss: 0.3108
Epoch 22/100
42000/42000 [==============================] - 4s - loss: 0.5354 - val_loss: 0.3024
Epoch 23/100
42000/42000 [==============================] - 4s - loss: 0.5172 - val_loss: 0.2973
Epoch 24/100
42000/42000 [==============================] - 4s - loss: 0.5222 - val_loss: 0.3037
Epoch 25/100
42000/42000 [==============================] - 4s - loss: 0.5208 - val_loss: 0.2940
Epoch 26/100
42000/42000 [==============================] - 4s - loss: 0.5154 - val_loss: 0.2948
Epoch 27/100
42000/42000 [==============================] - 4s - loss: 0.5258 - val_loss: 0.2918
Epoch 28/100
42000/42000 [==============================] - 4s - loss: 0.5033 - val_loss: 0.2889
Epoch 29/100
42000/42000 [==============================] - 4s - loss: 0.4962 - val_loss: 0.2828
Epoch 30/100
42000/42000 [==============================] - 4s - loss: 0.4848 - val_loss: 0.2761
Epoch 31/100
42000/42000 [==============================] - 4s - loss: 0.4884 - val_loss: 0.2881
Epoch 32/100
42000/42000 [==============================] - 4s - loss: 0.4873 - val_loss: 0.2794
Epoch 33/100
42000/42000 [==============================] - 4s - loss: 0.4823 - val_loss: 0.2686
Epoch 34/100
42000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2788
Epoch 35/100
42000/42000 [==============================] - 4s - loss: 0.4781 - val_loss: 0.2732
Epoch 36/100
42000/42000 [==============================] - 4s - loss: 0.4786 - val_loss: 0.2880
Epoch 37/100
42000/42000 [==============================] - 4s - loss: 0.4829 - val_loss: 0.2729
Epoch 38/100
42000/42000 [==============================] - 4s - loss: 0.4744 - val_loss: 0.2731
Epoch 39/100
42000/42000 [==============================] - 4s - loss: 0.4564 - val_loss: 0.2698
Epoch 40/100
42000/42000 [==============================] - 4s - loss: 0.4614 - val_loss: 0.2629
Epoch 41/100
42000/42000 [==============================] - 4s - loss: 0.4673 - val_loss: 0.2586
Epoch 42/100
42000/42000 [==============================] - 4s - loss: 0.4666 - val_loss: 0.2524
Epoch 43/100
42000/42000 [==============================] - 4s - loss: 0.4545 - val_loss: 0.2682
Epoch 44/100
42000/42000 [==============================] - 4s - loss: 0.4550 - val_loss: 0.2653
Epoch 45/100
42000/42000 [==============================] - 4s - loss: 0.4426 - val_loss: 0.2537
Epoch 46/100
42000/42000 [==============================] - 4s - loss: 0.4322 - val_loss: 0.2523
Epoch 47/100
42000/42000 [==============================] - 4s - loss: 0.4541 - val_loss: 0.2552
Epoch 48/100
42000/42000 [==============================] - 4s - loss: 0.4465 - val_loss: 0.2493
Epoch 49/100
42000/42000 [==============================] - 4s - loss: 0.4366 - val_loss: 0.2445
Epoch 50/100
42000/42000 [==============================] - 4s - loss: 0.4362 - val_loss: 0.2458
Epoch 51/100
42000/42000 [==============================] - 4s - loss: 0.4388 - val_loss: 0.2446
Epoch 52/100
42000/42000 [==============================] - 4s - loss: 0.4440 - val_loss: 0.2551
Epoch 53/100
42000/42000 [==============================] - 4s - loss: 0.4278 - val_loss: 0.2469
Epoch 54/100
42000/42000 [==============================] - 4s - loss: 0.4185 - val_loss: 0.2416
Epoch 55/100
42000/42000 [==============================] - 4s - loss: 0.4086 - val_loss: 0.2332
Epoch 56/100
42000/42000 [==============================] - 4s - loss: 0.4005 - val_loss: 0.2407
Epoch 57/100
42000/42000 [==============================] - 4s - loss: 0.4064 - val_loss: 0.2396
Epoch 58/100
42000/42000 [==============================] - 4s - loss: 0.4063 - val_loss: 0.2384
Epoch 59/100
42000/42000 [==============================] - 4s - loss: 0.4020 - val_loss: 0.2358
Epoch 60/100
42000/42000 [==============================] - 4s - loss: 0.4008 - val_loss: 0.2332
Epoch 61/100
42000/42000 [==============================] - 4s - loss: 0.4045 - val_loss: 0.2338
Epoch 62/100
42000/42000 [==============================] - 4s - loss: 0.4153 - val_loss: 0.2346
Epoch 63/100
42000/42000 [==============================] - 4s - loss: 0.4102 - val_loss: 0.2279
Epoch 64/100
42000/42000 [==============================] - 4s - loss: 0.4013 - val_loss: 0.2337
Epoch 65/100
42000/42000 [==============================] - 4s - loss: 0.3945 - val_loss: 0.2312
Epoch 66/100
42000/42000 [==============================] - 4s - loss: 0.3917 - val_loss: 0.2243
Epoch 67/100
42000/42000 [==============================] - 4s - loss: 0.3780 - val_loss: 0.2219
Epoch 68/100
42000/42000 [==============================] - 4s - loss: 0.3781 - val_loss: 0.2249
Epoch 69/100
42000/42000 [==============================] - 4s - loss: 0.3755 - val_loss: 0.2192
Epoch 70/100
42000/42000 [==============================] - 4s - loss: 0.3814 - val_loss: 0.2164
Epoch 71/100
42000/42000 [==============================] - 4s - loss: 0.3843 - val_loss: 0.2197
Epoch 72/100
42000/42000 [==============================] - 4s - loss: 0.3835 - val_loss: 0.2228
Epoch 73/100
42000/42000 [==============================] - 4s - loss: 0.3908 - val_loss: 0.2281
Epoch 74/100
42000/42000 [==============================] - 4s - loss: 0.3881 - val_loss: 0.2185
Epoch 75/100
42000/42000 [==============================] - 4s - loss: 0.3870 - val_loss: 0.2108
Epoch 76/100
42000/42000 [==============================] - 4s - loss: 0.3731 - val_loss: 0.2112
Epoch 77/100
42000/42000 [==============================] - 4s - loss: 0.3685 - val_loss: 0.2069
Epoch 78/100
42000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.2059
Epoch 79/100
42000/42000 [==============================] - 4s - loss: 0.3626 - val_loss: 0.2073
Epoch 80/100
42000/42000 [==============================] - 4s - loss: 0.3594 - val_loss: 0.2053
Epoch 81/100
42000/42000 [==============================] - 4s - loss: 0.3489 - val_loss: 0.2001
Epoch 82/100
42000/42000 [==============================] - 4s - loss: 0.3521 - val_loss: 0.2007
Epoch 83/100
42000/42000 [==============================] - 4s - loss: 0.3488 - val_loss: 0.2029
Epoch 84/100
42000/42000 [==============================] - 4s - loss: 0.3531 - val_loss: 0.1984
Epoch 85/100
42000/42000 [==============================] - 4s - loss: 0.3545 - val_loss: 0.2034
Epoch 86/100
42000/42000 [==============================] - 4s - loss: 0.3559 - val_loss: 0.2053
Epoch 87/100
42000/42000 [==============================] - 4s - loss: 0.3551 - val_loss: 0.2019
Epoch 88/100
42000/42000 [==============================] - 4s - loss: 0.3538 - val_loss: 0.2043
Epoch 89/100
42000/42000 [==============================] - 4s - loss: 0.3498 - val_loss: 0.2050
Epoch 90/100
42000/42000 [==============================] - 4s - loss: 0.3566 - val_loss: 0.2076
Epoch 91/100
42000/42000 [==============================] - 4s - loss: 0.3573 - val_loss: 0.2052
Epoch 92/100
42000/42000 [==============================] - 4s - loss: 0.3633 - val_loss: 0.1994
Epoch 93/100
42000/42000 [==============================] - 4s - loss: 0.3561 - val_loss: 0.2004
Epoch 94/100
42000/42000 [==============================] - 4s - loss: 0.3473 - val_loss: 0.2015
Epoch 95/100
42000/42000 [==============================] - 4s - loss: 0.3463 - val_loss: 0.1951
Epoch 96/100
42000/42000 [==============================] - 4s - loss: 0.3485 - val_loss: 0.1985
Epoch 97/100
42000/42000 [==============================] - 4s - loss: 0.3357 - val_loss: 0.1994
Epoch 98/100
42000/42000 [==============================] - 4s - loss: 0.3399 - val_loss: 0.1965
Epoch 99/100
42000/42000 [==============================] - 4s - loss: 0.3408 - val_loss: 0.1931
Epoch 100/100
42000/42000 [==============================] - 4s - loss: 0.3366 - val_loss: 0.1956





&lt;keras.callbacks.History at 0x2a5fdb3d278&gt;
</code></pre><ul>
<li>batch_size表示每个训练块包含的数据个数，</li>
<li>epochs表示训练的次数，</li>
<li>shuffle表示是否每次训练后将batch打乱重排，</li>
<li>verbose表示是否输出进度log，</li>
<li>validation_split指定验证集占比</li>
</ul>
<h2 id="输出测试结果"><a href="#输出测试结果" class="headerlink" title="输出测试结果"></a>输出测试结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"test set"</span>)</div><div class="line">scores = model.evaluate(X_test,Y_test,batch_size=<span class="number">200</span>,verbose=<span class="number">1</span>)</div><div class="line">print(<span class="string">""</span>)</div><div class="line">print(<span class="string">"The test loss is %f"</span> % scores)</div><div class="line">result = model.predict(X_test,batch_size=<span class="number">200</span>,verbose=<span class="number">1</span>)</div><div class="line"></div><div class="line">result_max = np.argmax(result, axis = <span class="number">1</span>)</div><div class="line">test_max = np.argmax(Y_test, axis = <span class="number">1</span>)</div><div class="line"></div><div class="line">result_bool = np.equal(result_max, test_max)</div><div class="line">true_num = np.sum(result_bool)</div><div class="line">print(<span class="string">""</span>)</div><div class="line">print(<span class="string">"The accuracy of the model is %f"</span> % (true_num/len(result_bool)))</div></pre></td></tr></table></figure>
<pre><code>test set
 8800/10000 [=========================&gt;....] - ETA: 0s
The test loss is 0.185958
10000/10000 [==============================] - 0s     

The accuracy of the model is 0.943400
</code></pre><ul>
<li>model.evaluate()计算了测试集中的识别的loss值。</li>
<li>通过model.predict()，我们可以得到对于测试集中每个数字的识别结果，每个数字对应一个表示每个数字都是多少概率的长度为10的张量。</li>
<li><p>通过np.argmax()，我们得到每个数字的识别结果和期望的识别结果</p>
</li>
<li><p>通过np.equal()，我们得到每个数字是否识别正确</p>
</li>
<li><p>通过np.sum()得到识别正确的总的数字个数</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;Keras实现简单的手写数字识别：构建模型、编译模型、训练数据、输出&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用Python和R语言从头开始理解和编写神经网络</title>
    <link href="http://yoursite.com/2017/07/24/Python26-%E4%BD%BF%E7%94%A8Python%E5%92%8CR%E8%AF%AD%E8%A8%80%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E7%90%86%E8%A7%A3%E5%92%8C%E7%BC%96%E5%86%99%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2017/07/24/Python26-使用Python和R语言从头开始理解和编写神经网络/</id>
    <published>2017-07-24T14:46:25.000Z</published>
    <updated>2017-08-03T09:53:05.101Z</updated>
    
    <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要=""><br>本篇文章是<a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" target="_blank" rel="external">原文</a>的翻译过来的，自己在学习和阅读之后觉得文章非常不错，文章结构清晰，由浅入深、从理论到代码实现，最终将神经网络的概念和工作流程呈现出来。自己将其翻译成中文，以便以后阅读和复习和网友参考。因时间（文字纯手打加配图）紧促和翻译水平有限，文章有不足之处请大家指正。<br><a id="more"></a></excerpt></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>你可以通过两种方式学习和实践一个概念：</p>
<ul>
<li>选项1：您可以了解一个特定主题的整个理论，然后寻找应用这些概念的方法。所以，你阅读整个算法的工作原理，背后的数学知识、假设理论、局限，然后去应用它。这样学习稳健但是需要花费大量的时间去准备。</li>
<li>选项2：从简单的基础开始，并就此主题研究直觉上的知识。接下来，选择一个问题并开始解决它。在解决问题的同时了解这些概念，保持调整并改善您对此问题的理解。所以，你去了解如何应用一个算法——实践并应用它。一旦你知道如何应用它，请尝试使用不同的参数和测试值，极限值去测试算法和继续优化对算法的理解。</li>
</ul>
<p>我更喜欢选项2，并采取这种方法来学习任何新的话题。我可能无法告诉你算法背后的整个数学，但我可以告诉你直觉上的知识以及基于实验和理解来应用算法的最佳场景。</p>
<p>在与其他人交流的过程中，我发现人们不用花时间来发展这种直觉，所以他们能够以正确的方式努力地去解决问题。</p>
<p>在本文中，我将从头开始讨论一个神经网络的构建，更多地关注研究这种直觉上的知识来实现神经网络。我们将在“Python”和“R”中编写代码。读完本篇文章后，您将了解神经网络如何工作，如何初始化权重，以及如何使用反向传播进行更新。</p>
<p>让我们开始吧</p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li>神经网络背后的简单直觉知识</li>
<li>多层感知器及其基础知识</li>
<li>涉及神经网络方法的步骤</li>
<li>可视化神经网络工作方法的步骤</li>
<li>使用Numpy（Python）实现NN</li>
<li>使用R实现NN</li>
<li>[可选]反向传播算法的数学观点</li>
</ul>
<h1 id="神经网络背后的直观知识"><a href="#神经网络背后的直观知识" class="headerlink" title="神经网络背后的直观知识"></a>神经网络背后的直观知识</h1><p>如果您是开发人员或了解一种工作——知道如何在代码中调试错误。您可以通过改变输入或条件来触发各种测试用例，并查找输出，输出的变化提供了一个提示：在代码中，去哪里寻找bug？ - 哪个模块要检查，哪些行要阅读。找到bug后，您进行更改并继续运行，直到您能够运行正确的代码或者实现应用程序。</p>
<p>神经网络的工作方式非常相似。它需要多个输入，通过来自多个隐藏层的多个神经元进行处理，并使用输出层返回结果。这个结果估计过程在技术上被称为“前向传播”。</p>
<p>接下来，我们将结果与实际输出进行比较。任务是使神经网络的输出接近实际（期望的）输出。在这些神经元中，每一个都会对最终输出产生一些误差，你如何减少这些误差呢？</p>
<p>我们尝试最小化那些对错误“贡献”更多的神经元的值和权重，并且在返回到神经网络的神经元并发现误差在哪里时发生。这个过程被称为“向后传播”。</p>
<p>为了减少迭代次数来实现最小化误差，神经网络通常使用称为“梯度下降”的算法，来快速有效地优化任务。</p>
<p>的确 ，这就是神经网络如何工作的！我知道这是一个非常简单的表示，但它可以帮助您以简单的方式理解事物。</p>
<h1 id="多层感知器及其基础知识"><a href="#多层感知器及其基础知识" class="headerlink" title="多层感知器及其基础知识"></a>多层感知器及其基础知识</h1><p>就像原子是形成地球上任何物质的基础 - 神经网络的基本形成单位是感知器。 那么，什么是感知器呢？</p>
<p>感知器可以被理解为需要多个输入并产生一个输出的任何东西。 例如，看下面的图片<br><img src="https://i.loli.net/2017/07/24/59756c063bbec.png" alt="感知器" title="感知器"><br>上述结构需要三个输入并产生一个输出，下一个逻辑问题是输入和输出之间的关系是什么？让我们从基本的方式着手，寻求更复杂的方法。</p>
<p>下面我讨论了三种创建输入输出关系的方法：</p>
<ol>
<li>通过直接组合输入和计算基于阈值的输出。例如：取x1 = 0，x2 = 1，x3 = 1并设置阈值= 0。因此，如果<code>x1 + x2 + x3&gt; 0</code>，则输出为1，否则为0.可以看出，在这种情况下，感知器会将输出计算为1。</li>
<li>接下来，让我们为输入添加权重。权重重视输入。例如，您分别为x1，x2和x3分配w1 = 2，w2 = 3和w3 = 4。为了计算输出，我们将输入与相应权重相乘，并将其与阈值进行比较，如w1 <em> x1 + w2 </em> x2 + w3 * x3&gt;阈值。与x1和x2相比，这些权重对于x3显得更重要。</li>
<li>最后，让我们添加偏置量：每个感知器也有一个偏置量，可以被认为是感知器多么灵活。它与某种线性函数y = ax + b的常数b类似，它允许我们上下移动线以适应数据更好的预测。假设没有b，线将始终通过原点（0，0），并且可能会得到较差的拟合。例如，感知器可以具有两个输入，在这种情况下，它需要三个权重。每个输入一个，偏置一个。现在输入的线性表示将如下所示：w1 <em> x1 + w2 </em> x2 + w3 <em> x3 + 1 </em> b。</li>
</ol>
<p>但是，上面所讲的感知器之间的关系都是线性的，并没有那么有趣。所以，人们认为将感知器演化成现在所谓的人造神经元，对于输入和偏差，神经元将使用非线性变换（激活函数）。</p>
<h1 id="什么是激活函数？"><a href="#什么是激活函数？" class="headerlink" title="什么是激活函数？"></a>什么是激活函数？</h1><p>激活函数将加权输入<code>（w1 * x1 + w2 * x2 + w3 * x3 + 1 * b）</code>的和作为参数，并返回神经元的输出。<br><img src="https://i.loli.net/2017/07/24/59757bc8a16d3.png" alt="激活函数" title="激活函数"></p>
<p>在上式中，我们用x0表示1，w0表示b。</p>
<p>激活函数主要用于进行非线性变换，使我们能够拟合非线性假设或估计复杂函数。 有多种激活功能，如：<code>“Sigmoid”</code>，<code>“Tanh”</code>，<code>ReLu</code>等等。</p>
<h1 id="前向传播，反向传播和训练次数-epochs"><a href="#前向传播，反向传播和训练次数-epochs" class="headerlink" title="前向传播，反向传播和训练次数(epochs)"></a>前向传播，反向传播和训练次数(epochs)</h1><p>到目前为止，我们已经计算了输出，这个过程被称为“正向传播”。 但是如果估计的输出远离实际输出（非常大的误差）怎么办？ 下面正是我们在神经网络中所做的：基于错误更新偏差和权重。 这种权重和偏差更新过程被称为“反向传播”。</p>
<p>反向传播（BP）算法通过确定输出处的损耗（或误差），然后将其传播回网络来工作， 更新权重以最小化每个神经元产生的错误。 最小化误差的第一步是确定每个节点w.r.t.的梯度（Derivatives），最终实现输出。 要获得反向传播的数学视角，请参阅下面的部分。</p>
<p>这一轮的前向和后向传播迭代被称为一个训练迭代也称为“Epoch”。<code>ps:e（一）poch（波）的意思;一个epoch是指把所有训练数据完整的过一遍</code></p>
<h1 id="多层感知器"><a href="#多层感知器" class="headerlink" title="多层感知器"></a>多层感知器</h1><p>现在，我们来看看多层感知器。 到目前为止，我们已经看到只有一个由3个输入节点组成的单层，即x1，x2和x3，以及由单个神经元组成的输出层。 但是，出于实际，单层网络只能做到这一点。 如下所示，MLP由层叠在输入层和输出层之间的许多隐层组成。<br><img src="https://i.loli.net/2017/07/24/59757fa6a6e02.png" alt="多层感知器" title="多层感知器"><br>上面的图像只显示一个单一的隐藏层，但实际上可以包含多个隐藏层。 在MLP的情况下要记住的另一点是，所有层都完全连接，即层中的每个节点（输入和输出层除外）连接到上一层和下一层中的每个节点。让我们继续下一个主题，即神经网络的训练算法（最小化误差）。 在这里，我们将看到最常见的训练算法称为梯度下降。</p>
<h1 id="全批量梯度下降和随机梯度下降"><a href="#全批量梯度下降和随机梯度下降" class="headerlink" title="全批量梯度下降和随机梯度下降"></a>全批量梯度下降和随机梯度下降</h1><p>Gradient Descent的第二个变体通过使用相同的更新算法执行更新MLP的权重的相同工作，但差异在于用于更新权重和偏差的训练样本的数量。</p>
<p>全部批量梯度下降算法作为名称意味着使用所有的训练数据点来更新每个权重一次，而随机渐变使用1个或更多（样本），但从不使整个训练数据更新权重一次。</p>
<p>让我们用一个简单的例子来理解这个10个数据点的数据集，它们有两个权重w1和w2。</p>
<ul>
<li><p>全批：您可以使用10个数据点（整个训练数据），并计算w1（Δw1）的变化和w2（Δw2）的变化，并更新w1和w2。</p>
</li>
<li><p>SGD：使用第一个数据点并计算w1（Δw1）的变化，并改变w2（Δw2）并更新w1和w2。 接下来，当您使用第二个数据点时，您将处理更新的权重</p>
</li>
</ul>
<h1 id="神经网络方法的步骤"><a href="#神经网络方法的步骤" class="headerlink" title="神经网络方法的步骤"></a>神经网络方法的步骤</h1><p><img src="https://i.loli.net/2017/07/24/59757fa6a6e02.png" alt="多层感知器" title="多层感知器"><br>我们来看一步一步地构建神经网络的方法（MLP与一个隐藏层，类似于上图所示的架构）。 在输出层，我们只有一个神经元，因为我们正在解决二进制分类问题（预测0或1）。 我们也可以有两个神经元来预测两个类的每一个。</p>
<p>先看一下广泛的步骤：</p>
<ol>
<li><p>我们输入和输出</p>
<ul>
<li>X作为输入矩阵</li>
<li>y作为输出矩阵</li>
</ul>
</li>
<li><p>我们用随机值初始化权重和偏差（这是一次启动，在下一次迭代中，我们将使用更新的权重和偏差）。 让我们定义：</p>
<ul>
<li>wh作为权重矩阵隐藏层</li>
<li>bh作为隐藏层的偏置矩阵</li>
<li>wout作为输出层的权重矩阵</li>
<li>bout作为偏置矩阵作为输出层</li>
</ul>
</li>
<li><p>我们将输入和权重的矩阵点积分配给输入和隐藏层之间的边，然后将隐层神经元的偏差添加到相应的输入，这被称为线性变换：</p>
<p>   <code>hidden_layer_input= matrix_dot_product(X,wh) + bh</code></p>
</li>
<li>使用激活函数（Sigmoid）执行非线性变换。 Sigmoid将返回输出1/(1 + exp(-x)).<br>   <code>hiddenlayer_activations = sigmoid(hidden_layer_input)</code></li>
<li><p>对隐藏层激活进行线性变换（取矩阵点积，并加上输出层神经元的偏差），然后应用激活函数（再次使用Sigmoid，但是根据您的任务可以使用任何其他激活函数 ）来预测输出</p>
<p>   <code>output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout</code></p>
<p>   <code>output = sigmoid(output_layer_input)</code></p>
<p><strong>所有上述步骤被称为“前向传播”（Forward Propagation）</strong></p>
</li>
<li>将预测与实际输出进行比较，并计算误差梯度（实际预测值）。 误差是均方损失= ((Y-t)^2)/2<br>   <code>E = y – output</code></li>
<li><p>计算隐藏和输出层神经元的斜率/斜率（为了计算斜率，我们计算每个神经元的每层的非线性激活x的导数）。 S形梯度可以返回 <code>x * (1 – x)</code>.</p>
<p>   <code>slope_output_layer = derivatives_sigmoid(output)</code></p>
<p>   <code>slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</code></p>
</li>
<li>计算输出层的变化因子（delta），取决于误差梯度乘以输出层激活的斜率<br>   <code>d_output = E * slope_output_layer</code></li>
<li>在这一步，错误将传播回网络，这意味着隐藏层的错误。 为此，我们将采用输出层三角形的点积与隐藏层和输出层之间的边缘的重量参数（wout.T）。<br>   <code>Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)</code></li>
<li>计算隐层的变化因子（delta），将隐层的误差乘以隐藏层激活的斜率<br>  <code>d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</code></li>
<li><p>在输出和隐藏层更新权重：网络中的权重可以从为训练示例计算的错误中更新。<br>   <code>wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate</code></p>
<p>   <code>wh =  wh + matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate</code><br>learning_rate：权重更新的量由称为学习率的配置参数控制）</p>
</li>
<li><p>在输出和隐藏层更新偏差：网络中的偏差可以从该神经元的聚合错误中更新。</p>
<ul>
<li>bias at output_layer =bias at output_layer + sum of delta of output_layer at row-wise * learning_rate</li>
<li><p>bias at hidden_layer =bias at hidden_layer + sum of delta of output_layer at row-wise * learning_rate  </p>
<p><code>bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate</code></p>
<p><code>bout = bout + sum(d_output, axis=0)*learning_rate</code></p>
</li>
</ul>
<p><strong>从6到12的步骤被称为“向后传播”(Backward Propagation)</strong></p>
</li>
</ol>
<p>一个正向和反向传播迭代被认为是一个训练周期。 如前所述，我们什么时候训练第二次，然后更新权重和偏差用于正向传播。</p>
<p>以上，我们更新了隐藏和输出层的权重和偏差，我们使用了全批量梯度下降算法。</p>
<h1 id="神经网络方法的可视化步骤"><a href="#神经网络方法的可视化步骤" class="headerlink" title="神经网络方法的可视化步骤"></a>神经网络方法的可视化步骤</h1><p>我们将重复上述步骤，可视化输入，权重，偏差，输出，误差矩阵，以了解神经网络（MLP）的工作方法。</p>
<ul>
<li><p><strong>注意：</strong></p>
<ul>
<li>对于良好的可视化图像，我有2或3个位置的十进制小数位。</li>
<li>黄色填充的细胞代表当前活动细胞</li>
<li>橙色单元格表示用于填充当前单元格值的输入</li>
</ul>
</li>
</ul>
<ul>
<li>步骤1：读取输入和输出<br><img src="https://i.loli.net/2017/07/24/597588fe182b4.png" alt="Step 1"></li>
<li>步骤2：用随机值初始化权重和偏差（有初始化权重和偏差的方法，但是现在用随机值初始化）<br><img src="https://i.loli.net/2017/07/24/597589475b8bb.png" alt="Step 2"></li>
<li>步骤3：计算隐层输入： <br><br><code>hidden_layer_input= matrix_dot_product(X,wh) + bh</code><br><img src="https://i.loli.net/2017/07/24/597589a62c037.png" alt="Step 3"></li>
<li>步骤4：对隐藏的线性输入进行非线性变换 <br><br><code>hiddenlayer_activations = sigmoid(hidden_layer_input)</code><br><img src="https://i.loli.net/2017/07/24/59758a0111ed8.png" alt="Step 4"></li>
<li><p>步骤5：在输出层执行隐层激活的线性和非线性变换 <br><br><code>output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout</code> <br><br><code>output = sigmoid(output_layer_input)</code><br><img src="https://i.loli.net/2017/07/24/59758a58893ba.png" alt="Step 5"></p>
</li>
<li><p>步骤6：计算输出层的误差（E）梯度 <br><br><code>E = y-output</code><br><img src="https://i.loli.net/2017/07/24/59758ad4a72ff.png" alt="Step 6"></p>
</li>
<li>步骤7：计算输出和隐藏层的斜率 <br><br><code>Slope_output_layer= derivatives_sigmoid(output)</code> <br><br><code>Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</code><br><img src="https://i.loli.net/2017/07/24/59758b26893ef.png" alt="py26-10.png"></li>
<li>步骤8：计算输出层的增量 <br><br><code>d_output = E * slope_output_layer*lr</code><br><img src="https://i.loli.net/2017/07/24/59758b61227a9.png" alt="py26-11.png"></li>
<li>步骤9：计算隐藏层的误差 <br><br><code>Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)</code><br><img src="https://i.loli.net/2017/07/24/59758ba276123.png" alt="py26-12.png"></li>
<li>步骤10：计算隐藏层的增量 <br><br><code>d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</code><br><img src="https://i.loli.net/2017/07/24/59758bd705865.png" alt="py26-13.png"></li>
<li>步骤11：更新输出和隐藏层的权重 <br><br><code>wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate</code> <br><br><code>wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate</code><br><img src="https://i.loli.net/2017/07/24/59758c13cc478.png" alt="py26-14.png"></li>
<li>步骤12：更新输出和隐藏层的偏置量<br><br><code>bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate</code><br><br><code>bout = bout + sum(d_output, axis=0)*learning_rate</code><br><img src="https://i.loli.net/2017/07/24/59758c71210be.png" alt="py26-15.png"></li>
</ul>
<p>以上，您可以看到仍然有一个很好的误差而不接近于实际目标值，因为我们已经完成了一次训练迭代。 如果我们多次训练模型，那么这将是一个非常接近的实际结果。 我完成了数千次迭代，我的结果接近实际的目标值（<code>[[0.98032096] [0.96845624] [0.04532167]]</code>）。</p>
<h1 id="使用Numpy（Python）实现NN"><a href="#使用Numpy（Python）实现NN" class="headerlink" title="使用Numpy（Python）实现NN"></a>使用Numpy（Python）实现NN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment">#Input array</span></div><div class="line">X=np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</div><div class="line"></div><div class="line"><span class="comment">#Output</span></div><div class="line">y=np.array([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</div><div class="line"></div><div class="line"><span class="comment">#Sigmoid Function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span> <span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</div><div class="line"></div><div class="line"><span class="comment">#Derivative of Sigmoid Function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">derivatives_sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> - x)</div><div class="line"></div><div class="line"><span class="comment">#Variable initialization</span></div><div class="line">epoch=<span class="number">5000</span> <span class="comment">#Setting training iterations</span></div><div class="line">lr=<span class="number">0.1</span> <span class="comment">#Setting learning rate</span></div><div class="line">inputlayer_neurons = X.shape[<span class="number">1</span>] <span class="comment">#number of features in data set</span></div><div class="line">hiddenlayer_neurons = <span class="number">3</span> <span class="comment">#number of hidden layers neurons</span></div><div class="line">output_neurons = <span class="number">1</span> <span class="comment">#number of neurons at output layer</span></div><div class="line"></div><div class="line"><span class="comment">#weight and bias initialization</span></div><div class="line">wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))</div><div class="line">bh=np.random.uniform(size=(<span class="number">1</span>,hiddenlayer_neurons))</div><div class="line">wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))</div><div class="line">bout=np.random.uniform(size=(<span class="number">1</span>,output_neurons))</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</div><div class="line">    <span class="comment">#Forward Propogation</span></div><div class="line">    hidden_layer_input1=np.dot(X,wh)</div><div class="line">    hidden_layer_input=hidden_layer_input1 + bh</div><div class="line">    hiddenlayer_activations = sigmoid(hidden_layer_input)</div><div class="line">    output_layer_input1=np.dot(hiddenlayer_activations,wout)</div><div class="line">    output_layer_input= output_layer_input1+ bout</div><div class="line">    output = sigmoid(output_layer_input)</div><div class="line">    <span class="comment">#Backpropagation</span></div><div class="line">    E = y-output</div><div class="line">    slope_output_layer = derivatives_sigmoid(output)</div><div class="line">    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)</div><div class="line">    d_output = E * slope_output_layer</div><div class="line">    Error_at_hidden_layer = d_output.dot(wout.T)</div><div class="line">    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer</div><div class="line">    wout += hiddenlayer_activations.T.dot(d_output) *lr</div><div class="line">    bout += np.sum(d_output, axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>) *lr</div><div class="line">    wh += X.T.dot(d_hiddenlayer) *lr</div><div class="line">    bh += np.sum(d_hiddenlayer, axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>) *lr</div><div class="line"></div><div class="line">print(<span class="string">"output of Forward Propogation:\n&#123;&#125;"</span>.format(output))</div><div class="line">print(<span class="string">"wout,bout of Backpropagation:\n&#123;&#125;,\n&#123;&#125;"</span>.format(wout,bout))</div></pre></td></tr></table></figure>
<pre><code>output of Forward Propogation:
[[ 0.98497471]
 [ 0.96956956]
 [ 0.0416628 ]]
wout,bout of Backpropagation:
[[ 3.34342103]
 [-1.97924327]
 [ 3.90636787]],
[[-1.71231223]]
</code></pre><h1 id="在R中实现NN"><a href="#在R中实现NN" class="headerlink" title="在R中实现NN"></a>在R中实现NN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># input matrix</span></div><div class="line">X=matrix(c(<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>),nrow = <span class="number">3</span>, ncol=<span class="number">4</span>,byrow = TRUE)</div><div class="line"></div><div class="line"><span class="comment"># output matrix</span></div><div class="line">Y=matrix(c(<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>),byrow=FALSE)</div><div class="line"></div><div class="line"><span class="comment">#sigmoid function</span></div><div class="line">sigmoid&lt;-function(x)&#123;</div><div class="line"><span class="number">1</span>/(<span class="number">1</span>+exp(-x))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># derivative of sigmoid function</span></div><div class="line">derivatives_sigmoid&lt;-function(x)&#123;</div><div class="line">x*(<span class="number">1</span>-x)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># variable initialization</span></div><div class="line">epoch=<span class="number">5000</span></div><div class="line">lr=<span class="number">0.1</span></div><div class="line">inputlayer_neurons=ncol(X)</div><div class="line">hiddenlayer_neurons=<span class="number">3</span></div><div class="line">output_neurons=<span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment">#weight and bias initialization</span></div><div class="line">wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=<span class="number">0</span>,sd=<span class="number">1</span>), inputlayer_neurons, hiddenlayer_neurons)</div><div class="line">bias_in=runif(hiddenlayer_neurons)</div><div class="line">bias_in_temp=rep(bias_in, nrow(X))</div><div class="line">bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE)</div><div class="line">wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=<span class="number">0</span>,sd=<span class="number">1</span>), hiddenlayer_neurons, output_neurons)</div><div class="line"></div><div class="line">bias_out=runif(output_neurons)</div><div class="line">bias_out_temp=rep(bias_out,nrow(X))</div><div class="line">bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE)</div><div class="line"><span class="comment"># forward propagation</span></div><div class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:epoch)&#123;</div><div class="line"></div><div class="line">hidden_layer_input1= X%*%wh</div><div class="line">hidden_layer_input=hidden_layer_input1+bh</div><div class="line">hidden_layer_activations=sigmoid(hidden_layer_input)</div><div class="line">output_layer_input1=hidden_layer_activations%*%wout</div><div class="line">output_layer_input=output_layer_input1+bout</div><div class="line">output= sigmoid(output_layer_input)</div><div class="line"></div><div class="line"><span class="comment"># Back Propagation</span></div><div class="line"></div><div class="line">E=Y-output</div><div class="line">slope_output_layer=derivatives_sigmoid(output)</div><div class="line">slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)</div><div class="line">d_output=E*slope_output_layer</div><div class="line">Error_at_hidden_layer=d_output%*%t(wout)</div><div class="line">d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer</div><div class="line">wout= wout + (t(hidden_layer_activations)%*%d_output)*lr</div><div class="line">bout= bout+rowSums(d_output)*lr</div><div class="line">wh = wh +(t(X)%*%d_hiddenlayer)*lr</div><div class="line">bh = bh + rowSums(d_hiddenlayer)*lr</div><div class="line"></div><div class="line">&#125;</div><div class="line">output</div></pre></td></tr></table></figure>
<h1 id="可选-反向传播算法的数学理解"><a href="#可选-反向传播算法的数学理解" class="headerlink" title="[可选]反向传播算法的数学理解"></a>[可选]反向传播算法的数学理解</h1><p>设Wi为输入层和隐层之间的权重。 Wh是隐层和输出层之间的权重。</p>
<p>现在，<code>h =σ（u）=σ（WiX）</code>，即h是u的函数，u是Wi和X的函数。这里我们将我们的函数表示为σ</p>
<p><code>Y =σ（u&#39;）=σ（Whh）</code>，即Y是u’的函数，u’是Wh和h的函数。</p>
<p>我们将不断参考上述方程来计算偏导数。</p>
<p>我们主要感兴趣的是找到两个项：∂E/∂Wi和∂E/∂Wh即改变输入和隐藏层之间权重的误差变化，改变隐层和输出之间权重的变化 层。</p>
<p>但是为了计算这两个偏导数，我们将需要使用部分微分的链规则，因为E是Y的函数，Y是u’的函数，u’是Wi的函数。</p>
<p>让我们把这个属性很好的用于计算梯度。</p>
<p>`∂E/∂Wh = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂Wh), ……..(1)</p>
<p>We know E is of the form E=(Y-t)2/2.</p>
<p>So, (∂E/∂Y)= (Y-t)`</p>
<p>现在，σ是一个S形函数，并具有σ（1-σ）形式的有意义的区分。 我敦促读者在他们身边进行验证。<br>所以, <code>(∂Y/∂u’)= ∂( σ(u’)/ ∂u’= σ(u’)(1- σ(u’))</code>.</p>
<p>但是, <code>σ(u’)=Y, So</code>,</p>
<p><code>(∂Y/∂u’)=Y(1-Y)</code></p>
<p>现在得出, <code>( ∂u’/∂Wh)= ∂( Whh)/ ∂Wh = h</code></p>
<p>取代等式（1）中的值我们得到，</p>
<p><code>∂E/∂Wh = (Y-t). Y(1-Y).h</code></p>
<p>所以，现在我们已经计算了隐层和输出层之间的梯度。 现在是计算输入层和隐藏层之间的梯度的时候了。<br><code>∂E/∂Wi =(∂ E/∂ h). (∂h/∂u).( ∂u/∂Wi)</code><br>但是，<code>(∂ E/∂ h) = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h)</code>. 在上述方程中替换这个值得到：</p>
<p><code>∂E/∂Wi =[(∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h)]. (∂h/∂u).( ∂u/∂Wi)……………(2)</code><br>那么，首先计算隐层和输出层之间的梯度有什么好处？</p>
<p>如等式（2）所示，我们已经计算出∂E/∂Y和∂Y/∂u’节省了空间和计算时间。 我们会在一段时间内知道为什么这个算法称为反向传播算法。</p>
<p>让我们计算公式（2）中的未知导数。</p>
<p><code>∂u’/∂h = ∂(Whh)/ ∂h = Wh</code></p>
<p><code>∂h/∂u = ∂( σ(u)/ ∂u= σ(u)(1- σ(u))</code></p>
<p>但是, <code>σ(u)=h, So,</code></p>
<p><code>(∂Y/∂u)=h(1-h)</code></p>
<p>得出, <code>∂u/∂Wi = ∂(WiX)/ ∂Wi = X</code></p>
<p>取代等式（2）中的所有这些值，我们得到：<br><br><code>∂E/∂Wi = [(Y-t). Y(1-Y).Wh].h(1-h).X</code></p>
<p>所以现在，由于我们已经计算了两个梯度，所以权重可以更新为:</p>
<p><code>Wh = Wh + η . ∂E/∂Wh</code></p>
<p> <code>Wi = Wi + η . ∂E/∂Wi</code></p>
<p>其中η是学习率。</p>
<p>所以回到这个问题：为什么这个算法叫做反向传播算法？</p>
<p>原因是：如果您注意到∂E/∂Wh和∂E/∂Wi的最终形式，您将看到术语（Yt）即输出错误，这是我们开始的，然后将其传播回输入 层重量更新。</p>
<p>那么，这个数学在哪里适合代码？</p>
<p><code>hiddenlayer_activations= H</code></p>
<p><code>E = Y-t</code></p>
<p><code>Slope_output_layer = Y（1-Y）</code></p>
<p><code>lr =η</code></p>
<p><code>slope_hidden_layer = h（1-h）</code></p>
<p><code>wout = Wh</code></p>
<p>现在，您可以轻松地将代码与数学联系起来。</p>
<h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>本文主要从头开始构建神经网络，并了解其基本概念。 我希望你现在可以理解神经网络的工作，如前向和后向传播的工作，优化算法（全批次和随机梯度下降），如何更新权重和偏差，Excel中每个步骤的可视化以及建立在python和R的代码.</p>
<p>因此，在即将到来的文章中，我将解释在Python中使用神经网络的应用，并解决与以下问题相关的现实生活中的挑战：</p>
<ol>
<li>计算机视觉</li>
<li>言语</li>
<li>自然语言处理</li>
</ol>
<p>我在写这篇文章的时候感到很愉快，并希望从你的反馈中学习。 你觉得这篇文章有用吗？ 感谢您的建议/意见。 请随时通过以下意见提出您的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">（转载请注明来源）</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;excerpt in=&quot;&quot; index=&quot;&quot; |=&quot;&quot; 首页摘要=&quot;&quot;&gt;&lt;br&gt;本篇文章是&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文&lt;/a&gt;的翻译过来的，自己在学习和阅读之后觉得文章非常不错，文章结构清晰，由浅入深、从理论到代码实现，最终将神经网络的概念和工作流程呈现出来。自己将其翻译成中文，以便以后阅读和复习和网友参考。因时间（文字纯手打加配图）紧促和翻译水平有限，文章有不足之处请大家指正。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
